<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>pytorch-2-深度学习 | chiblog</title><meta name="author" content="神经蛙"><meta name="copyright" content="神经蛙"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="pytorch-2-深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch-2-深度学习">
<meta property="og:url" content="https://chierhy.github.io/2023/03/03/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/19-pytorch-2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="chiblog">
<meta property="og:description" content="pytorch-2-深度学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg">
<meta property="article:published_time" content="2023-03-03T15:32:00.000Z">
<meta property="article:modified_time" content="2023-03-03T15:34:12.000Z">
<meta property="article:author" content="神经蛙">
<meta property="article:tag" content="AI👾">
<meta property="article:tag" content="笔记🎫">
<meta property="article:tag" content="python">
<meta property="article:tag" content="code💻">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg"><link rel="shortcut icon" href="https://pic4.zhimg.com/v2-da217cabde0a382120e68118571d60e3_r.jpg"><link rel="canonical" href="https://chierhy.github.io/2023/03/03/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/19-pytorch-2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch-2-深度学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-03 23:34:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="chiblog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://github.com/chierhy/chierhy.github.io/blob/master/%E9%99%84%E4%BB%B6/1.jpg?raw=true" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">chiblog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch-2-深度学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-03T15:32:00.000Z" title="发表于 2023-03-03 23:32:00">2023-03-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-03T15:34:12.000Z" title="更新于 2023-03-03 23:34:12">2023-03-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94/">科研</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="pytorch-2-深度学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p><a target="_blank" rel="noopener" href="https://handbook.pytorch.wiki/chapter2/2.2-deep-learning-basic-mathematics.html">https://handbook.pytorch.wiki/chapter2/2.2-deep-learning-basic-mathematics.html</a></p>
</blockquote>
<p>监督学习、无监督学习、半监督学习、强化学习(设定reward function)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear, Module, MSELoss</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">torch.__version__</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">20</span>,<span class="number">500</span>)</span><br><span class="line">y = <span class="number">5</span>*x + <span class="number">7</span></span><br><span class="line">plt.plot(x,y)</span><br><span class="line"></span><br><span class="line">x=np.random.rand(<span class="number">256</span>)</span><br><span class="line">noise=np.random.rand(<span class="number">256</span>)/<span class="number">4</span></span><br><span class="line">y = x * <span class="number">5</span> + <span class="number">7</span> + noise</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line">df[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line">df[<span class="string">&#x27;y&#x27;</span>] = y</span><br><span class="line">sns.lmplot(x=<span class="string">&#x27;x&#x27;</span>,y=<span class="string">&#x27;y&#x27;</span>,data=df)</span><br><span class="line"></span><br><span class="line">model=Linear(<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># (1,1)：输入的特征数量都是1</span></span><br><span class="line">criterion=MSELoss()</span><br><span class="line">optim=SGD(model.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line">epochs=<span class="number">2000</span></span><br><span class="line"><span class="comment"># x_train, y_train 的形状是 (256, 1)， 代表 mini-batch 大小为256， feature 为1. astype(&#x27;float32&#x27;) 是为了下一步可以直接转换为 torch.float.</span></span><br><span class="line">x_train = x.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y_train = y.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 整理输入和输出的数据，这里输入和输出一定要是torch的Tensor类型</span></span><br><span class="line">    inputs = torch.from_numpy(x_train)</span><br><span class="line">    labels = torch.from_numpy(y_train)</span><br><span class="line">    <span class="comment">#使用模型进行预测</span></span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="comment">#梯度置0，否则会累加</span></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 使用优化器默认方法优化</span></span><br><span class="line">    optim.step()</span><br><span class="line">    <span class="keyword">if</span> (i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">        <span class="comment">#每 100次打印一下损失函数，看看效果</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch &#123;&#125;, loss &#123;:1.4f&#125;&#x27;</span>.<span class="built_in">format</span>(i,loss.data.item()))  </span><br><span class="line"></span><br><span class="line">[w,b]=model.parameters()</span><br><span class="line"><span class="built_in">print</span>(w.item(),b.item())</span><br><span class="line"></span><br><span class="line">predicted = model.forward(torch.from_numpy(x_train)).data.numpy()</span><br><span class="line">plt.plot(x_train, y_train, <span class="string">&#x27;go&#x27;</span>, label = <span class="string">&#x27;data&#x27;</span>, alpha = <span class="number">0.3</span>)</span><br><span class="line">plt.plot(x_train, predicted, label = <span class="string">&#x27;predicted&#x27;</span>, alpha = <span class="number">1</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>梯度下降<br>普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。<br>如果训练样本的大小比较大时，一次读入不到内存或者现存中，那我们必须要使用 Mini-batch来分批的计算 - Mini-batch size的计算规则如下，在内存允许的最大情况下使用2的N次方个size<br><img src="/" alt="https://handbook.pytorch.wiki/chapter2/2.png"><br><img src="/" alt="https://handbook.pytorch.wiki/chapter2/3.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#torch.optim.SGD</span></span><br><span class="line"><span class="comment"># 如果设置了momentum，就是带有动量的SGD，可以不设置lr学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># RMSprop（root mean square prop）也是一种可以加快梯度下降的算法，利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，使其梯度下降的速度变得更快</span></span><br><span class="line">optimizer = torch.optim.RMSprop(model.parameters(), lr=<span class="number">0.01</span>, alpha=<span class="number">0.99</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法</span></span><br><span class="line"><span class="comment"># 这里的lr，betas，还有eps都是用默认值即可，所以Adam是一个使用起来最简单的优化方法</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>)</span><br></pre></td></tr></table></figure>

<p>方差度量了同样大小的训练集的变动所导致的学习性能的变化，即模型的泛化能力</p>
<p>欠拟合： - 增加网络结构，如增加隐藏层数目； - 训练更长时间； - 寻找合适的网络架构，使用更大的NN结构；</p>
<p>过拟合 ： - 使用更多的数据； - 正则化（ regularization）； - 寻找合适的网络结构；</p>
<p>正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度<br>需要说明的是：l1 相比于 l2 会更容易获得稀疏解.<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/37096933/answer/70507353">https://www.zhihu.com/question/37096933/answer/70507353</a></p>
<pre><code>L1正则化：损失函数基础上加上权重参数的绝对值
L2正则化：损失函数基础上加上权重参数的平方和
</code></pre>
<h1 id="cnn"><a href="#cnn" class="headerlink" title="cnn"></a>cnn</h1><p>我们通过卷积的计算操作来提取图像局部的特征，每一层都会计算出一些局部特征，这些局部特征再汇总到下一层，这样一层一层的传递下去，特征由小变大，最后在通过这些局部的特征对图片进行处理，这样大大提高了计算效率，也提高了准确度</p>
<p>kernel,3<em>3和5</em>5是最佳大小，根据经验。<br>在每一个卷积层中我们都会设置多个核，每个核代表着不同的特征，这些特征就是我们需要传递到下一层的输出，而我们训练的过程就是训练这些不同的核。</p>
<p>池化层是CNN的重要组成部分，通过减少卷积层之间的连接，降低运算复杂程度，池化层的操作很简单，就想相当于是合并，我们输入一个过滤器的大小，与卷积的操作一样，也是一步一步滑动，但是过滤器覆盖的区域进行合并，只保留一个值。 合并的方式也有很多种，例如我们常用的两种取最大值maxpooling，取平均值avgpooling。（降低网络训练参数及模型的过拟合程度。）</p>
<p>dropout是2014年 Hinton 提出防止过拟合而采用的trick，增强了模型的泛化能力 Dropout（随机失活）是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络，说的通俗一点，就是随机将一部分网络的传播掐断，听起来好像不靠谱，但是通过实际测试效果非常好。 有兴趣的可以去看一下原文Dropout: A Simple Way to Prevent Neural Networks from Overfitting <a target="_blank" rel="noopener" href="http://jmlr.org/papers/v15/srivastava14a.html">http://jmlr.org/papers/v15/srivastava14a.html</a></p>
<p>全链接层一般是作为最后的输出层使用，卷积的作用是提取图像的特征，最后的全连接层就是要通过这些特征来进行计算，输出我们所要的结果了，无论是分类，还是回归。<br>我们的特征都是使用矩阵表示的，所以再传入全连接层之前还需要对特征进行压扁，将他这些特征变成一维的向量，如果要进行分类的话，就是用sofmax作为输出，如果要是回归的话就直接使用linear即可。</p>
<p>1998， Yann LeCun 的 LeNet5 官网<a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/lenet/index.html">http://yann.lecun.com/exdb/lenet/index.html</a></p>
<p>卷积神经网路的开山之作，麻雀虽小，但五脏俱全，卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件 - 用卷积提取空间特征； - 由空间平均得到子样本； - 用 tanh 或 sigmoid 得到非线性； - 用 multi-layer neural network（MLP）作为最终分类器； - 层层之间用稀疏的连接矩阵，以避免大的计算成本。</p>
<p>输入：图像Size为3232。<br>输出：10个类别，分别为0-9数字的概率</p>
<ol>
<li>C1层是一个卷积层，有6个卷积核（提取6种局部特征），核大小为5 * 5</li>
<li>S2层是pooling层，下采样（区域:2 * 2 ）降低网络训练参数及模型的过拟合程度。</li>
<li>C3层是第二个卷积层，使用16个卷积核，核大小:5 * 5 提取特征</li>
<li>S4层也是一个pooling层，区域:2*2</li>
<li>C5层是最后一个卷积层，卷积核大小:5 * 5 卷积核种类:120<br>最后使用全连接层，将C5的120个特征进行分类，最后输出0-9的概率</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet5</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet5, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>) <span class="comment"># 这里论文上写的是conv,官方教程用了线性层</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        <span class="comment"># 最后使用全连接层，将C5的120个特征进行分类，最后输出0-9的概率</span></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = LeNet5()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a><br>2012，Alex Krizhevsky 可以算作LeNet的一个更深和更广的版本，可以用来学习更复杂的对象 论文 - 用rectified linear units（ReLU）得到非线性； - 使用 dropout 技巧在训练期间有选择性地忽略单个神经元，来减缓模型的过拟合； - 重叠最大池，避免平均池的平均效果； - 使用 GPU NVIDIA GTX 580 可以减少训练时间，这比用CPU处理快了 10 倍，所以可以被用于更大的数据集和图像上。</p>
<p>虽然 AlexNet只有8层，但是它有60M以上的参数总量，Alexnet有一个特殊的计算层，LRN层，做的事是对当前层的输出结果做平滑处理，这里就不做详细介绍了， Alexnet的每一阶段（含一次卷积主要计算的算作一层）可以分为8层： 1. con - relu - pooling - LRN ： 要注意的是input层是227*227，而不是paper里面的224，这里可以算一下，主要是227可以整除后面的conv1计算，224不整除。如果一定要用224可以通过自动补边实现，不过在input就补边感觉没有意义，补得也是0，这就是我们上面说的公式的重要性。</p>
<p>conv - relu - pool - LRN ： group&#x3D;2，这个属性强行把前面结果的feature map分开，卷积部分分成两部分做</p>
<p>conv - relu</p>
<p>conv - relu</p>
<p>conv - relu - pool</p>
<p>fc - relu - dropout ： dropout层，在alexnet中是说在训练的以1&#x2F;2概率使得隐藏层的某些neuron的输出为0，这样就丢到了一半节点的输出，BP的时候也不更新这些节点，防止过拟合。</p>
<p>fc - relu - dropout</p>
<p>fc - softmax</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">model = torchvision.models.alexnet(pretrained=<span class="literal">False</span>) <span class="comment">#我们不下载预训练权重</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<p>VGG<br>2015，牛津的 VGG。论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.1556.pdf">https://arxiv.org/pdf/1409.1556.pdf</a></p>
<p>每个卷积层中使用更小的 3×3 filters，并将它们组合成卷积序列<br>多个3×3卷积序列可以模拟更大的接收场的效果<br>每次的图像像素缩小一倍，卷积核的数量增加一倍<br>VGG有很多个版本，也算是比较稳定和经典的model。它的特点也是连续conv多计算量巨大。</p>
<p>VGG清一色用小卷积核，结合作者和自己的观点，这里整理出小卷积核比用大卷积核的优势：</p>
<p>根据作者的观点，input8 -&gt; 3层conv3x3后，output&#x3D;2，等同于1层conv7x7的结果； input&#x3D;8 -&gt; 2层conv3x3后，output&#x3D;2，等同于2层conv5x5的结果</p>
<p>卷积层的参数减少。相比5x5、7x7和11x11的大卷积核，3x3明显地减少了参数量</p>
<p>通过卷积和池化层后，图像的分辨率降低为原来的一半，但是图像的特征增加一倍，这是一个十分规整的操作: 分辨率由输入的224-&gt;112-&gt;56-&gt;28-&gt;14-&gt;7， 特征从原始的RGB3个通道-&gt; 64 -&gt;128 -&gt; 256 -&gt; 512</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="literal">False</span>) <span class="comment">#我们不下载预训练权重</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<p>GoogLeNet (Inception)<br>2014，Google Christian Szegedy 论文 - 使用1×1卷积块（NiN）来减少特征数量，这通常被称为“瓶颈”，可以减少深层神经网络的计算负担。 - 每个池化层之前，增加 feature maps，增加每一层的宽度来增多特征的组合性</p>
<p>googlenet最大的特点就是包含若干个inception模块，所以有时候也称作 inception net。 googlenet虽然层数要比VGG多很多，但是由于inception的设计，计算速度方面要快很多。<br><img src="attachment:image.png" alt="https://handbook.pytorch.wiki/chapter2/googlenet.png"></p>
<p>Inception架构的主要思想是找出如何让已有的稠密组件接近与覆盖卷积视觉网络中的最佳局部稀疏结构。现在需要找出最优的局部构造，并且重复几次。之前的一篇文献提出一个层与层的结构，在最后一层进行相关性统计，将高相关性的聚集到一起。这些聚类构成下一层的单元，且与上一层单元连接。假设前面层的每个单元对应于输入图像的某些区域，这些单元被分为滤波器组。在接近输入层的低层中，相关单元集中在某些局部区域，最终得到在单个区域中的大量聚类，在最后一层通过1x1的卷积覆盖。</p>
<p>上面的话听起来很生硬，其实解释起来很简单：每一模块我们都是用若干个不同的特征提取方式，例如 3x3卷积，5x5卷积，1x1的卷积，pooling等，都计算一下，最后再把这些结果通过Filter Concat来进行连接，找到这里面作用最大的。而网络里面包含了许多这样的模块，这样不用我们人为去判断哪个特征提取方式好，网络会自己解决（是不是有点像AUTO ML），在Pytorch中实现了InceptionA-E，还有InceptionAUX 模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># inception_v3需要scipy，所以没有安装的话pip install scipy 一下</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">model = torchvision.models.inception_v3(pretrained=<span class="literal">False</span>) <span class="comment">#我们不下载预训练权重</span></span><br><span class="line"><span class="comment"># print(model)</span></span><br></pre></td></tr></table></figure>

<p>ResNet<br>2015，Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun 论文 Kaiming He 何凯明（音译）这个大神大家一定要记住，现在很多论文都有他参与(mask rcnn, focal loss)，Jian Sun孙剑老师就不用说了，现在旷视科技的首席科学家。 刚才的GoogLeNet已经很深了，ResNet可以做到更深，通过残差计算，可以训练超过1000层的网络，俗称跳连接</p>
<p><code>退化问题</code><br>网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。这个就是网络退化的问题，退化问题说明了深度网络不能很简单地被很好地优化</p>
<p><code>残差网络的解决办法</code><br>深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那现在要解决的就是学习恒等映射函数了。让一些层去拟合一个潜在的恒等映射函数H(x) &#x3D; x，比较困难。如果把网络设计为H(x) &#x3D; F(x) + x。我们可以转换为学习一个残差函数F(x) &#x3D; H(x) - x。 只要F(x)&#x3D;0，就构成了一个恒等映射H(x) &#x3D; x. 而且，拟合残差肯定更加容易。<br><img src="/" alt="https://handbook.pytorch.wiki/chapter2/resnet.png"></p>
<p>我们在激活函数前将上一层（或几层）的输出与本层计算的输出相加，将求和的结果输入到激活函数中做为本层的输出，引入残差后的映射对输出的变化更敏感，其实就是看本层相对前几层是否有大的变化，相当于是一个差分放大器的作用。图中的曲线就是残差中的shoutcut，他将前一层的结果直接连接到了本层，也就是俗称的跳连接.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">False</span>) <span class="comment">#我们不下载预训练权重</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<p><img src="/" alt="https://handbook.pytorch.wiki/chapter2/cnn.png"><br>准确率和计算量之间的对比。建议是，小型图片分类任务，resnet18基本上已经可以了，如果真对准确度要求比较高，再选其他更好的网络架构。</p>
<h1 id="rnn"><a href="#rnn" class="headerlink" title="rnn"></a>rnn</h1><p>本质是：拥有记忆的能力，并且会根据这些记忆的内容来进行推断。因此，他的输出就依赖于当前的输入和记忆。</p>
<p>最常用的RNN类型是LSTM，它在捕获长期依赖性方面要比RNN好得多。 但不要担心，LSTM与我们将在本教程中开发的RNN基本相同，它们只是采用不同的方式来计算隐藏状态。 我们将在后面更详细地介绍LSTM。 以下是RNN在NLP中的一些示例： 语言建模与生成文本  机器翻译 语音识别 生成图像描述.</p>
<p>循环神经网络的基本结构特别简单，就是将网络的输出保存在一个记忆单元中，这个记忆单元和下一次的输入一起进入神经网络中</p>
<p>根据循环神经网络的结构也可以看出它在处理序列类型的数据上具有天然的优势。因为网络本身就是 一个序列结构，这也是所有循环神经网络最本质的结构。</p>
<p>记忆最大的问题在于它有遗忘性</p>
<p>pytorch 中使用 nn.RNN 类来搭建基于序列的循环神经网络，它的构造函数有以下几个参数： - input_size：输入数据X的特征值的数目。 - hidden_size：隐藏层的神经元数量，也就是隐藏层的特征数量。 - num_layers：循环神经网络的层数，默认值是 1。 - bias：默认为 True，如果为 false 则表示神经元不使用 bias 偏移参数。 - batch_first：如果设置为 True，则输入数据的维度中第一个维度就是 batch 值，默认为 False。默认情况下第一个维度是序列的长度， 第二个维度才是 - - batch，第三个维度是特征数目。 - dropout：如果不为空，则表示最后跟一个 dropout 层抛弃部分数据，抛弃数据的比例由该参数指定。<br>RNN 中最主要的参数是 input_size 和 hidden_size，这两个参数务必要搞清楚。其余的参数通常不用设置，采用默认值就可以了。</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html?highlight=rnn#torch.nn.RNN">https://pytorch.org/docs/stable/nn.html?highlight=rnn#torch.nn.RNN</a><br>$h_t &#x3D; \tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh}) $<br>公式里面的 $x_t$ 是我们当前状态的输入值，$h_{(t-1)}$ 就是上面说的要传入的上一个状态的hidden_state，也就是记忆部分。 整个网络要训练的部分就是 $W_{ih}$ 当前状态输入值的权重，$W_{hh}$ hidden_state也就是上一个状态的权重还有这两个输入偏置值。这四个值加起来使用tanh进行激活，pytorch默认是使用tanh作为激活，也可以通过设置使用relu作为激活函数。</p>
<p>RNN 因为多了 序列（sequence） 这个维度，要使用同一个模型跑 n 次前向传播，这个n就是我们序列设置的个数。 下面我们开始手动实现我们的RNN：参考的是karpathy大佬的文章：<a target="_blank" rel="noopener" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">https://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">rnn = torch.nn.RNN(<span class="number">20</span>, <span class="number">50</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">100</span>, <span class="number">32</span>, <span class="number">20</span>)</span><br><span class="line">h_0 =torch.randn(<span class="number">2</span>, <span class="number">32</span> ,<span class="number">50</span>)</span><br><span class="line">output,hn=rnn(<span class="built_in">input</span>, h_0) </span><br><span class="line"><span class="built_in">print</span>(output.size(), hn.size())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,input_size,hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.W_xh = torch.nn.Linear(input_size, hidden_size) <span class="comment">#因为最后的操作是相加 所以hidden要和output的shape一致</span></span><br><span class="line">        self.W_hh = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, hidden</span>):</span><br><span class="line">        <span class="keyword">return</span> self.step(x, hidden)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self, x, hidden</span>):</span><br><span class="line">        <span class="comment">#前向传播的一步</span></span><br><span class="line">        h1 = self.W_hh(hidden)</span><br><span class="line">        w1 = self.W_xh(x)</span><br><span class="line">        out = torch.tanh(h1 + w1)</span><br><span class="line">        hidden = self.W_hh.weight</span><br><span class="line">        <span class="keyword">return</span> out, hidden</span><br><span class="line">    </span><br><span class="line">rnn = RNN(<span class="number">20</span>, <span class="number">50</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">32</span> , <span class="number">20</span>)</span><br><span class="line">h_0 = torch.randn(<span class="number">32</span>, <span class="number">50</span>) </span><br><span class="line">seq_len = <span class="built_in">input</span>.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">    output, hn = rnn(<span class="built_in">input</span>[i, :], h_0)</span><br><span class="line"><span class="built_in">print</span>(output.size(), h_0.size())</span><br></pre></td></tr></table></figure>

<p>LSTM<br>LSTM 是 Long Short Term Memory Networks 的缩写，按字面翻译就是长的短时记忆网络。LSTM 的网络结构是 1997 年由 Hochreiter 和 Schmidhuber 提出的，随后这种网络结构变得非常流行。 LSTM虽然只解决了短期依赖的问题，并且它通过刻意的设计来避免长期依赖问题，这样的做法在实际应用中被证明还是十分有效的，有很多人跟进相关的工作解决了很多实际的问题，所以现在LSTM 仍然被广泛地使用。<a target="_blank" rel="noopener" href="https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45">https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45</a></p>
<p>标准的循环神经网络内部只有一个简单的层结构，而 LSTM 内部有 4 个层结构：</p>
<p>第一层是个忘记层：决定状态中丢弃什么信息<br>第二层tanh层用来产生更新值的候选项，说明状态在某些维度上需要加强，在某些维度上需要减弱<br>第三层sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个缩放的作用，极端情况下sigmoid输出0说明相应维度上的状态不需要更新<br>最后一层决定输出什么，输出值跟状态有关。候选项中的哪些部分最终会被输出由一个sigmoid层来决定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lstm = torch.nn.LSTM(<span class="number">10</span>, <span class="number">20</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">c0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">output, hn = lstm(<span class="built_in">input</span>, (h0, c0))</span><br><span class="line"><span class="built_in">print</span>(output.size(), hn[<span class="number">0</span>].size(), hn[<span class="number">1</span>].size())</span><br></pre></td></tr></table></figure>

<p>GRU<br>GRU 是 gated recurrent units 的缩写，由 Cho在 2014 年提出。GRU 和 LSTM 最大的不同在于 GRU 将遗忘门和输入门合成了一个”更新门”，同时网络不再额外给出记忆状态，而是将输出结果作为记忆状态不断向后循环传递，网络的输人和输出都变得特别简单。<br><img src="/" alt="https://handbook.pytorch.wiki/chapter2/gru.gif"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rnn = torch.nn.GRU(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">h_0= torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">output, hn = rnn(<span class="built_in">input</span>, h0)</span><br><span class="line"><span class="built_in">print</span>(output.size(),hn.size())</span><br></pre></td></tr></table></figure>

<p>循环网络的向后传播（BPTT）<br>在向前传播的情况下，RNN的输入随着每一个时间步前进。在反向传播的情况下，我们“回到过去”改变权重，因此我们叫它通过时间的反向传播（BPTT）。</p>
<p>我们通常把整个序列（单词）看作一个训练样本，所以总的误差是每个时间步（字符）中误差的和。权重在每一个时间步长是相同的（所以可以计算总误差后一起更新）。 1. 使用预测输出和实际输出计算交叉熵误差 2. 网络按照时间步完全展开 3. 对于展开的网络，对于每一个实践步计算权重的梯度 4. 因为对于所有时间步来说，权重都一样，所以对于所有的时间步，可以一起得到梯度（而不是像神经网络一样对不同的隐藏层得到不同的梯度） 5. 随后对循环神经元的权重进行升级</p>
<p>RNN展开的网络看起来像一个普通的神经网络。反向传播也类似于普通的神经网络，只不过我们一次得到所有时间步的梯度。如果有100个时间步，那么网络展开后将变得非常巨大，所以为了解决这个问题才会出现LSTM和GRU这样的结构。</p>
<p>RNN在NLP应用<br>词嵌入<br>在自然语言处理中，因为单词的数目过多比如有 10000 个不同的词，那么使用 one-hot 这样的方式来定义，效率就特别低，每个单词都是 10000 维的向量。其中只有一位是 1 ， 其余都是 0，特别占用内存，而且也不能体现单词的词性，因为每一个单词都是 one-hot，虽然有些单词在语义上会更加接近.但是 one-hot 没办法体现这个特点，所以 必须使用另外一种方式定义每一个单词。<br>用不同的特征来对各个词汇进行表征，相对与不同的特征，不同的单词均有不同的值这就是词嵌入。<br>词嵌入不仅对不同单词实现了特征化的表示，还能通过计算词与词之间的相似度，实际上是在多维空间中，寻找词向量之间各个维度的距离相似度，我们就可以实现类比推理，比如说夏天和热，冬天和冷，都是有关联关系的。<br>在 PyTorch 中我们用 nn.Embedding 层来做嵌入词袋模型，Embedding层第一个输入表示我们有多少个词，第二个输入表示每一个词使用多少维度的向量表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># an Embedding module containing 10 tensors of size 3</span></span><br><span class="line">embedding = torch.nn.Embedding(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># a batch of 2 samples of 4 indices each</span></span><br><span class="line"><span class="built_in">input</span> = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">9</span>]])</span><br><span class="line">output = embedding(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure>

<p>beam search<br>Beam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。虽然Beam Search算法是不完全的，但是用于了解空间较大的系统中，可以减少空间占用和时间。<br>Beam search可以看做是做了约束优化的广度优先搜索，首先使用广度优先策略建立搜索树，树的每层，按照启发代价对节点进行排序，然后仅留下预先确定的个数（Beam width-集束宽度）的节点，仅这些节点在下一层次继续扩展，其他节点被剪切掉。 1. 将初始节点插入到list中 2. 将给节点出堆，如果该节点是目标节点，则算法结束； 3. 否则扩展该节点，取集束宽度的节点入堆。然后到第二步继续循环。 4. 算法结束的条件是找到最优解或者堆为空。<br>在使用上，集束宽度可以是预先约定的，也可以是变化的，具体可以根据实际场景调整设定。</p>
<p>注意力模型<br>对于使用编码和解码的RNN模型，我们能够实现较为准确度机器翻译结果。对于短句子来说，其性能是十分良好的，但是如果是很长的句子，翻译的结果就会变差。 我们人类进行人工翻译的时候，都是一部分一部分地进行翻译，引入的注意力机制，和人类的翻译过程非常相似，其也是一部分一部分地进行长句子的翻译。</p>
<h1 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h1><p>logistic回归是一种广义线性回归（generalized linear model），与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 wx + b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将wx+b作为因变量，即y &#x3D;wx+b,而logistic回归则通过函数L将wx+b对应一个隐状态p，p &#x3D;L(wx+b),然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归。</p>
<p>说的更通俗一点，就是logistic回归会在线性回归后再加一层logistic函数的调用。</p>
<p>logistic回归主要是进行二分类预测，我们在激活函数时候讲到过 Sigmod函数，Sigmod函数是最常见的logistic函数，因为Sigmod函数的输出的是是对于0~1之间的概率值，当概率大于0.5预测为1，小于0.5预测为0。</p>
<p>German Credit数据是根据个人的银行贷款信息和申请客户贷款逾期发生情况来预测贷款违约倾向的数据集，数据集包含24个维度的，1000条数据，如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载数据 https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data=np.loadtxt(<span class="string">&quot;./data/german.data-numeric&quot;</span>)</span><br><span class="line"><span class="comment"># 对数据归一化处理</span></span><br><span class="line">n,l=data.shape</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(l-<span class="number">1</span>):</span><br><span class="line">    meanVal=np.mean(data[:,j])</span><br><span class="line">    stdVal=np.std(data[:,j])</span><br><span class="line">    data[:,j]=(data[:,j]-meanVal)/stdVal</span><br><span class="line"><span class="comment"># 打乱数据</span></span><br><span class="line">np.random.shuffle(data)</span><br><span class="line"><span class="comment"># 数据和标签 900条用于训练，100条作为测试</span></span><br><span class="line"><span class="comment"># german.data-numeric的格式为，前24列为24个维度，最后一个为要打的标签（0，1）</span></span><br><span class="line">train_data=data[:<span class="number">900</span>,:l-<span class="number">1</span>]</span><br><span class="line">train_lab=data[:<span class="number">900</span>,l-<span class="number">1</span>]-<span class="number">1</span></span><br><span class="line">test_data=data[<span class="number">900</span>:,:l-<span class="number">1</span>]</span><br><span class="line">test_lab=data[<span class="number">900</span>:,l-<span class="number">1</span>]-<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LR</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LR,self).__init__()</span><br><span class="line">        self.fc=nn.Linear(<span class="number">24</span>,<span class="number">2</span>) <span class="comment"># 由于24个维度已经固定了，所以这里写24</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        out=self.fc(x)</span><br><span class="line">        out=torch.sigmoid(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">pred,lab</span>):</span><br><span class="line">    t=pred.<span class="built_in">max</span>(-<span class="number">1</span>)[<span class="number">1</span>]==lab</span><br><span class="line">    <span class="keyword">return</span> torch.mean(t.<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line">net=LR() </span><br><span class="line">criterion=nn.CrossEntropyLoss() <span class="comment"># 使用CrossEntropyLoss损失</span></span><br><span class="line">optm=torch.optim.Adam(net.parameters()) <span class="comment"># Adam优化</span></span><br><span class="line">epochs=<span class="number">1000</span> <span class="comment"># 训练1000次</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 指定模型为训练模式，计算梯度</span></span><br><span class="line">    net.train()</span><br><span class="line">    <span class="comment"># 输入值都需要转化成torch的Tensor</span></span><br><span class="line">    x=torch.from_numpy(train_data).<span class="built_in">float</span>()</span><br><span class="line">    y=torch.from_numpy(train_lab).long()</span><br><span class="line">    y_hat=net(x)</span><br><span class="line">    loss=criterion(y_hat,y) <span class="comment"># 计算损失</span></span><br><span class="line">    optm.zero_grad() <span class="comment"># 前一步的损失清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optm.step() <span class="comment"># 优化</span></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>)%<span class="number">100</span> ==<span class="number">0</span> : <span class="comment"># 这里我们每100次输出相关的信息</span></span><br><span class="line">        <span class="comment"># 指定模型为计算模式</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        test_in=torch.from_numpy(test_data).<span class="built_in">float</span>()</span><br><span class="line">        test_l=torch.from_numpy(test_lab).long()</span><br><span class="line">        test_out=net(test_in)</span><br><span class="line">        <span class="comment"># 使用我们的测试函数计算准确率</span></span><br><span class="line">        accu=test(test_out,test_l)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch:&#123;&#125;,Loss:&#123;:.4f&#125;,Accuracy：&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>,loss.item(),accu))</span><br></pre></td></tr></table></figure>

<p>MNIST数据集手写数字识别</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">BATCH_SIZE=<span class="number">512</span> <span class="comment">#大概需要2G的显存</span></span><br><span class="line">EPOCHS=<span class="number">20</span> <span class="comment"># 总共训练批次</span></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>) <span class="comment"># 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多</span></span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">        datasets.MNIST(<span class="string">&#x27;data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, </span><br><span class="line">                       transform=transforms.Compose([</span><br><span class="line">                           transforms.ToTensor(),</span><br><span class="line">                           transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                       ])),</span><br><span class="line">        batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">        datasets.MNIST(<span class="string">&#x27;data&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">                           transforms.ToTensor(),</span><br><span class="line">                           transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                       ])),</span><br><span class="line">        batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># batch*1*28*28（每次会送入batch个样本，输入通道数1（黑白图像），图像分辨率是28x28）</span></span><br><span class="line">        <span class="comment"># 下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数，第三个参数指卷积核的大小</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>) <span class="comment"># 输入通道数1，输出通道数10，核的大小5</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, <span class="number">3</span>) <span class="comment"># 输入通道数10，输出通道数20，核的大小3</span></span><br><span class="line">        <span class="comment"># 下面的全连接层Linear的第一个参数指输入通道数，第二个参数指输出通道数</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">20</span>*<span class="number">10</span>*<span class="number">10</span>, <span class="number">500</span>) <span class="comment"># 输入通道数是2000，输出通道数是500</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">500</span>, <span class="number">10</span>) <span class="comment"># 输入通道数是500，输出通道数是10，即10分类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        in_size = x.size(<span class="number">0</span>) <span class="comment"># 在本例中in_size=512，也就是BATCH_SIZE的值。输入的x可以看成是512*1*28*28的张量。</span></span><br><span class="line">        out = self.conv1(x) <span class="comment"># batch*1*28*28 -&gt; batch*10*24*24（28x28的图像经过一次核为5x5的卷积，输出变为24x24）</span></span><br><span class="line">        out = F.relu(out) <span class="comment"># batch*10*24*24（激活函数ReLU不改变形状））</span></span><br><span class="line">        out = F.max_pool2d(out, <span class="number">2</span>, <span class="number">2</span>) <span class="comment"># batch*10*24*24 -&gt; batch*10*12*12（2*2的池化层会减半）</span></span><br><span class="line">        out = self.conv2(out) <span class="comment"># batch*10*12*12 -&gt; batch*20*10*10（再卷积一次，核的大小是3）</span></span><br><span class="line">        out = F.relu(out) <span class="comment"># batch*20*10*10</span></span><br><span class="line">        out = out.view(in_size, -<span class="number">1</span>) <span class="comment"># batch*20*10*10 -&gt; batch*2000（out的第二维是-1，说明是自动推算，本例中第二维是20*10*10）</span></span><br><span class="line">        out = self.fc1(out) <span class="comment"># batch*2000 -&gt; batch*500</span></span><br><span class="line">        out = F.relu(out) <span class="comment"># batch*500</span></span><br><span class="line">        out = self.fc2(out) <span class="comment"># batch*500 -&gt; batch*10</span></span><br><span class="line">        out = F.log_softmax(out, dim=<span class="number">1</span>) <span class="comment"># 计算log(softmax(x))</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">model = ConvNet().to(DEVICE)</span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, device, train_loader, optimizer, epoch</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span>(batch_idx+<span class="number">1</span>)%<span class="number">30</span> == <span class="number">0</span>: </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, batch_idx * <span class="built_in">len</span>(data), <span class="built_in">len</span>(train_loader.dataset),</span><br><span class="line">                <span class="number">100.</span> * batch_idx / <span class="built_in">len</span>(train_loader), loss.item()))</span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model, device, test_loader</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            output = model(data)</span><br><span class="line">            test_loss += F.nll_loss(output, target, reduction=<span class="string">&#x27;sum&#x27;</span>).item() <span class="comment"># 将一批的损失相加</span></span><br><span class="line">            pred = output.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># 找到概率最大的下标</span></span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss, correct, <span class="built_in">len</span>(test_loader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_loader.dataset)))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, EPOCHS + <span class="number">1</span>):</span><br><span class="line">    train(model, DEVICE, train_loader, optimizer, epoch)</span><br><span class="line">    test(model, DEVICE, test_loader)</span><br></pre></td></tr></table></figure>

<p>通过Sin预测Cos-RNN</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation</span><br><span class="line"><span class="keyword">import</span> math, random</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">TIME_STEP = <span class="number">10</span> <span class="comment"># rnn 时序步长数</span></span><br><span class="line">INPUT_SIZE = <span class="number">1</span> <span class="comment"># rnn 的输入维度</span></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>) </span><br><span class="line">H_SIZE = <span class="number">64</span> <span class="comment"># of rnn 隐藏单元个数</span></span><br><span class="line">EPOCHS=<span class="number">300</span> <span class="comment"># 总共训练次数</span></span><br><span class="line">h_state = <span class="literal">None</span> <span class="comment"># 隐藏层状态</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy生成数据</span></span><br><span class="line">steps = np.linspace(<span class="number">0</span>, np.pi*<span class="number">2</span>, <span class="number">256</span>, dtype=np.float32)</span><br><span class="line">x_np = np.sin(steps) </span><br><span class="line">y_np = np.cos(steps)</span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Sin and Cos&#x27;</span>,fontsize=<span class="string">&#x27;18&#x27;</span>)</span><br><span class="line">plt.plot(steps, y_np, <span class="string">&#x27;r-&#x27;</span>, label=<span class="string">&#x27;target (cos)&#x27;</span>)</span><br><span class="line">plt.plot(steps, x_np, <span class="string">&#x27;b-&#x27;</span>, label=<span class="string">&#x27;input (sin)&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNN, self).__init__()</span><br><span class="line">        self.rnn = nn.RNN(</span><br><span class="line">        input_size=INPUT_SIZE,</span><br><span class="line">        hidden_size=H_SIZE, </span><br><span class="line">        num_layers=<span class="number">1</span>, </span><br><span class="line">        batch_first=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line">        self.out = nn.Linear(H_SIZE, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, h_state</span>):</span><br><span class="line">         <span class="comment"># x (batch, time_step, input_size)</span></span><br><span class="line">         <span class="comment"># h_state (n_layers, batch, hidden_size)</span></span><br><span class="line">         <span class="comment"># r_out (batch, time_step, hidden_size)</span></span><br><span class="line">        r_out, h_state = self.rnn(x, h_state)</span><br><span class="line">        outs = [] <span class="comment"># 保存所有的预测值</span></span><br><span class="line">        <span class="keyword">for</span> time_step <span class="keyword">in</span> <span class="built_in">range</span>(r_out.size(<span class="number">1</span>)): <span class="comment"># 计算每一步长的预测值</span></span><br><span class="line">            outs.append(self.out(r_out[:, time_step, :]))</span><br><span class="line">        <span class="keyword">return</span> torch.stack(outs, dim=<span class="number">1</span>), h_state</span><br><span class="line">         <span class="comment"># 也可使用以下这样的返回值</span></span><br><span class="line">         <span class="comment"># r_out = r_out.view(-1, 32)</span></span><br><span class="line">         <span class="comment"># outs = self.out(r_out)</span></span><br><span class="line">         <span class="comment"># return outs, h_state</span></span><br><span class="line">            </span><br><span class="line">rnn = RNN().to(DEVICE)</span><br><span class="line">optimizer = torch.optim.Adam(rnn.parameters()) <span class="comment"># Adam优化，几乎不用调参</span></span><br><span class="line">criterion = nn.MSELoss() <span class="comment"># 因为最终的结果是一个数值，所以损失函数用均方误差</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">rnn.train()</span><br><span class="line">plt.figure(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    start, end = step * np.pi, (step+<span class="number">1</span>)*np.pi <span class="comment"># 一个时间周期</span></span><br><span class="line">    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)</span><br><span class="line">    x_np = np.sin(steps) </span><br><span class="line">    y_np = np.cos(steps)</span><br><span class="line">    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis]) <span class="comment"># shape (batch, time_step, input_size)</span></span><br><span class="line">    y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])</span><br><span class="line">    x=x.to(DEVICE)</span><br><span class="line">    prediction, h_state = rnn(x, h_state) <span class="comment"># rnn output</span></span><br><span class="line">    <span class="comment"># 这一步非常重要</span></span><br><span class="line">    h_state = h_state.data <span class="comment"># 重置隐藏层的状态, 切断和前一次迭代的链接</span></span><br><span class="line">    loss = criterion(prediction.cpu(), y) </span><br><span class="line">    <span class="comment"># 这三行写在一起就可以</span></span><br><span class="line">    optimizer.zero_grad() </span><br><span class="line">    loss.backward() </span><br><span class="line">    optimizer.step() </span><br><span class="line">    <span class="keyword">if</span> (step+<span class="number">1</span>)%<span class="number">20</span>==<span class="number">0</span>: <span class="comment">#每训练20个批次可视化一下效果，并打印一下loss</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;EPOCHS: &#123;&#125;,Loss:&#123;:4f&#125;&quot;</span>.<span class="built_in">format</span>(step,loss))</span><br><span class="line">        plt.plot(steps, y_np.flatten(), <span class="string">&#x27;r-&#x27;</span>)</span><br><span class="line">        plt.plot(steps, prediction.cpu().data.numpy().flatten(), <span class="string">&#x27;b-&#x27;</span>)</span><br><span class="line">        plt.draw()</span><br><span class="line">        plt.pause(<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<p>迁移学习： 基于样本的迁移，基于特征的迁移，基于模型的迁移，以及基于关系的迁移<br>对于不同的领域微调的方法也不一样，比如语音识别领域一般微调前几层，图片识别问题微调后面几层<br>对于图片来说，我们CNN的前几层学习到的都是低级的特征，比如，点、线、面，这些低级的特征对于任何图片来说都是可以抽象出来的，所以我们将他作为通用数据，只微调这些低级特征组合起来的高级特征即可，例如，这些点、线、面，组成的是圆还是椭圆，还是正方形，这些代表的含义是我们需要后面训练出来的。<br>对于语音来说，每个单词表达的意思都是一样的，只不过发音或者是单词的拼写不一样，比如 苹果，apple，apfel（德语），都表示的是同一个东西，只不过发音和单词不一样，但是他具体代表的含义是一样的，就是高级特征是相同的，所以我们只要微调低级的特征就可以了。</p>
<p>ConvNet as fixed feature extractor.： 其实这里有两种做法：<br>使用最后一个fc layer之前的fc layer获得的特征，学习个线性分类器(比如SVM)<br>重新训练最后一个fc layer<br>Fine-tuning the ConvNet<br>固定前几层的参数，只对最后几层进行fine-tuning,<br>对于上面两种方案有一些微调的小技巧，比如先计算出预训练模型的卷积层对所有训练和测试数据的特征向量，然后抛开预训练模型，只训练自己定制的简配版全连接网络。 这个方式的一个好处就是节省计算资源，每次迭代都不会再去跑全部的数据，而只是跑一下简配的全连接</p>
<p>Pretrained models<br>这个其实和第二种是一个意思，不过比较极端，使用整个pre-trained的model作为初始化，然后fine-tuning整个网络而不是某些层，但是这个的计算量是非常大的,就只相当于做了一个初始化。</p>
<p>如果数据集大小不同的话，可以在最后的fc层之前添加卷积或者pool层，使得最后的输出与fc层一致，但这样会导致准确度大幅下降，所以不建议这样做<br>对于不同的层可以设置不同的学习率，一般情况下建议，对于使用的原始数据做初始化的层设置的学习率要小于（一般可设置小于10倍）初始化的学习率，这样保证对于已经初始化的数据不会扭曲的过快，而使用初始化学习率的新层可以快速的收敛。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 微调实例</span></span><br><span class="line"><span class="comment"># https://handbook.pytorch.wiki/chapter4/4.1-fine-tuning.html</span></span><br></pre></td></tr></table></figure>

<h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>我们的结构化数据，一般都是一个csv文件或者数据库中的一张表格，所以对于结构化的数据，我们直接使用pasdas库处理就可以了</p>
<p>对于模型的训练，只能够处理数字类型的数据，所以这里面我们首先要将数据分成三个类别 - 训练的结果标签：即训练的结果，通过这个结果我们就能够明确的知道我们这次训练的任务是什么，是分类的任务，还是回归的任务。 - 分类数据：这类的数据是离散的，无法通过直接输入到模型中进行训练，所以我们在预处理的时候需要优先对这部分进行处理，这也是数据预处理的主要工作之一 - 数值型数据：这类数据是直接可以输入到模型中的，但是这部分数据有可能还是离散的，所以如果需要也可以对其进行处理，并且处理后会对训练的精度有很大的提升</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读入文件</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./data/adult.csv&#x27;</span>)</span><br><span class="line"><span class="comment">#salary是这个数据集最后要分类的结果</span></span><br><span class="line">df[<span class="string">&#x27;salary&#x27;</span>].unique()</span><br><span class="line"></span><br><span class="line">array([<span class="string">&#x27;&gt;=50k&#x27;</span>, <span class="string">&#x27;&lt;50k&#x27;</span>], dtype=<span class="built_in">object</span>)</span><br><span class="line"><span class="comment">#查看数据类型</span></span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment">#pandas的describe可以告诉我们整个数据集的大概结构，是非常有用的</span></span><br><span class="line">df.describe()</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看一共有多少数据</span></span><br><span class="line"><span class="built_in">len</span>(df)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练结果</span></span><br><span class="line">result_var = <span class="string">&#x27;salary&#x27;</span></span><br><span class="line"><span class="comment">#分类型数据</span></span><br><span class="line">cat_names = [<span class="string">&#x27;workclass&#x27;</span>, <span class="string">&#x27;education&#x27;</span>, <span class="string">&#x27;marital-status&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;relationship&#x27;</span>, <span class="string">&#x27;race&#x27;</span>,<span class="string">&#x27;sex&#x27;</span>,<span class="string">&#x27;native-country&#x27;</span>]</span><br><span class="line"><span class="comment">#数值型数据</span></span><br><span class="line">cont_names = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;fnlwgt&#x27;</span>, <span class="string">&#x27;education-num&#x27;</span>,<span class="string">&#x27;capital-gain&#x27;</span>,<span class="string">&#x27;capital-loss&#x27;</span>,<span class="string">&#x27;hours-per-week&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看分类类型数据数量和分布</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    <span class="keyword">if</span> col <span class="keyword">in</span> cat_names:</span><br><span class="line">        ccol=Counter(df[col])</span><br><span class="line">        <span class="built_in">print</span>(col,<span class="built_in">len</span>(ccol),ccol)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\r\n&quot;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 缺失数据的填充</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    <span class="keyword">if</span> col <span class="keyword">in</span> cat_names:</span><br><span class="line">        df[col].fillna(<span class="string">&#x27;---&#x27;</span>)</span><br><span class="line">        df[col] = LabelEncoder().fit_transform(df[col].astype(<span class="built_in">str</span>))</span><br><span class="line">    <span class="keyword">if</span> col <span class="keyword">in</span> cont_names:</span><br><span class="line">        df[col]=df[col].fillna(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment">#分割下训练数据和标签</span></span><br><span class="line">Y = df[<span class="string">&#x27;salary&#x27;</span>]</span><br><span class="line">Y_label = LabelEncoder()</span><br><span class="line">Y=Y_label.fit_transform(Y)</span><br><span class="line">Y</span><br><span class="line"></span><br><span class="line">X=df.drop(columns=result_var)</span><br><span class="line">X</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tabularDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X, Y</span>):</span><br><span class="line">        self.x = X.values</span><br><span class="line">        self.y = Y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.x[idx], self.y[idx])</span><br><span class="line"></span><br><span class="line">train_ds = tabularDataset(X, Y)</span><br><span class="line"><span class="comment"># 可以直接使用索引访问定义好的数据集中的数据</span></span><br><span class="line">train_ds[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型和训练 https://handbook.pytorch.wiki/chapter5/5.2-Structured-Data.html#:~:text=3.9000e%2B01%5D)%2C%0A%201)-,%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B,-%E6%95%B0%E6%8D%AE%E5%B7%B2%E7%BB%8F%E5%87%86%E5%A4%87</span></span><br></pre></td></tr></table></figure>

<p>Fashion MNIST进行分类<br><a target="_blank" rel="noopener" href="https://handbook.pytorch.wiki/chapter5/5.3-Fashion-MNIST.html">https://handbook.pytorch.wiki/chapter5/5.3-Fashion-MNIST.html</a></p>
<p>树莓派上编译安装pytorch<br><a target="_blank" rel="noopener" href="https://handbook.pytorch.wiki/pi/readme.html">https://handbook.pytorch.wiki/pi/readme.html</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://chierhy.github.io">神经蛙</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://chierhy.github.io/2023/03/03/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/19-pytorch-2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">https://chierhy.github.io/2023/03/03/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/19-pytorch-2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://chierhy.github.io" target="_blank">chiblog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI%F0%9F%91%BE/">AI👾</a><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0%F0%9F%8E%AB/">笔记🎫</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/code%F0%9F%92%BB/">code💻</a></div><div class="post_share"><div class="social-share" data-image="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/03/03/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/20-pytorch-3-%E5%8F%AF%E8%A7%86%E5%8C%96/"><img class="prev-cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">pytorch-3-可视化</div></div></a></div><div class="next-post pull-right"><a href="/2023/02/28/%E6%8A%80%E6%9C%AF%E5%B7%A5%E5%85%B7/5-%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/"><img class="next-cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">科研工具</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/02/25/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/18-Pytorch-1-%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" title="Pytorch-1-入门笔记"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-25</div><div class="title">Pytorch-1-入门笔记</div></div></a></div><div><a href="/2023/03/03/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/20-pytorch-3-%E5%8F%AF%E8%A7%86%E5%8C%96/" title="pytorch-3-可视化"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-03</div><div class="title">pytorch-3-可视化</div></div></a></div><div><a href="/2022/11/15/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/2-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-Pytorch%20CNN%20model/" title="学习记录-Pytorch CNN model"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-15</div><div class="title">学习记录-Pytorch CNN model</div></div></a></div><div><a href="/2023/02/05/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/16-Quantum%20Machine%20Learning/" title="Quantum Machine Learning"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-05</div><div class="title">Quantum Machine Learning</div></div></a></div><div><a href="/2022/11/17/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="深度学习"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-17</div><div class="title">深度学习</div></div></a></div><div><a href="/2023/09/12/%E7%94%9F%E6%B4%BB/%E6%88%91%E8%AF%BB=%E8%99%9A%E6%8B%9F%E6%98%BE%E7%A4%BA%EF%BC%9A%E5%BC%95%E9%A2%86%E6%9C%AA%E6%9D%A5%E7%9A%84%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E9%9D%A9%E5%91%BD/" title="我读&#x3D;虚拟显示：引领未来的人机交互革命"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-12</div><div class="title">我读&#x3D;虚拟显示：引领未来的人机交互革命</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://github.com/chierhy/chierhy.github.io/blob/master/%E9%99%84%E4%BB%B6/1.jpg?raw=true" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">神经蛙</div><div class="author-info__description">chierhy@163.com</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chierhy"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#cnn"><span class="toc-number">1.</span> <span class="toc-text">cnn</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#rnn"><span class="toc-number">2.</span> <span class="toc-text">rnn</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#logistic%E5%9B%9E%E5%BD%92"><span class="toc-number">3.</span> <span class="toc-text">logistic回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">4.</span> <span class="toc-text">数据处理</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/04/12/%E6%8A%80%E6%9C%AF%E5%B7%A5%E5%85%B7/1-%E6%9C%8D%E5%8A%A1%E5%99%A8-GPU/" title="服务器-GPU"><img src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="服务器-GPU"/></a><div class="content"><a class="title" href="/2025/04/12/%E6%8A%80%E6%9C%AF%E5%B7%A5%E5%85%B7/1-%E6%9C%8D%E5%8A%A1%E5%99%A8-GPU/" title="服务器-GPU">服务器-GPU</a><time datetime="2025-04-12T09:49:28.477Z" title="发表于 2025-04-12 17:49:28">2025-04-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/12/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" title="method-知识蒸馏"><img src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="method-知识蒸馏"/></a><div class="content"><a class="title" href="/2025/04/12/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" title="method-知识蒸馏">method-知识蒸馏</a><time datetime="2025-04-12T09:49:26.684Z" title="发表于 2025-04-12 17:49:26">2025-04-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/12/7/" title="无题"><img src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2025/04/12/7/" title="无题">无题</a><time datetime="2025-04-12T09:48:44.496Z" title="发表于 2025-04-12 17:48:44">2025-04-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025 By 神经蛙</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">See you!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>