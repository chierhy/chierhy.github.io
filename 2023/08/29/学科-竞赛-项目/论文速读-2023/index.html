<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>论文速读-2023 | chiblog</title><meta name="author" content="神经蛙"><meta name="copyright" content="神经蛙"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一些有意思的文章速读">
<meta property="og:type" content="article">
<meta property="og:title" content="论文速读-2023">
<meta property="og:url" content="https://chierhy.github.io/2023/08/29/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AE%BA%E6%96%87%E9%80%9F%E8%AF%BB-2023/index.html">
<meta property="og:site_name" content="chiblog">
<meta property="og:description" content="一些有意思的文章速读">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg">
<meta property="article:published_time" content="2023-08-29T09:33:00.000Z">
<meta property="article:modified_time" content="2023-12-26T13:17:33.000Z">
<meta property="article:author" content="神经蛙">
<meta property="article:tag" content="笔记🎫">
<meta property="article:tag" content="论文✏">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg"><link rel="shortcut icon" href="https://pic4.zhimg.com/v2-da217cabde0a382120e68118571d60e3_r.jpg"><link rel="canonical" href="https://chierhy.github.io/2023/08/29/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AE%BA%E6%96%87%E9%80%9F%E8%AF%BB-2023/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文速读-2023',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-26 21:17:33'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="chiblog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic4.zhimg.com/v2-da217cabde0a382120e68118571d60e3_r.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">chiblog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">论文速读-2023</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-29T09:33:00.000Z" title="发表于 2023-08-29 17:33:00">2023-08-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-26T13:17:33.000Z" title="更新于 2023-12-26 21:17:33">2023-12-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E7%A7%91/">学科</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">49k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>156分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文速读-2023"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="2023-8"><a href="#2023-8" class="headerlink" title="2023.8"></a>2023.8</h1><p>Improved 3D Morphable Model for Facial Action Unit Synthesis<br>为了克服传统三维人脸模型在局部面部表情动作合成方面的局限性，该文提出一种结合面部动作编码系统（FACS）和三维可变形模型（3DMM）的改进三维人脸模型。我们提出的3D面部模型可用于具有局部动作单元（AU）的3D面部表情合成。具体来说，AU作为先验知识引入3D面部模型，以捕获不同面部表情的解剖学定义的肌肉运动。我们提出的模型提取了单个AU的参数，这些参数也可用于表示AU信息，以促进AU识别。定性和定量实验结果表明，我们提出的模型可以生成具有特定AU标签的3D人脸。我们提出的模型的这些AU参数在AU分类中的表现优于传统3DMM的全局表达参数。</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/149928673">微表情识别（Micro expression recognition）简述 - 知乎 (zhihu.com)</a></p>
<p>Facial Action Units Detection to Identify Interest Emotion: An Application of Deep Learning<br>我们的系统允许检测从输入图像中定义兴趣情感的动作单元。换句话说,我们的分类器区分感兴趣的面部动作和其他情感状态。</p>
<p>Having Difficulties Reading the Facial Expression of Older Individuals? Blame It on the Facial Muscles, Not the Wrinkles<br>因此,面部肌肉锻炼可能提高老年人传达面部情绪表情的能力。</p>
<h1 id="2023-10"><a href="#2023-10" class="headerlink" title="2023.10"></a>2023.10</h1><p>文献对应的期刊可见链接最后几位<br><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544548.3581254">Toucha11y: Making Inaccessible Public Touchscreens Accessible | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>2023 CHI<br>尽管它们越来越受欢迎，但许多带有触摸屏的公共信息亭对盲人来说是无法访问的。Toucha11y是一个工作原型，它允许盲人用户独立使用现有的不可访问的触摸屏信息亭，并且几乎不费力气。Toucha11y由一个机械机器人组成，盲人用户可以将其检测到任意的触摸屏信息亭，以及他们智能手机上的配套应用程序。该机器人一旦连接到触摸屏上，将识别其内容，从数据库中检索相应的信息，并在用户的智能手机上呈现。因此，盲人可以使用智能手机内置的辅助功能来访问内容并进行选择。机械机器人将检测并激活相应的触摸屏界面。我们展示Toucha11y的系统设计以及一系列技术评估。通过用户研究，我们发现Toucha11y可以帮助盲人用户操作不可访问的触摸屏设备。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544548.3581088">Quantified Canine: Inferring Dog Personality From Wearables | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>能够评估狗的个性可以用来，例如，将收容所的狗与未来的主人匹配，并个性化狗的活动。这样的评估通常依赖于专家或管理给狗主人的心理量表，这两者都很昂贵。为了应对这一挑战，我们制造了一种名为“补丁守护者”的设备，可以绑在宠物的胸部，并通过加速度计和陀螺仪测量活动。在涉及12只健康狗的野外部署中，我们从两份经过验证的问卷中收集了1300小时的传感器活动数据和狗的个性测试结果。通过匹配这两个数据集，我们训练了十个机器学习分类器，从活动数据中预测狗的个性，在[0.63-0.90]中实现了AUC，这表明了使用可穿戴技术跟踪宠物心理信号的价值。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544548.3580807">Living with Light Touch | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>我们是一对母子，在相隔5000多英里的地方，我们一直在使用一对简单的自建通讯设备来保持联系的感觉。这些设备被称为Light Touch，只允许我们互相发送慢慢褪色的彩色灯光，但我们惊讶于我们与它们的持续互动对我们的意义。本文对我们过去两年的经历进行了自我人种学描述，包括我们使用这些设备的最初经历，并关注我们日常使用的各个方面。根据我们的观察，我们讨论了被证明对调解我们的联系感觉很重要的特征。然而，我们指出，它们的成功取决于我们的使用环境和我们联系的性质，并建议像轻触这样的简单系统可能支持情感交流，但前提是它们与环境和关系相匹配。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544548.3580906">Smartphone-derived Virtual Keyboard Dynamics Coupled with Accelerometer Data as a Window into Understanding Brain Health | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>我们研究了使用专门在定制智能手机键盘上打字时收集的加速度计数据来研究打字动态是否与情绪和认知的日常变化相关的可行性。作为一项正在进行的涉及情绪障碍的数字心理健康研究的一部分，我们从一个特征明确的临床样本（N&#x3D;85）中收集了数据，并将每次打字时的加速度计数据分为方向（直立与不直立）和运动（主动与不主动）。情绪障碍组尽管症状轻微（抑郁&#x2F;躁狂），但认知表现较低。在认知表现方面也存在昼夜模式差异：认知表现较高的个体打字速度更快，对一天中的时间不太敏感。他们在智能手机键盘使用方面也表现出更明确的昼夜模式：与认知能力较低的人相比，他们白天更多地使用键盘，晚上更多地减少使用，这表明他们的手机使用更健康。</p>
<p> [Sensing with Earables: A Systematic Literature Review and Taxonomy of Phenomena<br>    综述了earbles相关领域，不同应用、不同传感器。<br>    图做的很不错。看的时候看到很多相关参考论文可以记录下来之后看，比大海捞针的找方便。<br>    笔记记录在综述那篇。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3569483">Generalization and Personalization of Mobile Sensing-Based Mood Inference Models: An Analysis of College Students in Eight Countries: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 6, No 4</a><br>在过去十年中，ubicomp文献中已经研究了使用移动传感数据进行情绪推断。这种推断能够在一般移动应用程序中实现上下文感知和个性化的用户体验，并在移动健康应用程序中实现有价值的反馈和干预。然而，尽管模型泛化问题在许多研究中得到了强调，但重点一直是提高使用不同传感模式和机器学习技术的模型的准确性，在同质人群中收集数据集。相比之下，很少关注研究情绪推理模型的性能，以评估模型是否泛化到新的国家。在这项研究中，我们收集了来自八个国家（中国，丹麦，印度，意大利，墨西哥，蒙古，巴拉圭，英国）的678名参与者的329K自我报告的移动传感数据集，以评估地理多样性对情绪推理模型的影响。我们定义和评估国家特定（在一个国家内训练和测试）、大陆特定（在一个大陆内训练和测试）、国家不可知论（在训练数据上没有看到的国家进行测试）和多国（在多个国家进行训练和测试）方法，这些方法针对人口水平（非个性化）和混合（部分个性化）模型的两个情绪推理任务进行了传感器数据训练。我们表明，部分个性化的国家特定模型在两类（负价对正价）和三类（负价对中性价对正价）推断的0.76-0.94范围内的接收器操作特征曲线（AUROC）得分下表现最佳屈服区域。此外，通过国家不可知论方法，我们表明，即使模型是部分个性化的，模型与国家特定设置相比也表现不佳。我们还表明，在欧洲的情况下，特定大陆的模型优于多国模型。总的来说，我们揭示了情绪推理模型对新国家的泛化问题，以及国家的地理相似性如何影响情绪推理。</p>
<h1 id="2023-11"><a href="#2023-11" class="headerlink" title="2023.11"></a>2023.11</h1><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544548.3581397">GestureMeter: Design and Evaluation of a Gesture Password Strength Meter | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>在触摸屏上绘制的手势已被提议作为一种身份验证方法，以确保对智能手机的访问安全。它们提供了良好的可用性和理论上较大的密码空间。然而，最近的工作表明，用户倾向于选择简单或相似的手势作为他们的密码，使他们容易受到基于字典的猜测攻击。为了提高他们的安全性，本文描述了一种新颖的手势密码强度计，它基于一种评分算法交互式地提供安全评估和改进建议，该算法结合了概率模型、手势字典和一组新颖的笔画启发式方法。我们在在线和离线设置中评估了这个系统，并表明它支持创建明显更能抵抗猜测攻击的手势（高达67%），同时还保持了召回成功率和时间等可用性指标的性能。我们得出结论，手势密码强度计可以帮助用户选择更安全的手势密码。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544548.3581419">Squeez’In: Private Authentication on Smartphones based on Squeezing Gestures | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>在本文中，我们提出了Squeez’In，这是一种智能手机上的技术，通过以独特的模式握住和挤压手机来实现私人身份验证。我们首先通过分析参与者自行设计的手势和挤压行为，探索了用于身份验证的实用挤压手势的设计空间。结果表明，具有两个级别的触摸压力和持续时间的变长手势是最自然和明确的。然后，我们在现成的电容感应智能手机上实施了Squeez’In，并采用了SVM-GBDT模型来识别手势和用户特定的行为模式，在21个用户上进行测试时实现了99.3%的准确率和0.93 F1分数。随后为期14天的研究验证了Squeez’In的记忆性和长期稳定性。在可用性评估期间，与手势和pin码相比，Squeez’In在隐私和安全性方面实现了明显更快的身份验证速度和更高的用户偏好。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3581791.3596856">Wireless earbuds for low-cost hearing screening | Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services (acm.org)</a><br>我们展示了第一款可以通过检测耳声发射来进行听力筛查的无线耳塞硬件。传统观点认为，检测耳声发射，即耳蜗产生的微弱声音，需要敏感且昂贵的声学硬件。因此，用于听力筛查的医疗设备价值数千美元，在中低收入国家无法使用。我们证明，通过使用低成本声学硬件设计无线耳塞，并将其与无线传感算法相结合，我们可以可靠地识别耳声发射并进行听力筛查。我们的算法将调频啁啾声与低成本扬声器发出的宽带脉冲相结合，以可靠地将耳声发射与入耳反射和回声分开。我们在两个医疗保健场所对50只耳朵进行了临床研究。我们的研究表明，低成本耳塞以100%的灵敏度和89.7%的特异性检测听力损失，这与8000美元的医疗设备的性能相当。通过开发低成本和开源的可穿戴技术，我们的工作可能有助于通过使这些医疗设备民主化来解决听力筛查中的全球健康不平等问题。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3581791.3596848">Passive Vital Sign Monitoring via Facial Vibrations Leveraging AR&#x2F;VR Headsets | Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services (acm.org)</a>利用AR&#x2F;VR耳机通过面部振动进行被动生命体征监测<br>生命体征（例如，呼吸和心率）和个人身份是个性化医疗和保健的基本信息。增强现实&#x2F;虚拟现实（AR&#x2F;VR）的流行为在广泛的场景中实现长期健康监测提供了绝佳的机会，包括虚拟娱乐、教育和远程医疗。然而，商用现货 AR&#x2F;VR设备没有专用的生物传感器来提供生命体征和个人身份。在这项工作中，我们提出了一个新颖的框架，可以通过AR&#x2F;VR设备上的被动感知生成细粒度的生命体征信号和其他AR&#x2F;VR用户的个性化健康信息。特别是，我们发现用户由呼吸和心跳引起的微小面部振动可以影响AR&#x2F;VR耳机上现成的运动传感器，这些传感器编码了丰富的生命体征模式和独特的生物识别技术。所提出的框架进一步估计呼吸和心跳速率，检测性别和身份，并导出用户的体脂百分比。为了减轻身体运动的影响，我们设计了一种自适应过滤方案来取消自发和非自发运动伪影。我们还开发了独特的面部振动特征和深度学习技术，以促进生命体征信号重构和用户识别。广泛的实验表明，我们的框架可以实现生命体征信号重构和速率测量的低误差，以及身份和性别识别的95.51%和93.33%的准确率。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544548.3580775">LipIO: Enabling Lips as both Input and Output Surface | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>摘要。我们设计了LipIO，这是一种新颖的设备，使嘴唇能够同时用作输入和输出表面。LipIO包括两个重叠的柔性电极阵列：一个用于电容式触摸的外向阵列和一个用于电触觉刺激的面向嘴唇的阵列。佩戴LipIO时，用户通过嘴唇刺激感受界面的状态，并通过用舌头或相对的嘴唇触摸嘴唇来做出反应。更重要的是，LipIO提供了位于同一位置的触觉反馈，允许用户感受他们在嘴唇中的哪个位置触摸——这是实现眼睛和双手自由交互的关键。我们的三项研究验证了参与者在嘴唇上感知到的电触觉输出，随后用舌头触摸目标位置，平均准确率为93%，而佩戴LipIO时带有五个I&#x2F;O电极，具有位于同一位置的反馈。最后，我们展示了LipIO在四个示例性应用中的潜力，这些应用说明了它如何实现新型的免眼和免提微交互。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610895">PoseSonic: 3D Upper Body Pose Estimation Through Egocentric Acoustic Sensing on Smartglasses: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>在本文中，我们介绍了PoseSonic，一种用于估计上身姿势的智能眼镜智能声学传感解决方案。我们的系统只需要眼镜铰链上的两对麦克风和扬声器就可以发出FMCW编码的听不见的声学信号，并接收反射信号用于身体姿势估计。使用定制的深度学习模型，PoseSonic估计包括肩膀、肘部、手腕、臀部和鼻子在内的9个身体关节的3D位置。我们采用跨模态监督策略，使用同步的RGB视频帧作为地面实况来训练我们的模型。我们对22名参与者进行了实验室和半野外用户研究，以评估PoseSonic，当预测3D中的9个身体关节位置时，我们的用户独立模型在实验室环境中实现了6.17厘米的平均每个关节位置误差，在半野外环境中实现了14.12厘米的平均每个关节位置误差。我们的进一步研究表明，性能不受不同环境、设备重新安装或现实世界环境噪音的显著影响。最后，我们讨论了在现实世界应用中部署PoseSonic的机遇、挑战和局限性。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610878">LapTouch: Using the Lap for Seated Touch Interaction with HMDs: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>坐着时使用虚拟现实很常见，但是除了使用控制器或手势之外，关于坐着交互的研究很少。这项工作展示了LapTouch，它利用膝盖作为触摸界面，包括两项用户研究，为使用膝盖的直接和间接触摸交互的设计提供指导用户触摸的视觉反馈，以及没有向用户提供这种视觉反馈的无眼交互。第一项研究表明，直接交互可以提供95%准确率的有效布局，布局高达4×4，完成时间更短，而间接交互可以提供有效布局，布局高达4×5，但完成时间更长。考虑到用户体验，这表明4行和5列布局不是首选，建议使用最大为3×4布局的直接和间接交互。根据第二项研究，增加与支持向量机（SVM）的无眼交互允许使用广义模型的2×2布局和使用个性化模型的2×2、2×3和3×2布局。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610910">CASES: A Cognition-Aware Smart Eyewear System for Understanding How People Read: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>阅读的过程吸引了数十年的科学研究。该领域的工作主要集中在使用眼睛注视模式来揭示阅读时的认知过程。然而，眼睛注视模式遭受有限的分辨率、抖动噪声和认知偏差，导致跟踪认知阅读状态的准确性有限。此外，仅使用顺序眼睛注视数据忽略了文本的语言结构，破坏了为阅读过程中的认知状态提供语义解释的尝试。受文本语义语境对人类认知阅读过程的影响的激励，这项工作同时使用文本的语义语境和阅读过程中的视觉注意来更准确地预测认知状态的时间序列。为此，我们提出了一种认知感知智能眼镜系统（CASES），它在阅读过程中融合了语义语境和视觉注意模式。这两种特征模式是时间对齐的，并馈送到基于时间卷积网络的多任务分类深度模型，以自动估计并进一步语义解释阅读状态时间序列。CASES在眼镜中实现，其使用不会中断阅读过程，从而减少主观偏见。此外，视觉和语义信息之间的实时关联使视觉注意和语义上下文之间的相互作用能够得到更好的解释和解释。对25名受试者的烧蚀研究表明，与单独进行眼动追踪相比，CASES将句子的多标签阅读状态估计准确率提高了20.90%。使用CASES，我们开发了一个交互式阅读辅助系统。三个半月的部署与13项现场研究实现了与阅读研究相关的几个观察结果。特别是，观察到个人视觉历史如何在不同的文本粒度下与语义上下文交互。此外，当读者遇到处理困难时，CASES能够及时干预，从而促进对阅读所涉及的认知过程的自我意识，并帮助培养更有效的阅读习惯。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610907">VAX: Using Existing Video and Audio-based Activity Recognition Models to Bootstrap Privacy-Sensitive Sensors: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>考虑到数据的丰富性和使用大量标记训练数据的预训练ML模型的可用性，人类活动识别（HAR）的音频和视频模式的使用很常见。然而，音频和视频传感器也导致了严重的消费者隐私问题。因此，研究人员探索了隐私侵入性较小的替代模式，如毫米波多普勒雷达、IMU、运动传感器。然而，这些方法的关键限制是它们中的大多数不容易跨环境泛化，并且需要大量的原位训练数据。最近的工作提出了跨模态迁移学习方法，以缓解缺乏经过训练的标记数据并取得了一些成功。在本文中，我们推广了这一概念，创建了一个名为VAX（Video&#x2F;Audio to’X’）的新系统，其中从现有Video&#x2F;Audio ML模型中获取的训练标签用于为各种“X”隐私敏感传感器训练ML模型。值得注意的是，在VAX中，一旦隐私敏感传感器的ML模型经过训练，几乎没有用户参与，音频&#x2F;视频传感器就可以完全移除，以更好地保护用户的隐私。我们在十名参与者的家中构建和部署了VAX，同时他们进行了17项日常生活中的常见活动。我们的评估结果表明，经过训练，VAX可以使用其板载摄像头和麦克风检测17项活动中的大约15项，平均准确率为90%。对于可以使用摄像头和麦克风检测到的这些活动，VAX为隐私保护传感器训练每个家庭的模型。这些模型（平均准确率&#x3D;84%）不需要原位用户输入。此外，当VAX仅针对VAX A&#x2F;V管道未检测到的活动增加一个标记实例时（17个中的约2个），它可以以84%的平均准确率检测所有17个活动。我们的结果表明，VAX明显优于在每个家庭中每个活动使用一个标记实例的基线监督学习方法（平均准确率为79%），因为VAX将用户提供活动标签的负担减少了8倍（~2个标签对17个标签）。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610909">HMGAN: A Hierarchical Multi-Modal Generative Adversarial Network Model for Wearable Human Activity Recognition: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>可穿戴人类活动识别（WHAR）是泛在和移动计算的一个重要研究领域。深度WHAR模型因缺乏大量和种类繁多的标记数据而遭受过拟合问题，通常通过生成数据来扩大训练集，即数据增强（DA）来解决。生成对抗网络（GAN）已经显示出其卓越的数据生成能力，基于GAN的DA可以提高分类模型的泛化能力。然而，现有的GAN不能充分利用重要的模态信息，未能平衡模态细节和全局一致性，无法满足深度多模态WHAR的要求。本文针对WHAR提出了一种<strong>分层多模态</strong>GAN模型（HMGAN）。HMGAN由多个模态生成器、一个分层判别器和一个辅助分类器组成。多模态生成器可以学习传感器数据的复杂多模态数据分布。分层判别器可以为低级模态判别损失和高级整体判别损失提供判别输出，以在模态细节和全局一致性之间取得平衡。在五个公共WHAR数据集上的实验表明，HMGAN实现了WHAR的最先进性能，在准确率、宏观F1分数和加权F1分数方面分别平均优于最佳基线3.4%、3.8%和3.5%。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610894">VibPath: Two-Factor Authentication with Your Hand’s Vibration Response to Unlock Your Phone: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>智能设备市场的技术进步使智能手机成为我们生活的核心，保证了越来越安全的身份验证手段。尽管大多数智能手机都采用了基于生物识别的身份验证，但在几次失败的尝试后，大多数用户都可以选择使用密码快速绕过系统。为了增加一层安全性，双重验证（2FA）已经实施，但已被证明容易受到各种攻击。在本文中，我们介绍了VibPath，这是一种同时2FA方案，可以通过触摸行为了解用户的手神经肌肉系统。VibPath通过基于注意力的编码器-解码器网络捕获<strong>个人手和手腕之间的振动路径响应</strong>，不显眼地从冒名顶替者那里验证真正的用户。在一项有30名参与者参与的用户研究中，VibPath的平均性能为0.98准确率、0.99准确率、0.98召回率、0.98 f1分数的用户验证和94.3%的用户识别准确率。此外，我们还进行了几项广泛的研究，包括预期、持久性、脆弱性、可用性和系统开销研究，从多个方面评估VibPath的实用性和可行性。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610925">Can You Ear Me?: A Comparison of Different Private and Public Notification Channels for the Earlobe: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>耳垂是佩戴珠宝的著名位置，但也可能有希望用于电子输出，例如呈现通知。这项工作详细阐述了耳垂不同通知渠道的利弊。耳垂上的通知可以是私人的（只有佩戴者才能注意到），也可以是公共的（在给定的社交场合中在附近可以注意到）。一项有18名参与者的用户研究表明，私人渠道（戳、振动、私人声音、电触觉）的反应时间平均不到1秒，错误率（错过通知）不到1%。热暖和冷花费的时间明显更长，冷最不可靠（26%的错误率）。参与者更喜欢电触觉和振动。在公共频道中，声音（738毫秒）和发光二极管（828毫秒）之间的识别时间没有显著差异，但显示需要更长的时间（3175毫秒）。在22%的时候，显示的错误率最高。参与者普遍觉得在耳垂上佩戴通知设备很舒服。结果表明，耳垂确实是可穿戴技术的合适位置，如果适当地小型化，这对于电触觉和发光二极管是可能的。我们展示了应用场景并讨论了设计考虑因素。在健身中心进行的一项小型实地研究证明了耳垂通知概念在运动环境中的适用性。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610887">Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning | Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</a><br>Amazon Echo和Google Home等智能扬声器语音助手（VA）因其与智能家居设备和物联网（IoT）技术的无缝集成而被广泛采用。这些VA服务引发了<strong>隐私</strong>问题，特别是由于它们可以访问我们的语音。这项工作考虑了一个这样的用例：通过语音情绪识别（SER）对用户的情绪进行不负责任和未经授权的监视。本文介绍了DARE-GP，这是一种解决方案，它可以<strong>创建附加噪声来掩盖用户的情绪信息</strong>，同时保留他们语音的transcription-relevant部分。DARE-GP通过使用受限的基因编程方法来学习描绘目标用户情绪内容的频谱频率特征，然后生成提供这种隐私保护的通用对抗性音频扰动来做到这一点。与现有作品不同，DARE-GP提供了：a）对以前听不到的话语的实时保护，b）对以前看不见的黑盒SER分类器的保护，c）同时保护语音转录，以及d）在现实的声学环境中这样做。此外，这种规避对知识渊博的对手采用的防御是强大的。这项工作的评估最终是对两个现成的商业智能扬声器进行声学评估，使用与唤醒词系统集成的小尺寸（覆盆子派）来评估其真实世界的实时部署效果。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610930">Wi-Flex: Reflex Detection with Commodity WiFi: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>在本文中，我们对WiFi信号的惊吓反射检测感兴趣。我们提出，与接收信号带宽相关的两个参数，最大归一化带宽和带宽密集持续时间，可以成功地检测反射，并将其与非反射事件区分开来，甚至与那些涉及剧烈身体运动（例如某些运动）的事件区分开来。为了证实这一点，我们需要一个庞大的射频反射数据集，收集起来非常费力。另一方面，网上有许多可用的反射&#x2F;非反射视频。然后，我们提出了一种有效的方法，将视频内容转换为相应的接收射频信号的带宽，如果视频中的事件附近有一个链接，就会测量到相应的接收射频信号的带宽，通过在我们的问题和J. Carson在模拟FM无线电背景下的经典带宽建模工作（Carson’s规则）之间进行类比。然后，这使我们能够将在线反射&#x2F;非反射视频转换为即时的大射频带宽数据集，并相应地表征最佳的2D反射&#x2F;非反射决策区域，以便在WiFi的实际操作中使用。我们通过203个反射事件、322个非反射事件（包括142个剧烈的身体运动事件）、超过四个区域（包括几个穿墙事件）和15名参与者广泛测试我们的方法，实现了90.15%的正确反射检测率和2.49%的误报率（所有事件都是自然的）。虽然该论文通过惊吓反射进行了广泛的测试，但它也适用于运动类型的反射，因此也通过与运动相关的反射进行了测试。我们进一步展示了多人同时从事一系列活动的反射检测。还通过实验证明了所提议设计的最佳性。最后，我们进行了实验，通过量化守门员的反应来展示我们的方法在体育运动中提供具有成本效益和可量化指标的潜力。总的来说，我们的结果证实了一个快速、强大且具有成本效益的反射检测系统，而无需收集任何射频训练数据或训练神经网络。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610927">SmarCyPad: A Smart Seat Pad for Cycling Fitness Tracking Leveraging Low-cost Conductive Fabric Sensors: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>骑自行车是提高一个人整体健康水平的有效方法，如心血管健康、耐力、较低的身体力量和体脂百分比。为了提高健身性能，实时自行车健身跟踪不仅可以让骑自行车的人更好地控制他们的能量输出，还有助于推动锻炼强度，并让用户对他们的健身进度负责。然而，现有的自行车传感器（例如，安装在自行车轮毂或曲柄臂上的传感器）<strong>仅限于测量自行车的节奏和速度</strong>。尽管最近几项依赖身体传感器或摄像头的研究可以提供更细粒度的信息（例如，骑行位置和膝关节角度），但它们要么需要不方便的设置，要么会引起严重的隐私问题。为了规避这些限制，在本文中，我们提出了SmarCyPad，这是一种创新的<strong>智能座垫</strong>，可以连续且不显眼地跟踪五个骑行特定指标，包括节奏、单腿稳定性、腿部力量平衡、骑行位置和骑行者的膝关节角度。具体来说，我们在座垫中嵌入导电织物传感器，以感知骑行者臀部肌肉施加到自行车座椅上的压力。开发了一系列信号处理算法，以根据感知到的压力信号估计踏板周期，并进一步导出骑行节奏、单腿稳定性和腿部力量平衡。此外，我们利用深度学习模型来检测骑行者的骑行位置，并通过线性回归重建骑行者的膝关节角度。传感器和系统原型利用现成材料从头开始制造，总成本不到50美元。涉及15名参与者的广泛实验表明，SmarCyPad可以准确估计骑行节奏，平均误差为每分钟1.13轮，量化每条腿的骑行稳定性，检测骑行不平衡，以96.60%的准确率区分五个骑行位置，并连续跟踪膝关节角度，平均误差低至9.58度。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610900">Headar: Sensing Head Gestures for Confirmation Dialogs on Smartwatches with Wearable Millimeter-Wave Radar: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>点头和摇头是交流中直观且通用的手势。随着智能手表通过用户活动感知技术的进步变得越来越智能，智能手表的许多使用场景要求用户在确认对话框中快速响应，以接受或拒绝建议的操作。这些建议的操作包括拨打紧急电话、接受服务建议以及启动或停止锻炼计时器。在这些场景中，<strong>头部手势</strong>可能比触摸交互更可取，因为它免提且易于执行。我们建议Headar使用<strong>可穿戴毫米波</strong>感应来识别智能手表上的这些手势。我们首先调查头部手势以了解它们在对话环境中是如何执行的。然后我们调查了用户举起智能手表的位置和方向。这些研究的见解指导了Headar的实施。此外，我们进行了建模和仿真来验证我们的传感原理。我们使用当代深度学习技术开发了一个实时感知和推理管道，并通过用户研究（n&#x3D;15）和现场测试（n&#x3D;8）证明了我们提出的方法的可行性。我们的评估在9个类别的用户研究中产生了84.0%的平均准确率，包括点头和摇晃以及其他7个信号——静止、语音、触摸交互和4个非手势头部运动（即抬头、向左、向右和向下）。此外，我们在现场测试中获得了72.6%的准确率，这揭示了我们的方法在各种现实条件下的性能的丰富见解。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3570361.3613278">Screen Perturbation: Adversarial Attack and Defense on Under-Screen Camera | Proceedings of the 29th Annual International Conference on Mobile Computing and Networking (acm.org)</a><br>智能手机正在走向全屏设计，以获得更好的用户体验。这一趋势迫使前置摄像头被放置在屏幕下，从而导致了屏幕下摄像头（USC）。因此，屏幕的一小块区域被制成半透明，以允许光线到达USC。在本文中，我们利用半透明屏幕的特性来不显眼地修改其像素，人眼无法察觉，但会对USC图像产生扰动。这些屏幕扰动影响图像分类和人脸识别中的深度学习模型。它们可以用来保护用户隐私，或者在恶意情况下破坏前置摄像头的功能。我们设计了两种方法，一像素扰动和多像素扰动，可以在USC捕获的图像中添加屏幕扰动，并成功欺骗各种深度学习模型。我们在测试平台数据集和合成数据集上对三款商用全屏智能手机的评估表明，屏幕扰动显著降低了平均图像分类准确率，单像素扰动从85%下降到仅14%，多像素扰动从5.5%下降。对于人脸识别，平均准确率分别从91%下降到仅1.8%和0.25%。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3581791.3596859">SoundSieve: Seconds-Long Audio Event Recognition on Intermittently-Powered Systems | Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services (acm.org)</a><br>每个intermittently-powered传感系统的一个基本问题是，这些系统在较长时间内获取的信号也是间歇性的。因此，这些系统无法捕获跨越存储收集能量的电容器的多个充放电周期的较长持续时间事件的一部分。从应用程序的角度来看，这被视为输入数据中缺失值的零星突发——使用统计插值或插值方法可能无法恢复。在本文中，我们根据间歇性音频分类系统研究了这个问题，并设计了一个端到端系统——SoundSieve——它能够准确分类跨越间歇性系统的多个开关周期的音频事件。SoundSieve采用离线音频分析仪，该分析仪学习识别和预测必须采样的音频片段的重要片段，以确保音频的准确分类。在运行时，SoundSieve采用了一个轻量级、能量和内容感知的音频采样器来决定系统何时应该唤醒以捕获下一个音频块；和一个轻量级、间歇性感知的音频分类器，执行插补和设备上的推理。通过使用流行的音频数据集和真实系统进行广泛的评估，我们证明SoundSieve产生的推理结果比最先进的高5%-30%。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3495243.3564146">Enabling secure touch-to-access device pairing based on human body’s electrical response | Proceedings of the 28th Annual International Conference on Mobile Computing And Networking (acm.org)</a><br>最近在设备配对过程中减少用户参与的努力已经成功地引入了触摸访问。为了检测两个设备是否由同一个人持有，现有的触摸访问解决方案从共享信息源中提取特征以生成配对密钥。它们专注于通过仅要求用户简单触摸设备来验证设备的真实性，然而，忽略了设备持有者的合法性和配对意图。此外，配对密钥可能容易受到窃听攻击，因为它们是通过开放的无线链路（例如WiFi或蓝牙）交换的。在本文中，我们开发了一种安全的设备配对机制，该机制本质上使用人体来生成和传输用户特定的配对密钥，确保用户的合法性和配对意图，以及提高密钥传输的可靠性。我们的工作基于这样的观察：人体对流经其间的电信号产生独特的反应，不同的身体对信号诱发出截然不同的反应。设备上的内置麦克风捕捉环境声音作为熵源并将其转换为电信号，随后由人体进行处理和传输，用于设备配对。我们使用现成的麦克风构建了一个原型，并与31名参与者进行了广泛的实验，以评估其安全性能和可用性。结果表明，我们的系统实现了97.74%的配对成功率和2.28%的相等错误率。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610896">TAO: Context Detection from Daily Activity Patterns Using Temporal Analysis and Ontology: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>将细粒度的活动检测（例如，电话铃声、穿插沉默和步行的谈话）转换为语义上有意义且更丰富的上下文信息（例如，锻炼时打电话20分钟）对于实现一系列医疗保健和人机交互应用程序至关重要。之前的工作提出了构建活动模式的本体或时间分析，但在捕获复杂的真实世界上下文模式方面取得的成功有限。我们介绍了TAO，这是一个混合系统，它利用基于OWL的本体和时间聚类方法来检测来自人类活动的高级上下文。TAO可以描述一个接一个发生的连续活动以及交错或并行发生的活动，以比之前的工作更准确地检测更丰富的上下文集。我们在真实世界的活动数据集（Casas和Extra ensory）上评估TAO，并表明我们的系统平均分别实现了87%和80%的上下文检测准确率。我们在现实世界环境中部署和评估TAO，八名参与者每人使用我们的系统三个小时，展示了TAO在现实世界中捕捉语义上有意义的上下文的能力。最后，为了展示上下文的有用性，我们制作了评估生产力和压力的健康应用程序原型，并表明使用TAO提供的上下文计算的健康指标比基线方法（平均在30%以内）更接近地面实况（平均在1.1%以内）。</p>
<p>[Contact Tracing for Healthcare Workers in an Intensive Care Unit | Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies](<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610924">https://dl.acm.org/doi/10.1145/3610924</a><br>接触者追踪是在大流行期间缓解2019冠状病毒病传播的有力工具。一线医护人员在医院单位感染的风险特别高。本文介绍了ContAct TraCing for医院（CATCH），这是一种专为医院环境中的医护人员设计的自动接触者追踪系统。CATCH采用分布在整个医院单位的嵌入式设备，以检测佩戴低功耗蓝牙（BLE）信标的医护人员之间的密切接触者。我们首先确定了一组不同的接触者追踪场景，这些场景基于现实世界中重症加强护理病房（ICU）的不同环境特征以及医护人员在单位内不同空间的不同工作模式。然后，我们开发了一套针对每个场景量身定制的新颖接触者追踪方法。CATCH已在一家主要医疗中心的ICU部署和评估，通过广泛的实验证明了接触者追踪比现有方法具有更高的准确性。此外，真实世界的案例研究强调了CATCH与标准接触者追踪实践相比的有效性和效率。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610921">MicroCam: Leveraging Smartphone Microscope Camera for Context-Aware Contact Surface Sensing: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>这项研究的主要重点是手机与其周围表面之间谨慎而微妙的日常接触交互。这种交互预计将促进移动上下文感知，包括配药更新、智能切换模式（例如，静音模式）或启动命令（例如，停用警报）等方面。我们引入了MicroCam，这是一种基于接触的传感系统，它采用智能手机IMU数据来检测手机放置的常规状态，并利用内置显微镜摄像头来捕捉复杂的表面细节。特别是，收集自然数据集以原位获取真实的表面纹理以进行训练和测试。此外，我们基于持续学习优化了算法的深度神经网络组件，以准确区分对象类别（例如，表格）和材料成分（例如，木材）。实验结果突出了所提出方法的卓越准确性、鲁棒性和通用性。最后，我们以我们的原型为中心进行了全面的讨论，包括系统性能以及潜在的应用程序和场景等主题。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610920">ProxiFit: Proximity Magnetic Sensing Using a Single Commodity Mobile toward Holistic Weight Exercise Monitoring: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>尽管许多工作将运动监控带到智能手机和智能手表，但此类系统中使用的惯性传感器需要设备处于运动状态才能检测运动。我们介绍了ProxiFit，这是一种非常实用的设备上运动监控系统，即使设备保持静止，也能够对运动进行分类和计数。利用运动设备中天然磁性的新型接近感应，ProxiFit带来了（1）一种不涉及设备运动的新运动类别，例如下半身机器运动，以及（2）一种新的离体运动监控模式，在这种模式下，智能手机可以在锻炼期间方便地在用户面前查看。ProxiFit通过选择适当的预处理、否定对抗性运动伪影以及设计轻量级但耐噪声的分类器来解决微弱磁感应的常见问题。此外，通过设计独特但具有挑战性的训练策略，克服了application-specific挑战，例如设备种类繁多以及获取大型数据集的不切实际。我们在14个月内评估了ProxiFit在多达10台举重机器（5台下半身和5台上半身）和4台可穿戴和标牌模式下的自由重量练习，共有19名用户，在3个健身房，并验证了针对用户和天气变化、空间和旋转设备位置偏差以及相邻机器干扰的稳健性。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3581791.3596859">SoundSieve: Seconds-Long Audio Event Recognition on Intermittently-Powered Systems | Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services (acm.org)</a><br>每个intermittently-powered传感系统的一个基本问题是，这些系统在较长时间内获取的信号也是间歇性的。因此，这些系统无法捕获跨越存储收集能量的电容器的多个充放电周期的较长持续时间事件的一部分。从应用程序的角度来看，这被视为输入数据中缺失值的零星突发——使用统计插值或插值方法可能无法恢复。在本文中，我们根据间歇性音频分类系统研究了这个问题，并设计了一个端到端系统——SoundSieve——它能够准确分类跨越间歇性系统的多个开关周期的音频事件。SoundSieve采用离线音频分析仪，该分析仪学习识别和预测必须采样的音频片段的重要片段，以确保音频的准确分类。在运行时，SoundSieve采用了一个轻量级、能量和内容感知的音频采样器来决定系统何时应该唤醒以捕获下一个音频块；和一个轻量级、间歇性感知的音频分类器，执行插补和设备上的推理。通过使用流行的音频数据集和真实系统进行广泛的评估，我们证明SoundSieve产生的推理结果比最先进的高5%-30%。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3596261">Synthetic Smartwatch IMU Data Generation from In-the-wild ASL Videos | Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</a><br>可穿戴设备中可用于IMU的训练数据的稀缺对基于IMU的美国手语（ASL）识别提出了严重的挑战。在本文中，我们提出以下问题：我们能否将大量公开可用的、在野外的ASL视频“翻译”为其相应的IMU数据？我们通过呈现一个视频到IMU翻译框架（Vi2IMU）来回答这个问题，该框架将用户视频作为输入，并从用户手腕的角度估计IMU加速度和陀螺。Vi2IMU由两个模块组成，一个手腕方向估计模块，通过仔细结合手关节位置来考虑手腕旋转，另一个是加速度和陀螺预测模块，该模块利用方向进行转换，同时捕获手部运动和形状的贡献，以产生逼真的手腕加速度和陀螺数据。我们通过将公开可用的ASL视频翻译成相应的手腕IMU数据来评估Vi2IMU，并纯粹使用翻译后的数据训练手势识别模型。我们的结果表明，与使用测量的IMU数据训练的相同模型相比，使用翻译数据的模型表现相当好。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610881">SignRing: Continuous American Sign Language Recognition Using IMU Rings and Virtual IMU Data: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>手语是聋哑人和重听人（DHH）广泛使用的自然语言。先进的可穿戴设备被开发用于自动识别手语。然而，它们受到缺乏标记数据的限制，这导致词汇量小，即使在数据采集方面付出了艰苦的努力，性能也不令人满意。在这里，我们提出了SignRing，这是一个基于IMU的系统，它突破了传统的数据增强方法，利用在线视频生成虚拟IMU（v-IMU）数据，并通过达到934的词汇量和高达16个光泽的句子来推动基于可穿戴系统的边界。v-IMU数据是通过从双视图视频中重建3D手部运动并计算3轴加速度数据生成的，通过该数据，我们能够在混合了一半v-IMU和一半IMU训练数据（每个样本2339个）的情况下实现6.3%的单词错误率（WER），在100%v-IMU训练数据（6048个样本）的情况下实现14.7%的WER，与8.3%WER（使用2339个IMU数据样本进行训练）的基线性能相比。我们在v-IMU和IMU数据之间进行了比较，以证明v-IMU数据的可靠性和通用性。这项跨学科工作涵盖了可穿戴传感器开发、计算机视觉技术、深度学习和语言学等各个领域，可以为具有相似研究目标的研究人员提供有价值的见解。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9918171">ViTag: Online WiFi Fine Time Measurements Aided Vision-Motion Identity Association in Multi-person Environments | IEEE Conference Publication | IEEE Xplore</a><br>在本文中，我们提出了ViTag来关联多模态数据中的用户身份，特别是那些从相机和智能手机获得的数据。ViTag将视觉跟踪器生成的一系列边界框与来自智能手机的惯性测量单元（IMU）数据和Wi-Fi精细时间测量（FTM）相关联。我们将问题表述为序列到序列的关联（seq2seq）转换。在这个两步过程中，我们的系统首先使用多模态LSTM编码器-解码器网络（X-Translator）执行跨模态转换，该网络将一种模态转换为另一种模态，例如纯粹从相机边界框中侦察IMU和FTM读数。其次，关联模块在相机和手机域之间查找身份匹配，然后将转换后的模态与来自相同模态的观察数据相匹配。与现有的工作相比，我们提出的方法可以在所有用户可能都在执行相同活动的多人场景中关联身份。在真实世界的室内和室外环境中的广泛实验表明，在相机和手机数据（IMU和FTM）上的在线关联在1到3秒的窗口上实现了88.39%的平均身份精度（IDP），优于最先进的Vi-Fi（82.93%）。对手机域内的模式的进一步研究表明，FTM可以平均提高12.56%的关联性能。最后，来自我们的灵敏度实验的结果证明了ViTag在不同噪声和环境变化下的鲁棒性。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9826015">Vi-Fi: Associating Moving Subjects across Vision and Wireless Sensors | IEEE Conference Publication | IEEE Xplore</a><br>在本文中，我们介绍了Vi-Fi，这是一种多模态系统，它利用用户的智能手机WiFi精细定时测量（FTM）和惯性测量单元（IMU）传感器数据将在相机镜头上检测到的用户与其相应的智能手机标识符（例如WiFi MAC地址）相关联。我们的方法使用循环多模态深度神经网络，利用FTM和IMU测量以及用户与相机之间的距离（深度信息）来学习亲和力矩阵。作为比较的基线方法，我们还提出了一种使用二分图匹配的传统非深度学习方法。为了便于评估，我们收集了一个多模态数据集，其中包括具有深度信息的相机视频（RGB-D），WiFi FTM和IMU测量，用于不同现实环境下的多个参与者。使用关联精度作为评估Vi-Fi在将相机馈送上的人类用户与其手机ID关联时的保真度的关键指标，我们表明Vi-Fi实现了81%（实时）到91%（离线）的关联精度。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3580784">eat2pic: An Eating-Painting Interactive System to Nudge Users into Making Healthier Diet Choices: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 1</a><br>鉴于人类饮食行为的复杂性，开发互动来改变用户的饮食方式或饮食选择是一项挑战。在这项研究中，我们提出了一个名为eat2pic的交互式系统，旨在鼓励健康的饮食习惯，例如采用均衡的饮食和更慢的饮食，将选择食物的任务限制在为风景图片添加颜色的任务中。eat2pic系统包括一根配备传感器的筷子（一对中的一根）和两种类型的数字画布。它通过实时识别用户的饮食行为并将结果显示在名为“一餐eat2pic”的小画布上来提供快速反馈。此外，它还通过在名为“一周eat2pic”的大画布上显示用户食用的食物的颜色数量来提供缓慢反馈。前者被设计和实施为帮助人们吃得更慢的指南，后者鼓励人们选择更平衡的菜单。通过两项用户研究，我们探索了与eat2pic交互的体验，其中用户的日常饮食行为反映在一系列“绘画”中，即自动化系统产生的图像。实验结果表明，eat2pic可能会在膳食选择和饮食时提供反思的机会，以及帮助用户更加意识到他们的饮食方式以及他们的日常膳食有多平衡。我们期望这个系统能够激发用户对不同饮食和饮食方式的好奇心。这项研究也有助于扩大与饮食支持相关的产品和服务的设计空间。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3539490.3539603">Conversational AI Therapist for Daily Function Screening in Home Environments | Proceedings of the 1st ACM International Workshop on Intelligent Acoustic Systems and Applications</a><br>智能设备的发展正在使典型的家庭更加智能。在这项工作中，我们与治疗师合作，引入了一种基于家庭的人工智能治疗师，该治疗师利用智能家居环境来筛选日常功能并推断居住者的心理健康状况。与现有的通过对话识别用户心理状态的“聊天机器人”不同，我们的人工智能治疗师还利用整个家庭的智能设备和传感器来推断心理健康状况并评估用户的日常功能。我们提出了一系列37个维度的日常功能，我们的系统通过与用户交谈以及使用整个家庭的传感器和智能传感器检测日常活动事件来观察这些功能。我们的系统利用这37个维度与新颖的自然语言处理架构相结合，来检测精神状态（例如，愤怒或沮丧）、幸福感和日常功能的异常，并在检测到异常时生成安慰用户的响应。通过一系列用户研究，我们证明我们的系统可以自然地与用户交谈，准确地检测幸福感异常，并提供适当的安慰用户的响应。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3583780.3615101">Unleashing the Power of Shared Label Structures for Human Activity Recognition | Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</a><br>当前的人类活动识别（HAR）技术将活动标签视为整数类ID，而没有明确地对类标签的语义学进行建模。我们观察到不同的活动名称通常具有共享结构。例如，“打开门”和“打开冰箱”都有“打开”作为动作；“踢足球”和“打网球”都有“球”作为对象。标签名称中的这种共享结构可以转化为感官数据中的相似性，对常见结构进行建模将有助于揭示不同活动之间的知识，特别是对于样本有限的活动。在本文中，我们提出了SHARE，这是一个HAR框架，它考虑了不同活动的标签名称的共享结构。为了利用共享结构，SHARE包括一个用于从输入感官时间序列中提取特征的编码器和一个用于生成标签名称作为标记序列的解码器。我们还提出了三种标签增强技术来帮助模型更有效地捕获跨活动的语义结构，包括一个基本的令牌级增强，以及两个利用预训练模型功能的增强嵌入级和序列级增强。SHARE在七个HAR基准数据集的广泛实验中优于最先进的HAR模型。我们还在少镜头学习和标签不平衡设置中进行评估，并观察到更显着的性能差距。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3560905.3568303">CycleGAN Based Unsupervised Domain Adaptation for Machine Fault Diagnosis | Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems</a><br>故障诊断在保证机器的正常运行和安全生产方面起着至关重要的作用。近年来，数据驱动技术在机器故障诊断方面获得了很大的普及。但这些技术大多假设训练和测试数据具有相同的分布。然而，在大多数实际应用场景中，由于操作条件的变化、传感器位置的不同等不同因素，可以观察到训练（源）和测试（目标）数据之间的域差异。经典方法无法解决这种域差异，从而导致性能不佳。当目标完全未标记时，问题变得更具挑战性。为了解决这种情况，域自适应技术被用来将从标记的源域学到的知识转移到未标记的目标域。最近，基于对抗性网络的域自适应已被广泛探索用于故障诊断。但对抗性损失本身并不能保证将源转换到所需的目标域（类一致）。在这里，我们建议使用使用1D-CycleGAN的循环一致性损失来学习源到目标映射，用于轴承故障诊断的无监督自适应。所提出的方法针对两种不同的场景进行了评估，源和目标来自（i）相同的机器但不同的工作条件和（ii）不同但相关的机器。实验结果表明，虽然所提出的方法的性能与第一种情况的最佳性能基准相当，但对于具有挑战性的第二种情况，它显着优于所有最先进的方法。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3535782.3535840">Voice Impersonation for Thai Speech Using CycleGAN over Prosody | Proceedings of the 4th International Conference on Management Science and Industrial Engineering (acm.org)</a><br>语音模拟对于模仿目标说话者的各个方面来说可能是一项具有挑战性的任务。本文提出了一种使用循环一致对抗网络（CycleGAN）对语音转换（VC）中的非并行训练数据进行韵律转换的方法。CycleGAN模型在未配对图像的风格转换方面表现出出色的性能。在这种方法中，转换后的语音是由目标语音的韵律和频谱特征的转换产生的。这样它可以更好地代表目标说话者的角色。评估合成语音的标准评估程序的实验结果表明，我们的韵律CycleGAN显着优于传统的非并行VC方法。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3610919">SIDA: Self-Supervised Imbalanced Domain Adaptation for Sound Enhancement and Cross-Domain WiFi Sensing: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 7, No 3</a><br>2019冠状病毒病（2019冠状病毒病）肺炎仍然存在，其主要主诉是干咳。医生设计无线听诊器以方便诊断，然而，肺部声音很容易受到外部噪音的干扰。为了实现肺部声音增强，先前的研究大多假设干净和嘈杂的数据量是相同的。由于数据采集和注释的大量劳动，这一假设几乎没有得到满足。跨域的数据不平衡广泛发生在现实世界的物联网系统中，例如声音增强和基于WiFi的人体传感。在本文中，我们提出了SIDA，这是一种用于声音增强和WiFi传感的自监督不平衡域适应框架，使其成为物联网系统的通用时序域适应解决方案。SIDA提出了一种自监督的不平衡域自适应模型，该模型在样本有限的少数域、样本丰富的多数域中分别学习时间序列信号的表示，以及它们的映射关系。对于肺部声音增强，我们进一步提出了相位校正模型来清理相位，并提出了SNR预测算法来递归地在不平衡的嘈杂和干净的肺部声音数据集中执行域自适应。广泛的实验表明，SIDA在合成和现实的不平衡肺部声音数据集上分别将嘈杂样本的SNR提高了16.49dB和4.06dB。对于基于WiFi的人类传感，SIDA设计了一种基于跨域WiFi的人类识别模型，而不考虑行走轨迹。一群人在现实的测试环境中行走的特定轨迹被认为是少数域，其他几个轨迹作为多数域存储在服务器上。广泛的实验表明，SIDA可以以94.72%的平均准确率识别个人，并且在跨域人类识别任务中显着优于高度不平衡的WiFi数据集上的基线。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544548.3581562">Using Logs Data to Identify When Software Engineers Experience Flow or Focused Work | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>除了自我报告数据之外，我们缺乏识别流程的可靠和非侵入性方法。然而，退一步，承认流程发生在专注的时期，让我们有机会通过隔离专注的工作来在测量流程方面取得进展。在这里，我们采用混合方法来设计一个基于日志的指标，该指标利用机器学习和全面的日志数据集合来识别相关行动的时期（指示专注），并使用日记数据和季度调查数据根据自我报告的专注或流程时间来验证该指标。我们的结果表明，我们可以确定大型科技公司的软件工程师何时体验到专注的工作，其中包括流程实例。该指标与工程工作有关，但可以在其他领域中利用，以非破坏性的方式测量人们何时体验到专注。未来的研究可以在此工作的基础上识别与流程其他方面相关的信号。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/fullHtml/10.1145/3586183.3606808">FeetThrough: Electrotactile Foot Interface that Preserves Real-World Sensations (acm.org)</a><br>触觉界面已经扩展到足部，以增强基于足部的活动，例如走路时的引导或踩在虚拟纹理上。大多数足部触觉使用机械执行器，即振动电机。然而，我们认为振动电机并不是所有足部触觉的理想执行器。相反，我们证明电触觉刺激提供了使其成为强大的脚触觉界面的品质：（1）佩戴电触觉的用户不仅可以感受到刺激，还可以更好地感受脚下的地形——这一点至关重要，因为我们的脚也负责不平坦地形和楼梯上的平衡——电触觉实现了这种改进的“感觉通过”效果，因为它比振动触觉致动器更薄，在我们的原型中为0.1毫米；（2）虽然单个振动触觉致动器也会振动周围的皮肤区域，但我们发现电触觉的改进两点辨别阈值；（3）电触觉可以直接应用于鞋底、鞋垫或袜子，从而实现赤脚互动体验等新应用，或者不需要用户使用内置振动电机的定制鞋。最后，我们展示了电触觉脚界面不仅允许用户感受虚拟信息，还允许用户感受鞋子下的真实地形的应用，例如用户在地面道具上行走的虚拟现实体验和触觉导航系统，该系统通过虚拟触觉铺路增加地面，以帮助低视力情况下的行人。</p>
<hr>
<p><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Automated-Hilbert-Envelope-Based-Respiration-Rate-Reddy-Manikandan/b2d2df75151f0d3f285243dcf438332bdb3bc71a">Automated Hilbert Envelope Based Respiration Rate Measurement from PPG Signal for Wearable Vital Signs Monitoring Devices | Semantic Scholar</a><br>呼吸频率（RR）是预测严重疾病症状的最重要的生命体征之一，也被用作早期疾病预警（早期发现患者病情恶化）和监测人的身体和情绪压力的重要指标或重要生理参数。在本文中，我们提出了一种使用光体积描记图（PPG）信号的基于希尔伯特包络的自动呼吸频率估计方法。所提出的希尔伯特变换RR（HT-RR）方法通过使用从BIDMC和CapnoBase数据库中获取的信号进行测试。在基准性能指标上，所提出的方法在30秒和60秒PPG信号分别具有3.7（1.8-5.5）呼吸&#x2F;分钟（brpm）和2.6（0.8-5.5）brpm的中位数（25-75百分位数）方面的平均绝对误差（MAE）。评估结果进一步表明，从30秒持续时间的PPG信号中计算RR值需要4.81±0.80毫秒。该方法在提高可穿戴便携式诊断系统的准确性和可靠性方面具有巨大潜力。观察到该方法优于最近的RR估计方法。</p>
<h1 id="2023-12"><a href="#2023-12" class="headerlink" title="2023.12"></a>2023.12</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/ftp/arxiv/papers/2308/2308.08985.pdf">Home monitoring for frailty detection through sound and speaker diarization analysis</a></p>
<p>随着法国、欧洲和世界各地人口的老龄化，人们对保证可靠和隐私保护的家庭监控以预防脆弱的新系统有着浓厚的兴趣。这项工作是全球环境音频分析系统的一部分，旨在通过人类和日常生活中的声音识别、语音存在和说话者数量检测来帮助识别日常生活活动（ADL）。重点是说话者数量检测。在本文中，我们介绍了声音处理和说话者分类的最新进展如何改进现有的嵌入式系统。我们研究了两种新方法的性能，并讨论了基于DNN的方法的好处，这些方法将性能提高了约100</p>
<p>hEARt: Motion-resilient Heart Rate Monitoring with In-ear Microphones<br>随着入耳式可穿戴设备的迅速普及，研究界已经开始研究合适的入耳式心率（HR）检测系统。HR是心血管健康和身体健康的关键生理标志。因此，近年来，使用可穿戴设备进行持续可靠的HR监测越来越受到关注。可穿戴设备中现有的HR检测系统主要依赖于Photoplethysmography（PPG）传感器，然而，这些传感器因在人体运动存在时性能不佳而臭名昭著。在这项工作中，利用闭塞效应的声音增强特性，可以通过密封耳道入口（一些现有耳机已经这样做以改善噪声消除）来产生，我们首次研究了基于入耳式音频的运动弹性HR监测。这是通过使用入耳式麦克风测量人类耳道中HR诱导的声音来完成的。具体而言，我们开发了一种基于小波变换的新型运动伪影（MA）去除技术，然后是一种HR估计算法，从与其他活动（例如，走路、跑步和说话）相结合的入耳式音频信号中提取HR。与现有工作不同，我们在一组不同的运动伪影和说话时对我们的技术进行了系统评估。通过从15名受试者在四个活动中收集的数据，我们证明我们的方法在静止、行走、跑步和说话时分别实现了0.88±0.27 BPM、8.11±3.89 BPM、13.79±5.61 BPM和7.49±3.23 BPM的平均绝对误差（MAE），为新的非侵入性和负担得起的HR监控打开了大门，该监控具有日常活动可用的性能。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.06955">[1802.06955] Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation (arxiv.org)</a><br>在过去的几年里，基于深度学习（DL）的语义分割方法一直在提供最先进的性能。更具体地说，这些技术已经成功地应用于医学图像分类、分割和检测任务。一种深度学习技术U-Net已经成为这些应用中最受欢迎的技术之一。在本文中，我们提出了基于U-Net的循环卷积神经网络（RCNN）以及基于U-Net模型的循环残差卷积神经网络（RRCNN），它们分别被命名为RU-Net和R2U-Net。所提出的模型利用了U-Net、残差网络以及RCNN的强大功能。这些提议的架构对于分割任务有几个优点。首先，残差单元有助于训练深度架构。其次，具有循环残差卷积层的特征积累确保了分割任务的更好特征表示。第三，它允许我们设计具有相同数量的网络参数的更好的U-Net架构，具有更好的医学图像分割性能。所提出的模型在视网膜图像中的血管分割、皮肤癌分割和肺部病变分割等三个基准数据集上进行了测试。实验结果表明，与包括U-Net和残差U-Net（ResU-Net）在内的等效模型相比，分割任务的性能优越。</p>
<p><a target="_blank" rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/24141593/">A technique for estimating the occlusion effect for frequencies below 125 Hz - PubMed (nih.gov)</a><br>目的：在低频时，通过阻塞耳道，例如通过助听器（HA）的耳罩，可以提高听道中骨传导声音的水平。这种“阻塞效应”（OE）的物理测量需要头骨振动。在以前的研究中，使用自声或听觉骨传导振动器来产生这种振动，结果是在低于125 Hz的频率下无法测量OE。然而，低于这个频率对HA用户的音乐感知很重要。目的是开发和评估一种方法，对低于125 Hz的频率给出OE的下限估计。</p>
<p>设计：使用具有扩展低频响应的低噪声放大器来记录插入参与者耳道的微型麦克风的输出。信号来自参与者的心跳和血流的声音，通过耳道壁的骨传导传输。同时记录颈动脉脉冲，以便对麦克风信号进行时间锁定平均（从而降低噪声）。记录来自7名耳科和听力测量正常的参与者，使用临床探头尖端产生闭塞。记录也来自9名参与者的重叠组，使用快速设置的印模材料提供更一致的闭塞程度。未闭塞和闭塞条件下记录信号水平的差异为OE的幅度提供了下限。</p>
<p>结果：平均OE随频率的降低而增加，对于低于40 Hz的频率，OE达到约40 dB的平台。对于一些个人录音，对于低于20 Hz的频率，OE达到50 dB。在闭塞的情况下，大多数参与者可以听到心跳。</p>
<p>结论：OE在低频时可能非常大。使用带有封闭接头的HAs，既可以用来防止声反馈，也可以用来放大低频，可能会导致不可接受的OE。作者建议在通道深处使用密封来减少OE，因为通道壁更加坚硬。</p>
<p><a target="_blank" rel="noopener" href="https://www.mdpi.com/2076-3417/13/5/3100">Applied Sciences | Free Full-Text | Overview of Voice Conversion Methods Based on Deep Learning (mdpi.com)</a><br>语音转换是一个将说话者身份的本质无缝转移到另一个说话者身上的过程，同时保留他们说话的内容。这种用法是通过混合语音处理技术的算法来完成的，如语音分析、说话者分类和语音编码。尖端的语音转换技术的特点是深度神经网络有效地将说话者的声音与其语言内容分开。本文基于当前最先进的语音转换方法，全面概述了这一科学领域的发展状况。</p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S1746809423008984?via=ihub">Addressing intra-subject variability in electrocardiogram-based biometric systems through a hybrid architecture - ScienceDirect</a><br>为了解决基于心电图的生物识别系统中受试者内部变异性的关键挑战，我们提出了一种新的混合分类架构。该架构利用基准和非基准特征，有效地减轻了变异性的影响并提高了系统性能。我们在私人数据库和公共MIT-BIH心律失常数据库上进行的评估显示出有希望的结果。在私人数据库中，在对未包含在培训中的课程的心跳进行分类时，F1分数提高了12%。补充的是，在公共数据库中，通过结合统计和基准特征以及小波和基准特征，分类只需要一个心跳，准确率达到了99.98%。因此，通过混合方法集成不同的特征类型为处理生物识别系统中的高心脏变异性提供了一个有希望的解决方案。</p>
<h1 id="authentication"><a href="#authentication" class="headerlink" title="authentication"></a>authentication</h1><p><a target="_blank" rel="noopener" href="https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs223617">A secure lightweight fuzzy embedder based user authentication scheme for internet of medical things applications - IOS Press</a><br>摘要：医疗物联网（IoMT）是一个由医疗设备、硬件基础设施和软件组成的网络，允许医疗信息技术通过网络进行通信。IoMT传感器将医疗数据传输到服务器以进行快速诊断。由于它处理用户的私人和机密信息，安全性是主要目标。现有的物联网身份验证方案要么使用双因素（用户名、密码），要么使用多因素（用户名、密码、生物特征）来验证用户。通常，结构characteristics-based生物特征（如面部、虹膜、掌纹或指纹）被用作附加因素。这些生物特征有可能是伪造的。因此，这些基于结构生物特征的身份验证方案无法提供隐私、安全、真实性和完整性。基于生物动力学的生物声学信号在人机交互时代得到了关注，因为它是每个用户的独特特征。因此，我们使用基于频域的生物声学作为生物特征输入。因此，本工作提出了一种基于模糊嵌入的安全轻量级生物声学用户认证方案，用于医疗物联网应用。此外，物联网传感器倾向于动态加入和离开网络，所提出的方案采用中国剩余技术生成组密钥，以保护网络免受前传感器节点的攻击。所提出的方案的安全性使用正式的验证工具AVISPA（互联网安全协议和应用程序的自动验证）进行验证。通过将所提出的方案在安全特性、计算和通信成本方面与现有系统进行比较来衡量系统的性能。它证明了所提出的系统优于现有系统。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1109/TMC.2021.3133275#sec-cit">Towards Nonintrusive and Secure Mobile Two-Factor Authentication on Wearables | IEEE Transactions on Mobile Computing (acm.org)</a><br>移动设备有望应用双重验证来提高系统安全性。现有的解决方案有一定的限制，需要额外的用户努力，这可能会严重影响用户体验并延迟认证时间。在本文中，我们提出了PPGPass，这是一种新颖的移动双重验证系统，它利用了大多数腕戴式可穿戴设备中可用的Photoplethysmography（PPG）传感器。PPGPass同时执行密码&#x2F;模式&#x2F;签名身份验证和基于生理的身份验证。为了实现非侵入性和安全性，我们设计了一种两阶段算法，将干净的心跳信号与被运动伪影污染的PPG信号分开，这样用户就不必故意保持身体静止。此外，为了处理生物识别技术受到损害时的不可取消问题，我们设计了一种可重复且不可反转的方法来生成可取消的特征模板作为替代凭据。我们利用随机森林和支持向量数据描述的强大功能来检测对手并验证用户身份。据我们所知，PPGPass是第一个基于PPG传感器的非侵入式安全移动双重验证。大量实验表明，PPGPass可以实现3.11%的错误接受率和3.71%的错误识别率，这证实了其高有效性、安全性和可用性。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9796740">Push the Limit of WiFi-based User Authentication towards Undefined Gestures | IEEE Conference Publication | IEEE Xplore</a><br>随着智能室内环境的发展，用户认证成为支持各种安全访问的必不可少的机制。虽然最近的研究表明使用WiFi对具有人类活动或手势的用户进行身份验证取得了初步成功，但它们依赖于预定义的身体手势，并且在遇到未定义的身体手势时表现不佳。这项工作旨在通过未定义的身体手势而不仅仅是预定义的身体手势来实现基于WiFi的用户身份验证，即实现与手势无关的用户身份验证。在本文中，我们首先探索身体手势背后的生理特征，发现身体手势诱导的WiFi信号下的统计分布可以表现出与特定身体手势无关的不变的个体唯一性。受此观察结果的启发，我们提出了一种用户身份验证系统，该系统利用WiFi信号以与手势无关的方式识别个人。具体而言，我们设计了一种基于对抗性学习的模型，该模型抑制特定手势特征，并提取与特定身体手势无关的不变个体唯一性，以与手势无关的方式对用户进行认证。室内环境中的广泛实验表明，所提系统在与手势无关的用户认证中是可行和有效的。</p>
<h2 id="biometrics"><a href="#biometrics" class="headerlink" title="biometrics"></a>biometrics</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10228921">HeartPrint: Passive Heart Sounds Authentication Exploiting In-Ear Microphones | IEEE Conference Publication | IEEE Xplore</a> 2023<br>近年来，生物识别技术越来越多地集成到可穿戴设备中，以增强数据隐私和安全性。与此同时，可穿戴设备的普及反过来又为利用各种嵌入式传感模式捕获新颖的生物识别技术创造了独特的机会。在本文中，我们研究了一种结合i）心脏运动、ii）骨传导和iii）身体不对称的独特性的新的颞内生物识别技术。具体来说，我们将HeartPrint设计为一个被动但安全的用户身份验证系统：它利用（广泛使用的）双入耳式麦克风（IEM）捕获的骨传导心音来验证用户，而巧妙地利用IEM使其对用户透明，而不会损害耳机的正常功能。为了抑制耳机产生的其他身体声音和音频的干扰，我们开发了一种新颖的干扰消除方法，使用改进的非负矩阵分解将干净的心音与背景干扰分开。我们从三个方面进一步探索IEM记录心音的唯一性，以提取新颖的生物特征表示，在此基础上HeartPrint利用配备持续学习方法的卷积神经模型，在漂移身体条件下实现准确认证。在45名参与者身上使用18对商用耳机进行的广泛实验证实，HeartPrint可以实现1.6%的FAR和1.8%的FRR，同时有效应对重大攻击、复杂干扰和硬件多样性。</p>
<p>2022  <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9920171">Touchscreens Can Reveal User Identity: Capacitive Plethysmogram-Based Biometrics | IEEE Journals &amp; Magazine | IEEE Xplore</a><br>生物识别技术被广泛用于用户识别&#x2F;认证，但事实很少被注意到，一般的电容式触摸屏可以通过触摸信号揭示用户身份。本文提出了一种基于电容式触摸屏捕获的心脏信号进行可靠用户识别的具有固有活性检测的新生物识别方法，即电容性脉搏图（CPG）。并设计了一个系统的框架，用于CPG收集、处理和利用以识别用户。具体而言，由于手指在触摸过程中通常会与多个感应电极形成电容器，因此我们可以从屏幕输出中同时提取多个CPG信号。然后我们提出了一系列预处理算法来过滤CPG以增强信号质量。最后，为了进一步利用过滤后的CPG信号并提取用于识别用户的高效特征，我们构建了一个基于3D注意力和CNN度量学习的编码器。实验结果表明，该方法在实验室环境下的平均准确率为96.73%，FAR为3.03%，FRR为7.35%，揭示了CPG在各种电容式触摸屏设备上的用户隐私保护和数据安全潜力。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3534606">ToothSonic: Earable Authentication via Acoustic Toothprint: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 6, No 2</a><br>Earables（耳朵可穿戴设备）正在迅速成为一个涵盖各种个人应用的新平台。因此，传统的身份验证方法由于其有限的输入接口而变得不那么适用和不方便。然而，耳机通常具有丰富的头部周围传感能力，可以利用这些能力来捕获新型生物识别技术。在这项工作中，我们提出了ToothSonic，利用用户执行牙齿手势产生的牙印诱导的声波效应进行可听认证。特别是，我们设计了具有代表性的牙齿手势，可以产生携带牙印信息的有效声波。为了可靠地捕获声学牙印，它利用耳道的闭塞效应和耳机的内向麦克风。然后它提取多级声学特征来反映内在牙印信息进行身份验证。ToothSonic的主要优点是它适用于可听设备，并且可以抵抗各种欺骗攻击，因为声学牙印是通过用户的私人牙齿-耳朵通道捕获的，该通道调制和加密声波。我们对25名参与者的实验研究表明，ToothSonic仅通过用户的一个牙齿手势就实现了高达95%的准确率。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3460120.3485340">Earable Authentication via Acoustic Toothprint | Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security</a><br>Earables（耳朵可穿戴）正在迅速成为一个新平台，以实现各种个人应用。传统的身份验证方法因此变得不太适用，并且由于其有限的输入接口而不方便耳机。然而，Earables通常具有丰富的头部传感能力，可以利用这些能力来捕获新型的生物识别技术。在这项工作中，我们提出了ToothSonic，它利用用户执行牙齿手势产生的牙印诱导的声波效应进行用户身份验证。特别是，我们设计了几种具有代表性的牙齿手势，可以产生携带牙印信息的有效声波。为了可靠地捕获声学牙印，它利用耳道的闭塞效应和耳机的内向麦克风。然后它提取多级声学特征来表示固有的声学牙印进行身份验证。ToothSonic的主要优势是它适用于耳机，并且可以抵抗各种欺骗攻击，因为声学牙印是通过用户的私人牙齿-耳朵通道捕获的，而其他人不知道。我们对20名参与者的初步研究表明，ToothSonic仅通过三个牙齿手势就实现了97%的准确率。</p>
<h1 id="伪影"><a href="#伪影" class="headerlink" title="伪影"></a>伪影</h1><p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41598-021-90437-7">Eye-blink artifact removal from single channel EEG with k-means and SSA | Scientific Reports (nature.com)</a><br>近年来，便携式electroencephalogram（EEG）设备的使用在临床和非临床应用中都变得流行起来。为了给受试者提供更多的舒适感并测量几个小时的EEG信号，这些设备通常由较少的EEG通道组成，甚至具有单个EEG通道。然而，眼电图（EOG）信号，也称为眨眼伪影，由眼睑的不自主运动产生，总是污染EEG信号。很少有技术可用于从单通道EEG中去除这些伪影，并且这些技术中的大多数都修改了EEG信号的未污染区域。在本文中，我们开发了一个新的框架，该框架结合了无监督机器学习算法（k-means）和奇异谱分析（SSA）技术，以去除眨眼伪影，而无需修改实际的EEG信号。工作的新颖性在于基于脑电信号的时域特征和无监督机器学习算法提取了眨眼伪影，提取的眨眼伪影通过SSA方法进一步处理，最后从被污染的单通道脑电信号中减去，得到校正后的脑电信号。合成和真实脑电信号的结果证明了所提方法相对于现有方法的优越性。此外，基于频率的测量[功率谱比（\（\Gamma\））和平均绝对误差（MAE）]也表明，所提方法在去除眨眼伪影的同时没有修改脑电信号的未污染区域。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2307.05339v1.pdf">A Self-Supervised Algorithm for Denoising Photoplethysmography Signals for Heart Rate Estimation from Wearable</a><br>智能手表和其他可穿戴设备配备了photoplethysmography（PPG）传感器，用于监测心率和心血管健康的其他方面。然而，从此类设备收集的PPG信号容易受到噪声和运动伪影的破坏，从而导致心率估计的错误。典型的去噪方法以消除大部分形态信息的方式过滤或重建信号，即使是从有用的保留信号的干净部分。在这项工作中，我们开发了一种去噪PPG信号的算法，该算法重建了信号的损坏部分，同时保留了PPG信号的干净部分。我们的新框架依赖于自我监督训练，我们利用干净PPG信号的大型数据库来训练去噪自动编码器。正如我们所展示的，我们的重建信号比领先的心率估计方法提供了来自PPG信号的更好的心率估计。进一步的实验表明，使用我们的算法，PPG信号的心率变异性（HRV）估计有了显著改善。我们得出结论，我们的算法对PPG信号进行去噪，可以改善可穿戴设备对许多不同健康指标的下游分析。</p>
<p><a target="_blank" rel="noopener" href="https://deepai.org/publication/motion-resilient-heart-rate-monitoring-with-in-ear-microphones">Motion-resilient Heart Rate Monitoring with In-ear Microphones | DeepAI</a><br>随着入耳式可穿戴设备的迅速普及，研究界已经开始研究合适的入耳式心率（HR）检测系统。HR是心血管健康和身体健康的关键生理标志。因此，近年来，使用可穿戴设备进行持续可靠的HR监测越来越受到关注。可穿戴设备中现有的HR检测系统主要依赖于Photoplethysmography（PPG）传感器，然而，这些传感器因在人体运动存在时性能不佳而臭名昭著。在这项工作中，利用闭塞效应的声音增强特性，可以通过密封耳道入口（一些现有耳机已经这样做以改善噪声消除）来产生，我们首次研究了基于入耳式音频的运动弹性HR监测。这是通过使用入耳式麦克风测量人类耳道中HR诱导的声音来完成的。具体而言，我们开发了一种基于小波变换的新型运动伪影（MA）去除技术，然后是一种HR估计算法，从与其他活动（例如，走路、跑步和说话）相结合的入耳式音频信号中提取HR。与现有工作不同，我们在一组不同的运动伪影和说话时对我们的技术进行了系统评估。通过从15名受试者在四个活动中收集的数据，我们证明我们的方法在静止、行走、跑步和说话时分别实现了0.88±0.27 BPM、8.11±3.89 BPM、13.79±5.61 BPM和7.49±3.23 BPM的平均绝对误差（MAE），为新的非侵入性和负担得起的HR监控打开了大门，该监控具有日常活动可用的性能。</p>
<p><a target="_blank" rel="noopener" href="https://deepai.org/publication/deepbeat-a-multi-task-deep-learning-approach-to-assess-signal-quality-and-arrhythmia-detection-in-wearable-devices">DeepBeat: A multi-task deep learning approach to assess signal quality and arrhythmia detection in wearable devices | DeepAI</a><br>可穿戴设备能够在理论上对步数、能量消耗和心率等生理测量进行连续、纵向监测。尽管可穿戴设备对心房颤动等异常心律的分类具有巨大潜力，但商业算法仍然是专有的，并且倾向于专注于源自放置在手腕上的绿色光谱LED传感器的心率变异性，其中噪声仍然是一个未解决的问题。在这里，我们开发了一种多任务深度学习方法来评估可穿戴photoplethysmography设备中的信号质量和心律失常事件检测，以实时检测心房颤动（AF）。我们在超过一百万个模拟未标记生理信号上训练我们的算法，并在来自3个不同可穿戴设备的100多人的超过500K标记信号的精选数据集上进行微调。我们证明，与传统的基于随机森林的方法（精度：0.24，召回：0.58，f1:0.34，auPRC：0.44）和单个任务CNN（精度：0.59，召回：0.69，f1:0.64，auPRC：0.68）相比，我们的架构通过卷积去噪自动编码器使用无监督迁移学习极大地提高了静止参与者的AF检测性能（pr：0.94，rc：0.98，f1:0.96，auPRC：0.96）。此外，我们使用来自独立设计设备的数据验证了动态受试者前瞻性衍生复制队列的算法性能。我们表明，两阶段训练可以帮助解决大型注释良好的数据集稀缺的生物医学应用中常见的数据不平衡问题。总之，通过模拟和迁移学习的结合，我们开发并应用了一种多任务架构来解决可穿戴手腕传感器的AF检测问题，展示了高水平的准确性，并解决了机械噪声的棘手挑战。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVlabs/noise2noise">NVlabs&#x2F;noise2noise: Noise2Noise: Learning Image Restoration without Clean Data - Official TensorFlow implementation of the ICML 2018 paper (github.com)</a><br>Unsupervised<br>我们通过机器学习将基本的统计推理应用于信号重构——学习将损坏的观察结果映射到干净的信号——并得出一个简单而有力的结论：仅通过查看损坏的示例就可以学习恢复图像，使用干净的数据进行性能训练，有时甚至超过训练，而无需明确的图像先验或损坏的似然模型。在实践中，我们展示了单个模型学习去除照片噪声、去噪合成蒙特卡罗图像和重建采样不足的核磁共振扫描——所有这些都被不同的过程损坏——仅基于噪声数据。 training of image restoration models without the need for clean data.</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.05172">[2109.05172] Incorporating Real-world Noisy Speech in Neural-network-based Speech Enhancement Systems (arxiv.org)</a><br>Semi-supervised<br>监督语音增强依赖于训练期间降级语音信号及其干净参考信号的并行数据库。该设置禁止使用可能更好地表示使用此类系统的场景的真实世界降级语音数据。在本文中，我们探索了使监督语音增强系统能够在真实世界降级语音数据上进行训练的方法。具体来说，我们提出了一种语音增强的半监督方法，其中我们首先训练一个改进的矢量量化变分自动编码器，该编码器解决了一个源分离任务。然后，我们使用这个经过训练的自动编码器通过计算基于三元组的无监督损失函数，进一步训练一个使用真实世界嘈杂语音数据的增强网络。实验表明，在训练语音增强系统中合并真实世界数据的结果很有希望。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8168192">On generating mixing noise signals with basis functions for simulating noisy speech and learning dnn-based speech enhancement models | IEEE Conference Publication | IEEE Xplore</a> 2017</p>
<p>我们首先研究了用于训练基于深度神经网络（DNN）的语音增强的噪声和干净语音特征之间的非线性映射函数的噪声样本的泛化问题。然后建立了一个经验证明来解释为什么基于 DNN 的方法具有良好的噪声泛化能力，前提是在生成用于训练的各种噪声语音样本时包含大量噪声类型。结果表明，任意噪声信号段可以由微结构噪声基的线性组合很好地表示。因此，我们建议通过设计一组紧凑和解析的噪声基来生成这些混合噪声信号，而不使用任何现实的噪声类型。实验表明，这种噪声生成方案可以产生与使用50种真实噪声类型相当的性能。此外，通过用合成的噪声基补充收集的噪声类型，我们观察到显着的性能改进，这意味着不仅可以减轻大量真实世界噪声信号的收集，而且可以实现良好的噪声泛化能力。</p>
<p><a target="_blank" rel="noopener" href="https://iopscience.iop.org/article/10.1088/1361-6579/ac3b3d">A supervised machine learning semantic segmentation approach for detecting artifacts in plethysmography signals from wearables - IOPscience</a><br>目的。配备体积描记（PPG）传感器的可穿戴设备为早期诊断和心脏状况的持续筛查提供了一种低成本、长期的解决方案。然而，从此类设备收集的 PPG 信号经常遭受伪影引起的损坏。这项研究的目标是开发一种有效的监督算法来定位 PPG 信号中伪影的区域。方法。我们将伪影检测视为一维分割问题。我们通过 active-contour-based 损失和适应的 U-Net 架构的新颖组合来解决它。所提出的算法在 PPG DaLiA 训练集上进行了训练，并在 PPG DaLiA 测试集上进一步评估了 WESAD 数据集和 TROIKA 数据集。主要结果。我们用 DICE 分数进行了评估，DICE 分数是计算机视觉领域分割精度评估的成熟指标。所提出的方法在所有三个数据集上的性能都大大优于基线方法（比下一个最佳方法高出7个百分点）。在 PPG DaLiA 测试集、WESAD 数据集和 TROIKA 数据集上，所提出的方法分别实现了0.8734±0.0018、0.9114±0.0033和0.8050±0.0116。下一个最佳方法仅实现了0.8068±0.0014、0.8446±0.0013和0.7247±0.0050的意义。所提出的方法能够以高精度精确定位伪影的确切位置；过去，我们只有 PPG 信号质量好还是差的二分类。这种更细致入微的信息对于进一步告知检测心律失常的算法设计至关重要。</p>
<p><a target="_blank" rel="noopener" href="https://www.frontiersin.org/articles/10.3389/felec.2021.685513/full">Frontiers | Motion Artifact Removal Techniques for Wearable EEG and PPG Sensor Systems (frontiersin.org)</a><br>运动伪影的去除是一个关键的挑战，尤其是在日常运动暴露的可穿戴electroencephalography（EEG）和photoplethysmography（PPG）设备中。最近，由于基于脑电图的脑机接口（BCI）和可穿戴PPG设备的日常医疗保健使用受到关注，运动伪影去除技术的重要性增加了。本文介绍了EEG和PPG传感器系统的发展。然后，回顾了对运动伪影的理解及其通过硬件和&#x2F;或软件方式实现的减少方法。研究了各种电极类型、模拟读出电路和信号处理技术来去除EEG运动伪影。此外，还介绍了最近的具有运动伪影减少的入耳式EEG技术。此外，还提出了补偿PPG独立&#x2F;相关运动伪影的技术。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8942617">Sensor Fusion in Human Cyber Sensor System for Motion Artifact Removal from NIRS Signal | IEEE Conference Publication | IEEE Xplore</a><br>近红外光谱（NIRS）信号已广泛用于监测临床和心理调查中的血流动力学变化，以及用于步态和康复的脑机接口（BCI）等人体系统接口。然而，由于在循环系统中运动的人的运动伪影的存在，血流动力学变化的估计可能会变得模糊，为了更准确的估计，应该将其删除。为了更准确地记录运动信息，开发了一种可穿戴无线 NIRS 网络传感器系统，该系统能够记录来自靠近近红外光学传感器的多传感器集成惯性测量单元（IMU）的运动相关信号。虽然与光学传感器处的运动高度相关的多轴加速度计、陀螺仪和磁力计信号可以很好地估计近红外信号中的运动伪影，但运动融合算法可以通过克服单个传感器的内在限制（如不精确和漂移）来提供更准确的近红外信号中的运动伪影估计。这项研究旨在确定基于运动融合算法的信号和来自 IMU 的单个传感器读数的组合是否可以提供更准确的近红外信号中的运动伪影校正。结果表明，当运动融合信号用于估计和去除运动伪影时，信噪比（SNR）显着提高。结果表明，运动融合算法可以提供更准确的运动伪影估计和去除，从而支持更好地检测血流动力学变化。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41598-022-25949-x">Data quality evaluation in wearable monitoring | Scientific Reports (nature.com)</a><br>从腕部采集的神经生理信号的可穿戴记录为癫痫监测提供了巨大的潜力。然而，数据质量仍然是影响数据可靠性的最具挑战性的因素之一。我们建议使用组合数据质量评估工具来评估多模态可穿戴数据。我们分析了来自四个癫痫中心的癫痫患者的数据。患者佩戴腕带记录加速度测量、皮肤电活动、血量脉搏和皮肤温度。我们计算数据完整性并评估设备佩戴时间（在身上）和特定模态的信号质量分数。我们包括632名住院患者的37,166小时和39名门诊患者的90,776小时。所有模式都受到伪影的影响。与机载设备记录和存储（高达9%）相比，使用数据流时的数据丢失更高（住院患者队列中高达49%，各个记录的平均值）。估计设备戴在身上时间百分比的身体得分在队列中一直很高（超过80%）。基于既定指数的一些模式的信号质量在夜间高于白天。统一报告的数据质量和多模态信号质量指数是可行的，使研究结果更具可比性，并有助于开发癫痫监测所需的设备和评估程序。</p>
<p><a target="_blank" rel="noopener" href="https://ascelibrary.org/doi/10.1061/JCEMD4.COENG-13263">Evaluation of Data Processing and Artifact Removal Approaches Used for Physiological Signals Captured Using Wearable Sensing Devices during Construction Tasks | Journal of Construction Engineering and Management | Vol 150, No 1 (ascelibrary.org)</a><br>可穿戴传感设备（WSD）在监控建筑工人安全方面有着巨大的前景。它们可以实时跟踪工人并发送与安全相关的信息，从而做出更有效和预防性的决策。WSD 在建筑工地特别有用，因为它们可以跟踪工人的健康、安全和活动水平，以及其他有助于优化日常任务的指标。WSD 还可以帮助工人识别与健康相关的安全风险（如身体疲劳）并采取适当的行动来减轻它们。然而，这些 WSD 产生的数据噪音很大，并且被周围环境、实验设备或受试者生理状态可能引入的伪影污染。这些伪影非常强烈，并且经常在现场实验中发现。因此，当有很多伪影时，信号质量会下降。最近，信号处理的发展极大地增强了伪影去除，极大地提高了性能。因此，拟议的审查旨在对目前用于分析数据和从 construction-related 任务期间通过 WSD 获得的生理信号中去除伪影的方法进行深入分析。首先，本研究概述了可能记录建筑工人的生理信号，以监测他们的健康和安全。其次，本审查确定了对信号的效用最不利的最普遍的伪影。第三，对现有伪影去除方法进行了全面审查。第四，分析了每种已识别的伪影检测和去除方法的优缺点。最后，本综述为未来研究提供了一些建议，以提高捕获的生理信号的质量，用于使用工件去除方法监测建筑工人的健康和安全。</p>
<h2 id="acc-based"><a href="#acc-based" class="headerlink" title="acc-based"></a>acc-based</h2><p><a target="_blank" rel="noopener" href="https://research.manchester.ac.uk/en/publications/motion-artefact-removal-in-electroencephalography-and-electrocard">Motion artefact removal in electroencephalography and electrocardiography by using multichannel inertial measurement units and adaptive filtering — Research Explorer The University of Manchester</a><br>本文提出了一种新的有源电极设计，用于基于惯性测量单元（IMU）的 electroencephalogram（EEG）和心电图（ECG）传感器，以去除信号采集过程中的运动伪影。IMU 不是为整个记录单元测量来自单一来源的运动数据，而是连接到每个单独的 EEG 或 ECG 电极以收集局部运动数据。然后通过使用归一化最小均方（NLMS）自适应滤波来使用这些数据来去除运动伪影。结果表明，所提出的有源电极设计可以减少胸部运动和头部摆动运动场景中来自 EEG 和 ECG 信号的运动污染。然而，我们发现性能各不相同，因此需要我们的算法与更复杂的信号处理配对，以识别在提高信号质量方面有益的场景。新的仪器硬件允许执行数据驱动的伪影去除，与广泛使用的盲源分离方法相比，提供了一种新的数据驱动方法，并有助于在野外执行 EEG 记录。</p>
<h2 id="Statistical"><a href="#Statistical" class="headerlink" title="Statistical"></a>Statistical</h2><p><a target="_blank" rel="noopener" href="https://www.mdpi.com/1424-8220/22/9/3169">Sensors | Free Full-Text | Motion Artifacts Correction from Single-Channel EEG and fNIRS Signals Using Novel Wavelet Packet Decomposition in Combination with Canonical Correlation Analysis (mdpi.com)</a><br>electroencephalogram（EEG）和功能性近红外光谱（fNIRS）信号，本质上高度非平稳，在使用可穿戴传感器记录时，极大地受到运动伪影的影响。由于成功检测各种神经和神经肌肉疾病在很大程度上依赖于干净的 EEG 和 fNIRS 信号，因此使用可靠和稳健的方法从 EEG 和 fNIRS 信号中去除&#x2F;减少运动伪影是至关重要的。在这方面，本文提出了两种稳健的方法：（i）小波包分解（WPD）和（ii）WPD 与典型相关分析（WPD-CCA）相结合，用于单通道 EEG 和 fNIRS 信号的运动伪影校正。使用基准数据集测试了这些提出的技术的有效性，并使用两个既定的性能矩阵测量了所提出方法的性能：（i）信噪比的差异</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/6075241">Automatic Motion and Noise Artifact Detection in Holter ECG Data Using Empirical Mode Decomposition and Statistical Approaches | IEEE Journals &amp; Magazine | IEEE Xplore</a><br>我们提出了一种实时检测运动和噪声（MN）伪影的方法，当从动态心电图监视器收集心电图信号时，它经常会干扰准确的节律评估。我们的 MN 伪影检测方法包括两个阶段。第一阶段涉及使用经验模式分解中的一阶内在模式函数（F-IMF）来隔离伪影的动态，因为它们主要集中在较高的频率。我们方法的第二阶段使用 F-IMF 时间序列上的三个统计度量来寻找随机性和可变性的特征，这是 MN 伪影的标志性特征：香农熵、均值和方差。然后，我们使用来自15名健康受试者的 Holter 数据的接收器-操作员特征曲线来导出与这些统计度量相关的阈值，以在干净和 MN 伪影的数据段之间进行分离。使用来自15个训练数据集的阈值，我们在另外30名健康受试者身上测试了我们的算法。我们的结果表明，我们的算法能够检测到 MN 伪影的存在，灵敏度和特异性分别为96.63%和94.73%。此外，当我们将我们之前开发的用于心房颤动（AF）检测的算法应用于那些被标记为不含 MN 伪影的片段时，特异性从73.66%增加到85.04%，而不会损失灵敏度（74.48%-74.62%）在6名被诊断为 AF 的受试者身上。最后，使用 MATLAB 代码计算时间小于0.2 s，表明算法的实时应用对于动态心电图监测是可能的。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10025094">Noise Removal of ECG signal using Multi-Techniques | IEEE Conference Publication | IEEE Xplore</a><br>心电图（ECG）信号受到广泛噪声源的严重影响。信号可以通过从表示中去除不需要的成分来去噪。我们提出了新方法，演示了它如何处理信号，并讨论了它的属性。<strong>电力线干扰（PLW）、基线噪声、电极运动伪影噪声和肌电图（EMG）噪声</strong>是 ECG 信号上最常见的受影响噪声。去噪 ECG 信号是获得纯信号特征的关键步骤，可以提取这些特征以进行准确诊断。这项研究侧重于 ECG 信号中常见噪声的各种来源，以及去除噪声的信号处理技术。<strong>离散小波变换</strong>可用于从 ECG 信号（DWT）中去除基线噪声。电力线噪声可以通过 Notch 滤波器将其去除。自适应滤波被认为是消除肌电噪声的好方法，我们可以用一种新算法来解决这个问题。最小均方（LMS）自适应滤波器和递归最小二乘（RLS）滤波器用于去除电极运动伪影噪声。MIT-BIH 心律失常数据集利用 Matlab2019b 程序进行心电图分析。</p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s41870-023-01615-x">An efficient approach for denoising EOG artifact through optimal wavelet selection | International Journal of Information Technology (springer.com)</a><br>Electroencephalography（EEG）是一种非侵入性方法，用于捕获大脑神经元产生的电势，这对于诊断癫痫、睡眠障碍、脑肿瘤和痴呆症等神经系统疾病至关重要。然而，EEG 信号有时会被不希望的信号污染，称为伪影。这些伪影是由视网膜偶极运动和眼睑运动产生的，导致 EEG 记录中具有显着振幅和扭曲的尖峰。研究人员已经探索了基于小波的技术来从 EEG 信号中去除 EOG 伪影，其中 Haar、Symlet 和 Daubechies 是常见的小波。然而，选择最合适的小波仍然是一个挑战。拟议的研究发现，db7是去除 EOG 伪影的最佳小波，最低 RMSE 值为54.97，计算时间为0.5798 s。所提出的小波独立分量分析方法优于现有的独立分量分析方法，特别是 Fast-ICA，计算时间（\DeltaC_t）为6.27 s。所提出的方法旨在通过选择合适的母小波函数有效地降低噪声来提高 EOG 信号的质量。</p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s13369-023-07845-2">Two-Stage Motion Artifact Reduction Algorithm for rPPG Signals Obtained from Facial Video Recordings | Arabian Journal for Science and Engineering (springer.com)</a><br>近年来，发表了许多关于非接触式测量和监测从面部视频记录中推断出的心率信号的研究文章。这些文章中提出的技术，例如检查婴儿心率的变化，在许多不希望直接放置任何硬件设备的情况下提供了非侵入性评估。然而，在包括噪声运动伪影的情况下进行精确测量仍然是一个需要克服的障碍。在这篇研究文章中，提出了一种面部视频记录中降噪的两阶段方法。系统的第一阶段包括将每（30）秒获取的信号分成（60）个分区，然后将每个分区移动到平均水平，然后重新组合它们以形成估计的心率信号。第二阶段利用小波变换对从第一阶段获得的信号进行去噪。将去噪信号与从脉搏血氧仪获取的参考信号进行比较，从而产生平均偏差误差（0.13）、均方根误差（3.41）和相关系数（0.97）。所提出的算法应用于（33）个正在接受正常网络摄像头以获取其视频记录的个体，这可以很容易地在家庭、医院或任何其他环境中进行。最后，值得注意的是，这种无创远程技术对于获取心脏信号同时保持社交距离很有用，这是2019年冠状病毒病当前时期的一个可取特征。</p>
<p><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6387309/">Motion Artifact Reduction for Wrist-Worn Photoplethysmograph Sensors Based on Different Wavelengths - PMC (nih.gov)</a><br>腕戴式光电容积描记器（PPG）传感器的长期心率（HR）监测能够以高用户舒适度评估日常生活中的健康状况。然而，PPG 信号容易受到运动伪影（MA）的影响，这显著影响了估计的生理参数（如 HR）的准确性。本文针对腕戴式 PPG 传感器提出了一种基于不同波长的 MA 去除的新型模块化算法框架。该框架使用绿色 PPG 信号进行 HR 监测，并使用红外 PPG 信号作为运动参考。所提出的框架包括四个主要步骤：运动检测、使用连续波长变换的运动去除、近似 HR 估计和信号重构。针对6名健康受试者执行21种运动的数据集，根据心电图（ECG）的 HR 误差评估所提出的算法。提出的 MA 去除方法在周期性、随机和连续非周期性运动情况下分别将 HR 估计的平均误差从4.3、3.0和3.8 bpm 降低到0.6、1.0和2.1 bpm。</p>
<p>2022 <a target="_blank" rel="noopener" href="https://www.mdpi.com/2072-666X/14/2/252">Micromachines | Free Full-Text | Photoplethysmography-Based Distance Estimation for True Wireless Stereo (mdpi.com)</a><br>最近，人们研究了用可穿戴设备提供医疗保健服务。对于资源有限（例如空间、功耗和面积）的真正无线立体声（TWS），要实现这一点，需要用一个传感器同时实现多种功能。Photoplethysmography（PPG）传感器是一种代表性的医疗保健传感器，它根据心率测量重复数据。然而，由于PPG数据是生物学的，它们受到运动伪影和受试者特征的影响。因此，PPG数据需要降噪。在本文中，我们提出了TWS的PPG信号的距离估计算法。对于距离估计，我们设计了一个波形调整（WA）滤波器，该滤波器在保持前后数据关系的同时最小化噪声，一个名为MobileNet的轻量级深度学习模型和一个PPG监控测试平台。距离估计的标准数量设置为三个。为了验证所提算法，我们将几个指标与其他滤波器和AI模型进行了比较，所提算法的最高准确率、精确度、召回率和f1得分分别为92.5%、92.6%、92.8%和0.927，当信号长度为15时，其他算法的实验结果在某些情况下表现出比所提算法更高的指标，但所提模型表现出最快的推理时间。</p>
<h2 id="machine-learning"><a href="#machine-learning" class="headerlink" title="machine learning"></a>machine learning</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8561280/">Bidirectional Recurrent Auto-Encoder for Photoplethysmogram Denoising | IEEE Journals &amp; Magazine | IEEE Xplore</a><br>随着智能手表和移动医疗保健市场的发展，Photoplethysmography（PPG）变得无处不在。然而，PPG容易受到各种类型的噪声的影响，这些噪声在不受控制的环境中永远存在，获得有意义信号的关键取决于PPG的成功去噪。在这种情况下，已经开发了去噪PPG的算法，但许多算法在受控设置中得到验证，或者依赖于必须全部正常工作的多个步骤。本文提出了一种基于双向循环去噪自编码器（BRDAE）的新型PPG去噪算法，该算法需要最少的预处理步骤，并且具有波形特征强调的好处，而不是简单的去噪。BRDAE在具有人工增强噪声的数据集上进行了训练和验证，并在一个大型开放数据库上进行了测试，该数据库包含从重症监护病房患者收集的PPG信号以及从9名受试者24小时内日常生活中间歇性收集的PPG数据。使用经过训练的BRDAE去噪在验证期间将噪声增强数据的信噪比提高了7.9 dB。在测试数据集中，去噪的PPG在与参考相关性和均方根误差方面与原始PPG相比，在心率检测方面显示出统计学上的显着改善。这些结果表明，所提出的方法是去噪PPG信号的有效解决方案，并通过为脉搏波形分析提供PPG特征强调来承诺超越传统去噪的值。</p>
<h1 id="情绪"><a href="#情绪" class="headerlink" title="情绪"></a>情绪</h1><h2 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h2><h2 id="头戴式-HMD"><a href="#头戴式-HMD" class="headerlink" title="头戴式 HMD"></a>头戴式 HMD</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9786815">Analyzing the Effect of Diverse Gaze and Head Direction on Facial Expression Recognition With Photo-Reflective Sensors Embedded in a Head-Mounted Display | IEEE Journals &amp; Magazine | IEEE Xplore</a><br>作为头戴式显示器（HMD）用户的面部表情识别技术之一，嵌入式光反射传感器已经被使用。在本文中，我们研究了注视和面部方向如何影响使用嵌入式光反射传感器的面部表情识别。首先，我们通过移动1）眼睛和2）头部来收集五种面部表情（中性、快乐、愤怒、悲伤、惊讶）的数据集。使用该数据集，我们通过五种方式构建面部表情分类器并评估每个分类器的分类精度来分析注视和面部方向的影响。结果显示，学习所有注视点数据的单个分类器实现了最高的分类性能。然后，我们调查了哪个面部部位受到注视和面部方向的影响。结果表明，注视方向影响面部上部，而面部方向影响面部下部。此外，通过去除面部表情再现性的偏差，我们在三种条件下调查了注视和面部方向的纯效应。结果表明，在注视方向方面，为每个方向构建分类器显著提高了分类精度。然而，在面部方向方面，分类器条件之间存在细微差异。我们的实验结果暗示，多个注视和面部方向对应的多个分类器提高了面部表情识别精度，但收集注视和面部垂直运动的数据是提高面部表情识别精度的实用解决方案。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9786815">Analyzing the Effect of Diverse Gaze and Head Direction on Facial Expression Recognition With Photo-Reflective Sensors Embedded in a Head-Mounted Display | IEEE Journals &amp; Magazine | IEEE Xplore</a><br>作为头戴式显示器（HMD）用户的面部表情识别技术之一，嵌入式光反射传感器已经被使用。在本文中，我们研究了注视和面部方向如何影响使用嵌入式光反射传感器的面部表情识别。首先，我们通过移动1）眼睛和2）头部来收集五种面部表情（中性、快乐、愤怒、悲伤、惊讶）的数据集。使用该数据集，我们通过五种方式构建面部表情分类器并评估每个分类器的分类精度来分析注视和面部方向的影响。结果显示，学习所有注视点数据的单个分类器实现了最高的分类性能。然后，我们调查了哪个面部部位受到注视和面部方向的影响。结果表明，注视方向影响面部上部，而面部方向影响面部下部。此外，通过去除面部表情再现性的偏差，我们在三种条件下调查了注视和面部方向的纯效应。结果表明，在注视方向方面，为每个方向构建分类器显著提高了分类精度。然而，在面部方向方面，分类器条件之间存在细微差异。我们的实验结果暗示，多个注视和面部方向对应的多个分类器提高了面部表情识别精度，但收集注视和面部垂直运动的数据是提高面部表情识别精度的实用解决方案。</p>
<h2 id="Non-invasive"><a href="#Non-invasive" class="headerlink" title="Non-invasive"></a>Non-invasive</h2><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544793.3560325">OCOsense Glasses for Facial Expressions Recognition and Contextual Affective Computing in Real World and Augmented Reality | Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers</a><br>本文介绍了带有集成传感器的新型 OCOSenseTM 智能眼镜，主要是非接触式光学照相（OMG）OCOTM 传感器、9轴惯性测量单元（IMU）和高度计。该眼镜与智能手机应用程序连接，该应用程序有助于面部肌肉激活和头部运动的连续和实时测量，从而允许实时检测面部表情和用户的活动。我们将展示该系统在实践中的使用方式，即参与者将佩戴 OCOSenseTM 眼镜，该眼镜将传感器数据流式传输到平板电脑上，传感器数据的实时可视化和数据解释将在平板电脑上呈现，例如面部表情（微笑、皱眉、惊讶）和活动。我们相信 OCOSenseTM 眼镜是可穿戴设备的下一件大事，它将允许更好地理解用户的环境、活动、情绪状态等，这可以很容易地结合在增强现实和扩展现实环境中。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3448891.3450332">Consumer Wearables and Affective Computing for Wellbeing Support | MobiQuitous 2020 - 17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services (acm.org)</a><br>本文介绍了带有集成传感器的新型 OCOSenseTM 智能眼镜，主要是非接触式光学照相（OMG）OCOTM 传感器、9 轴惯性测量单元（IMU）和高度计。该眼镜与智能手机应用程序连接，该应用程序有助于面部肌肉激活和头部运动的连续和实时测量，从而允许实时检测面部表情和用户的活动。我们将展示该系统在实践中的使用方式，即参与者将佩戴 OCOSenseTM 眼镜，该眼镜将传感器数据流式传输到平板电脑上，传感器数据的实时可视化和数据解释将在平板电脑上呈现，例如面部表情（微笑、皱眉、惊讶）和活动。我们相信 OCOSenseTM 眼镜是可穿戴设备的下一件大事，它将允许更好地理解用户的环境、活动、情绪状态等，这可以很容易地结合在增强现实和扩展现实环境中。</p>
<h2 id="voice"><a href="#voice" class="headerlink" title="voice"></a>voice</h2><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544549.3585869">I Know Your Feelings Before You Do: Predicting Future Affective Reactions in Human-Computer Dialogue | Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>当前口语对话系统（SDS）通常充当被动听众，仅在接收到用户语音后才做出反应。为了实现类似人类的对话，我们提出了一种新颖的未来预测架构，该架构允许SDS在用户说话之前根据其当前行为预测未来的情感反应。在这项工作中，我们调查了两种场景：语音和笑声。在语音中，我们建议根据其与系统当前情感的时间关系以及与系统当前对话行为（DA）的因果关系来预测用户未来的情感。在笑声中，我们建议使用系统在当前回合中的笑声行为来预测用户笑声的发生和类型。对人机对话的初步分析证明了人类和机器人所显示的情感和笑声的同步性，以及他们对话中的DA-情感因果关系。这验证了我们的架构可以为预期SDS的开发做出贡献。</p>
<h2 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h2><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3491102.3501835">Affective State Prediction from Smartphone Touch and Sensor Data in the Wild | Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>对用户情感状态的了解可以通过提供更个性化的体验（例如，搜索结果和新闻文章）来改善他们与智能手机的互动。我们提出了一种基于现实世界环境中智能手机上收集的数据的情感状态分类模型。从击键过程中的触摸事件和来自惯性传感器的信号中，我们提取了二维热图作为输入到卷积神经网络中，以预测智能手机用户的情感状态。为了评估，我们在10周内对82名参与者进行了野外数据采集。我们的模型准确地预测了三个级别（低、中、高）的效价（AUC高达0.83）、唤醒（AUC高达0.85）和支配性（AUC高达0.84）。我们还表明，仅使用惯性传感器数据，我们的模型就实现了类似的性能（AUC高达0.83），使我们的方法对隐私的侵犯更小。通过向用户个性化我们的模型，我们表明性能增加了额外的0.07 AUC。<br><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544549.3583913">TOUCHLESS: Demonstrations of Contactless Haptics for Affective Touch. | Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>在这项工作中描述了一组非接触式触觉原理的演示者。这些技术基于静电毛发勃起、化学化合物和超声波。此外，还展示了与情感触摸相关的应用，从讲故事到生物信号传输，伴随着一个简单的应用程序，以简单的方式编辑动态触觉模式。演示者是Touchless项目的结果，这是一个H2020欧洲合作项目，整合了3所大学和3家公司。这些演示者是非接触式触觉体验，因此促进了来和互动的范例，用户可以接近演示亭并直接体验应用程序，而无需佩戴设备，使体验快速卫生。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544548.3581288">FabTouch: A Tool to Enable Communication and Design of Tactile and Affective Fabric Experiences | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>织物的触觉体验不仅是一种感官体验，也是一种情感体验。我们对织物产品的选择，如服装，通常基于它们的感觉。有效地交流这种体验对于设计触觉织物体验至关重要。然而，仍然缺乏对织物触觉和情感体验的全面理解，阻碍了工具的开发，以促进这些体验的交流。在本文中，我们研究了27名参与者对9个棉花样品的织物体验。我们结合定性和定量方法创建了FabTouch，这是一种促进织物体验设计对话的新颖工具。我们发现了织物触摸体验的六个阶段，包括织物触摸反应、感官联想和情感反应。设计师的初步反馈表明，FabTouch可以在实践和教育中丰富设计过程，并可以为物理和数字设计探索创造灵感。</p>
<h2 id="未分类"><a href="#未分类" class="headerlink" title="未分类"></a>未分类</h2><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/2493432.2493440">Your reactions suggest you liked the movie | Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</a><br>本文描述了一个以多种粒度对内容（主要是电影和视频）进行自动评级的系统。我们的主要观察结果是，当今智能手机和平板电脑上丰富的传感器集可用于捕捉用户在这些设备上观看电影时的各种用户反应。例如，从笑声的声音特征来检测哪些场景很有趣，到平板电脑的静止表示激烈的戏剧。此外，与大多数传统系统不同，这些评级不需要只产生一个数字分数，而是可以扩展以捕捉用户体验。我们将这些想法结合到一个基于Android的原型中，称为Pulse，并与11名用户进行测试，每个用户在三星平板电脑上观看了4到6部电影。令人鼓舞的结果显示，用户的实际评级与系统生成的评级之间存在一致的相关性。通过更严格的测试和优化，Pulse可能会成为现实世界采用的候选者。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/2733373.2806276">How Was It? | Proceedings of the 23rd ACM international conference on Multimedia</a><br>在本文中，我们提出了一种通过处理移动传感器数据来理解观众对现场舞蹈表演的反应的方法。我们认为，利用智能手机中已经可用的传感功能，可以潜在地大规模测量观众对表演的隐性反应。在这项工作中，我们利用普通公众在舞蹈表演中佩戴的三轴加速度计来预测对许多调查答案的反应，包括享受、沉浸感、向他人推荐活动的意愿以及情绪的变化。我们还使用接近和加速度传感分析了观看舞蹈表演的行为如何反映在人们随后的社会行为中。据我们所知，这是第一个使用普及移动传感来调查自发反应以预测现场表演的情感评估的工作。使用单个身体佩戴的加速度计来监控一组观众成员，我们能够以90%的平衡分类准确率预测他们是否喜欢该事件。观众身体动作的集体协调也突出了观众后来报告的难忘时刻。在这样的设置中有效地使用身体动作来测量情感反应是特别令人惊讶的，因为传统上，生理信号如皮肤电导或基于大脑的信号是更普遍接受的测量内隐情感反应的方法。我们的实验为自动化技术的研究和通过表演期间和之后的自发和隐性观众反应对现实世界事件进行隐性标记的应用开辟了有趣的新方向。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3059454.3059465">The Play Is a Hit | Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition</a><br>研究表明，生理传感器为量化观众参加文化活动的体验提供了一种有价值的机制。在掌声或问卷调查的力度下，生物传感器提供了细粒度的定时数据，可用于推断观众体验的质量。不幸的是，可用的商业传感器是为实验室或家庭使用和研究而设计的，专注于个人而不是人群的反应。在这项研究中，我们展示了我们自己设计的生理测量系统，该系统克服了在戏剧环境中使用和部署传感器的挑战（匿名性和隐私性，实时收集数据，支持30-100人的大型人群），可穿戴系统有两个独特的功能，区分成人观众和儿童观众。我们报告了我们的实验结果，特别回答了制片人、导演和艺术家提出的研究问题。</p>
<p><a target="_blank" rel="noopener" href="https://dlnext.acm.org/doi/10.1145/3550324">Frisson Waves: Exploring Automatic Detection, Triggering and Sharing of Aesthetic Chills in Music Performances: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 6, No 3</a><br>Frisson是身体反应的感觉和体验，例如颤抖，皮肤刺痛和鸡皮疙瘩。通过促进体现感觉的人际传递来使用夹带，我们呈现“Frisson Waves”，旨在增强现场音乐表演体验。“Frisson Waves”是一个探索性实时系统，用于在音乐表演期间以波浪式模式在观众身上检测、触发和共享frisson。该系统由用于检测frisson的生理感应腕带和用于诱导frisson的热触觉颈带组成。在受控环境中，我们评估了frisson的检测（n&#x3D;19）和触发（n&#x3D;15）。根据我们的发现，我们与48名观众一起使用我们的系统进行了一场野外音乐音乐会，以共享frisson。本文总结了一个访问、触发和共享frisson的框架。我们报告了我们对“Frisson Waves”的研究见解、经验教训和局限性。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3314410">W!NCE: Unobtrusive Sensing of Upper Facial Action Units with EOG-based Eyewear: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 3, No 1</a><br>不显眼和持续监控面部表情的能力对从情感计算到医疗保健和娱乐业的各种应用领域都有影响。标准面部动作编码系统（FACS）以及基于摄像头的方法已被证明可以提供面部表情的客观指标；然而，由于隐私问题和摄像头的尴尬定位，这些方法在移动应用中也可能相当有限。为了弥合这一差距，W！NCE重新使用了市售的Electrooculography-based眼镜（J！NS MEME），用于连续和不显眼地感应高保真的上面部动作单元。W！NCE使用涉及运动伪影去除和面部动作检测的两阶段处理管道检测面部手势。我们通过对17名用户在静止和动态设置下的数据进行广泛评估、持续疼痛监测的试点研究和几个性能基准来验证我们系统的适用性。我们的结果非常令人鼓舞，表明我们可以检测到五个不同的面部动作单元，在静止状态下平均F1得分为0.88，在动态环境下平均F1得分为0.82，并且我们可以准确检测由于疼痛引起的面部手势。</p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-42293-5_6">Exploring Eye Expressions for Enhancing EOG-Based Interaction | SpringerLink</a><br>本文探讨了使用现成的眼动追踪设备JINS MEME对基于EOG的交互的眼睛表情进行分类。先前的研究已经证明了使用眼动（定向，平滑的追逐）和眼睛表情（眨眼，眨眼）的眼动图（EOG）进行免提人机交互的潜力。我们收集了一套全面的14种眼睛手势，以探索在机器学习模型中如何很好地将两种类型的眼睛手势分类在一起。使用使用15个工程特征对我们收集的数据进行训练的随机森林分类器，我们获得了0.77（AUC）的整体分类性能。我们的结果表明，我们可以可靠地对眼睛表情进行分类，增强了免提交互的可用眼睛手势范围。随着基于EOG技术的不断发展和完善，我们的发现对提高该技术的可用性以及需要更丰富的眼睛手势词汇来免提交互的个人具有长期影响。</p>
<p><a target="_blank" rel="noopener" href="https://www.jstage.jst.go.jp/article/transinf/E105.D/9/E105.D_2020ZDL0003/_article">Exploring Sensor Modalities to Capture User Behaviors for Reading Detection (jst.go.jp)</a><br>在开发具有成本效益的计算系统时，使用适当的传感器准确描述用户行为始终很重要。本文使用J！ NS MEME记录的数据集进行细粒度读数检测，这是一种带有眼电图（EOG）、加速度计和陀螺仪传感器的眼部设备。我们为三种传感器的所有可能组合生成模型，并采用自监督学习和监督学习来理解最佳传感器设置。结果表明，只有EOG传感器的性能与其他传感器的最佳组合大致相同。这为选择适当的传感器进行细粒度读数检测提供了见解，从而实现了具有成本效益的计算。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3491101.3519656">Moody Man: Improving creative teamwork through dynamic affective recognition | Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>虽然工作场所的交流现在很大一部分发生在网上，但当前的平台并不完全支持社会认知非语言交流，这阻碍了虚拟团队的共同理解和创造力。鉴于基于文本的交流是虚拟协作的主要渠道，我们提出了一种利用基于人工智能的动态情感识别系统的新颖解决方案。该应用程序以视觉表示和“情绪”（语气、表情符号）和主要“情绪状态”（例如快乐、愤怒）的百分比细分的形式，提供了关于 Slack 中交流情感内容的实时反馈。我们在一个准实验中测试了该应用程序的可用性，该实验有来自不同背景、语言分析和用户访谈的30名参与者。研究结果表明，该应用程序显着增加了虚拟团队内的共享理解和创造力。新出现的主题包括由情感识别辅助的印象形成，支持长期关系发展；确定了与人工智能检测到的透明度和情感复杂性相关的挑战。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3586183.3606805">Pantœnna: Mouth pose estimation for ar&#x2F;vr headsets using low-profile antenna and impedance characteristic sensing | Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</a><br>忠实捕捉用户整体姿势的方法在AR&#x2F;VR中立即得到应用，从多模态输入到富有表现力的化身。尽管身体跟踪受到了最多关注，但嘴巴也特别重要，因为它是语音和面部表情的通道。在这项工作中，我们描述了一种基于射频的新方法，用于使用集成到VR&#x2F;AR耳机底部的天线捕捉嘴巴姿势。我们的方法避开了基于相机的方法固有的隐私问题，同时支持基于音频的方法无法实现的无声面部表情。此外，与EMG和EIT等生物传感方法相比，我们的方法不需要与佩戴者的身体接触，并且可以完全独立于耳机中，提供高度的物理稳健性和用户实用性。我们详细介绍了我们的实施以及两项用户研究的结果，这些研究显示，在没有重新校准的情况下，磨损会话中11个口腔关键点的平均3D误差为2.6毫米。</p>
<h1 id="人体活动识别"><a href="#人体活动识别" class="headerlink" title="人体活动识别"></a>人体活动识别</h1><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3513023">PhyMask: Robust Sensing of Brain Activity and Physiological Signals During Sleep with an All-textile Eye Mask | ACM Transactions on Computing for Healthcare</a><br>临床级可穿戴睡眠监测是一个具有挑战性的问题，因为它需要同时监测大脑活动、眼球运动、肌肉活动、心肺功能和身体大体运动。这需要在不同的位置佩戴多个传感器，以及在头部放置不舒服的粘合剂和分立的电子元件。因此，现有的可穿戴设备要么损害舒适性，要么损害跟踪睡眠变量的准确性。我们提出了PhyMask，这是一种全纺织睡眠监测解决方案，实用且舒适，适合持续使用，仅使用放置在头部的舒适纺织传感器即可获取所有感兴趣的睡眠信号。我们表明，PhyMask可用于准确测量精确睡眠阶段跟踪所需的所有信号，并在现实世界环境中稳健地提取高级睡眠标记，如纺锤体和K复合物。我们针对多导睡眠图（PSG）验证了PhyMask，并表明它明显优于两种commercially-available睡眠跟踪可穿戴设备——Fitbit和Oura Ring。</p>
<p><a target="_blank" rel="noopener" href="https://www.mdpi.com/2072-666X/14/2/252">Micromachines | Free Full-Text | Photoplethysmography-Based Distance Estimation for True Wireless Stereo (mdpi.com)</a><br>最近，人们研究了用可穿戴设备提供医疗保健服务。对于资源有限（例如空间、功耗和面积）的真正无线立体声（TWS），要实现这一点，需要用一个传感器同时实现多种功能。Photoplethysmography（PPG）传感器是一种代表性的医疗保健传感器，它根据心率测量重复数据。然而，由于 PPG 数据是生物学的，它们受到运动伪影和受试者特征的影响。因此，PPG 数据需要降噪。在本文中，我们提出了 TWS 的 PPG 信号的距离估计算法。对于距离估计，我们设计了一个波形调整（WA）滤波器，该滤波器在保持前后数据关系的同时最小化噪声，一个名为 MobileNet 的轻量级深度学习模型和一个 PPG 监控测试平台。距离估计的标准数量设置为三个。为了验证所提算法，我们将几个指标与其他滤波器和 AI 模型进行了比较，所提算法的最高准确率、精确度、召回率和 f1得分分别为92.5%、92.6%、92.8%和0.927，当信号长度为15时，其他算法的实验结果在某些情况下表现出比所提算法更高的指标，但所提模型表现出最快的推理时间。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10054552">A 3-D-Printed Portable EMG Wristband for the Quantitative Detection of Finger Motion | IEEE Journals &amp; Magazine | IEEE Xplore</a><br>高度需要开发一种负担得起的表面肌电图（EMG）腕带，用于改进假肢的控制，适合模仿人类手部功能。通过使用D-打印技术，腕带可定制，适用于各种手臂尺寸或形状。在本文中，我们开发了用于控制应用的3-打印腕带。它由五对定制的蛇形干电极、一个肌电图传感器和一个信号处理印刷电路板（PCB）组成，以同时检测用户的手指运动。腕带通过单传感器系统展示了其在肌电图信号处理中的稳定性，这与其他多传感器系统设备不同。我们验证了蛇形电极的稳定性是可弯曲和可拉伸的。与现有的刚性干电极相比，蛇形电极通过提供与皮肤表面的共形接触来改进信号检测。这种灵活性使电极可以放置在皮肤表面的任何形状。在志愿者进行的一系列测试中，我们表明收集到的肌电信号通过信号处理反映了肌肉运动。在75磅的肌肉收缩下，腕带显示出0.556 mV&#x2F;lb的信号灵敏度和27 dB的信噪比（SNR），以证明其有效的肌电感应能力。此外，我们还定量地展示了不同水平的信号强度和肌肉力量之间的关系。这项工作显示出对假肢领域的先进控制系统及其相应市场的巨大潜力。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3447526.3472037">ARO: Exploring the Design of Smart-Ring Interactions for Encumbered Hands | Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction (acm.org)</a><br>2021<br>通过用于增强数字外设的小型化智能环，指尖计算已经引起了越来越多的兴趣。这种随时可用的输入设备的一个关键优势是不需要握住设备进行交互，因为它在需要时仍然固定在手指上以供访问。这种可穿戴设备可以通过抓握或握住物体，即使手被束缚，也可以与内容进行交互。我们的研究旨在了解这种基本智能环优势的特性。我们设计了一个智能环原型，ARO（空中、环上、对象上交互），它在抓握物体的同时促进输入。为了更好地识别交互可能性，我们展示了一项启发研究的结果，通过该研究，我们将ARO可能的各种微手势形式分组，同时在不同的抓握要求下握住物体。然后，我们探索了用户使用智能环执行不同导航任务（即缩放和平移）的能力。在我们的研究中，与在物体上检测到的手势相比，用户在使用空中或环形交互时效率最高。此外，空中是我们参与者最喜欢的。根据我们的发现，我们总结了对未来智能环和指尖设备设计的建议，以便在双手受到阻碍时实现高效交互。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3501712.3529718">Snoozy: A Chatbot-Based Sleep Diary for Children Aged Eight to Twelve | Proceedings of the 21st Annual ACM Interaction Design and Children Conference</a><br>许多儿童患有睡眠问题，这可能不利于他们的发育和健康。治疗临床医生依靠睡眠日记来评估患者如何体验睡眠。目前使用的睡眠日记是为成年人制作的，并要求父母为他们的孩子填写。儿童的数字睡眠日记可以提供更可靠的报告，并增强儿童参与他们的治疗。我们报告了Snoozy的设计，这是一款基于聊天机器人的睡眠日记，适用于8到12岁的儿童。遵循基于线人的设计方法，我们：1）采访了临床医生和父母2）让儿童作为共同设计者（N&#x3D;8）、用户测试参与者（N&#x3D;17）和现场测试参与者（N&#x3D;5）。早期的工作已经检查了聊天机器人在儿童非临床个人信息学中的潜力。我们的研究展示了儿童如何通过聊天机器人向临床医生报告与睡眠相关的经历，聊天机器人提出清晰和有指导的问题，并以善意和同理心进行交流。</p>
<h2 id="teeth"><a href="#teeth" class="headerlink" title="teeth"></a>teeth</h2><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3397481.3450645">TeethTap: Recognizing Discrete Teeth Gestures Using Motion and Acoustic Sensing on an Earpiece | 26th International Conference on Intelligent User Interfaces (acm.org)</a><br>牙齿手势成为不同情况和可访问性目的的替代输入方式。在本文中，我们介绍了TeethTap，这是一种新颖的免眼和免手输入技术，可以识别多达13种离散的牙齿敲击手势。TeethTap采用可穿戴3D打印耳机，在双耳后带有IMU传感器和接触式麦克风，串联工作以分别检测下巴运动和声音数据。TeethTap使用支持向量机通过融合声学和运动数据来对手势进行噪声分类，并使用运动数据进行手势分类，通过动态时间规整（DTW）距离测量实现K-Neasters-Neighbor（KNN）。一项有11名参与者参加的用户研究表明，在实验室环境中，TeethTap可以识别13种手势，实时分类准确率为90.9%。我们进一步揭示了在单个与两侧都有传感器时不同牙齿手势的准确率差异。此外，我们还探索了现实环境下的激活手势，包括吃饭、说话、走路和跳跃。根据我们的发现，我们进一步讨论了将TeethTap集成到未来设备中的潜在应用和实际挑战。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3154862.3154866">Did you remember to brush? | Proceedings of the 11th EAI International Conference on Pervasive Computing Technologies for Healthcare (acm.org)</a><br>未能定期刷牙可能会产生令人惊讶的严重健康后果，从牙周病到冠心病再到胰腺癌。在照顾老年人和&#x2F;或痴呆症患者时，这个问题尤其令人担忧，因为他们经常忘记或无法进行标准的健康活动，如刷牙、洗手和服药。以确保这些人得到正确的照顾，他们被置于看护人或家庭成员的监督下，同时限制了他们的独立性，并给他们的家庭成员和看护人带来了巨大的负担。为了解决这个问题，我们开发了一种基于手腕安装的加速度计的非侵入性可穿戴系统，以准确识别一个人何时刷牙。我们通过为期一个月的野外研究测试了我们系统的功效，并实现了94%的准确率和0.82的F测量值。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/2750858.2804259">Evaluating tooth brushing performance with smartphone sound data | Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</a><br>本文提出了一种使用从智能手机收集的音频来评估刷牙性能的新方法。为此，我们使用隐马尔可夫模型（HMM）来识别包括各种类型刷牙动作的音频数据，例如刷前牙的外表面和刷后牙的内表面。然后，我们使用HMM的输出构建回归模型来估计刷牙性能分数，例如后内牙刷牙的笔画质量和前牙刷牙的持续时间。用于训练这些回归模型的分数来自专门从事牙科护理指导的牙医，由此产生的回归模型估计与牙医分配的分数密切对应的性能分数。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3570361.3613287">LiT: Fine-grained Toothbrushing Monitoring with Commercial LED Toothbrush | Proceedings of the 29th Annual International Conference on Mobile Computing and Networking (acm.org)</a><br>忽视适当的口腔卫生已被证明可能导致严重的口腔疾病，随着时间的推移会导致并发症。仔细刷牙可以缓解这个问题，但个人在牙齿的各个区域投入的时间不足是很常见的。我们提出LiT可以实时监控16个Bass技术表面的刷牙情况。LiT依赖于带有蓝色LED的商用牙刷作为发射器，并且只需要2个低成本的光电传感器作为牙刷头上的接收器。然而，光在口腔中的传输通道尚不清楚。找到最佳部署位置并最大限度地减少光电传感器的数量具有挑战性。为了解决这些障碍，我们设计了2个光电传感器的定位，并在口腔内创建了一个传输模型，以从理论上验证可行性。此外，实施中的障碍包括准确分离刷牙动作、门牙外表面光线的干扰和个体变异性。为了克服这些挑战，我们开发了相应的技术和综合框架。对16名用户的实验表明，LiT实现了95.3%的高准确率，刷牙持续时间的误差估计为6.1%。此外，LiT还证明了在用户运动和环境干扰下的弹性。</p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-31891-7_4">YouBrush: Leveraging Edge-Based Machine Learning in Oral Care | SpringerLink</a><br>一个人声称刷牙的时间长度和实际持续时间经常会出现脱节；推荐的刷牙持续时间为2分钟。本文试图弥合这种特殊的脱节。我们推出了YouBrush，这是一款低延迟、低摩擦、响应迅速的移动应用程序，旨在改善用户的口腔护理方案。YouBrush是一款IOS移动应用程序，通过在设备上整合由Apple的createML开发的高度准确的深度学习刷牙检测模型，将以前只有智能牙刷用户才能使用的功能民主化。机器学习模型在边缘运行，为用户提供低延迟、高度响应的脚本指导刷牙体验。此外，我们还精心设计了应用内游戏化技术，以进一步促进用户交互、粘度和口腔护理依从性。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3463494">mTeeth: Identifying Brushing Teeth Surfaces Using Wrist-Worn Inertial Sensors: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 5, No 2</a><br>确保每天刷牙时所有牙齿表面都被充分覆盖可以降低几种口腔疾病的风险。在本文中，我们提出了mTeeth模型，以使用腕戴式惯性传感器检测在自然自由生活环境中使用手动牙刷刷牙的牙齿表面。为了明确标记对应于不同表面的传感器数据并捕获仅持续几毫秒的所有过渡，我们提出了一种轻量级方法来检测刷牙笔画的微事件，该方法可以清晰地划分刷牙表面之间的过渡。使用从刷牙笔画中提取的特征，我们提出了一种贝叶斯集成方法，该方法利用了牙齿表面之间的自然层次结构和它们之间的过渡模式。为了训练和测试，我们丰富了从自然环境中收集的公开可用的腕戴式惯性传感器数据集，其中包含刷牙表面时间和过渡时刻的时间同步精确标签。我们从114集注释10,230个不同表面上的刷牙实例，并评估广泛的人与人之间和内部人之间的插曲变异性对机器学习模型的刷牙表面检测性能的影响。</p>
<h1 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h1><h2 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9878163">Mordo: Silent Command Recognition Through Lightweight Around-Ear Biosensors | IEEE Journals &amp; Magazine | IEEE Xplore</a><br>智能设备的普及鼓励了对可穿戴人机交互的日益增长的要求。为了提高用户接受度，此类交互需要易于操作和不引人注目的特性。在本文中，我们首次提出通过轻量级和环绕耳生物传感系统Mordo来识别无声命令，该系统可以轻松地与<strong>耳机</strong>集成，操纵智能设备，并最大限度地减少社交尴尬。特别是，我们首先确定构建命令的经验原则，并根据环绕耳配置对命令进行实验筛选。其次，我们根据单通道信噪比（SNR）和分类精度选择最佳环绕耳传感器配置。第三，我们提出了一种多流CNN-LSTM网络来学习环绕耳信号和命令之间的时空映射。最后，进行了广泛的实验来评估可行性和稳定性。结果表明，平均准确率为89.66%，优于类似任务的其他算法。稳定性测试表明，我们的系统在命令变形和头部运动下表现出足够的稳定性。我们通过逐渐减小训练数据大小来证明收集这种规模数据的必要性。我们还通过降低空间和时间分辨率来验证我们的方法对其他传感参数的泛化能力。概念验证设计将旨在进一步开发用于无声命令识别的商业产品。</p>
<h1 id="健康"><a href="#健康" class="headerlink" title="健康"></a>健康</h1><p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41598-022-18356-9">Agreement between two photoplethysmography-based wearable devices for monitoring heart rate during different physical activity situations: a new analysis methodology | Scientific Reports (nature.com)</a><br>可穿戴设备越来越多地被用于监测心率（HR）。然而，它们在研究或临床水平上分析连续 HR 的有用性是值得怀疑的。这项研究的目的是分析不同可穿戴设备在基于 photoplethysmography 测量 HR 时的一致性水平，根据不同的身体位置和身体活动水平，并与黄金标准心电图进行比较。所提出的方法测量了几个时间尺度之间的一致性，因为不同的可穿戴设备以不同的采样率获得 HR。18名大学生（10名男性，8名女性；22±2.45岁）参加了一项实验室研究。参与者同时佩戴 Apple Watch 和 Polar Vantage 手表。心电图使用 BIOPAC 系统测量。三种设备连续同时记录 HR，在4种不同情况下连续5分钟：仰卧、坐着、站着和在跑步机上以4 km&#x2F;h 的速度行走。心率估计是通过每个设备的软件提供的最大精度获得的，并通过在几个时间尺度上取平均值进行比较，因为可穿戴设备在不同的采样率下获得心率，尽管5秒和30秒的结果更详细。布兰德-奥特曼（B-A）图显示，当参与者躺下时，心电图和任何智能手表的数据之间没有明显差异。在这个位置，在5秒和30秒取平均值时，偏差很低。不同的是，B-A 图显示，当情况涉及某种程度的身体活动时，存在差异，尤其是对于较短的时期。也就是说，当在跑步机上行走时和在短时间尺度内行走时，设备和心电图之间的差异更大。显示差异最大的设备是 Polar Watch，结果最好的是 Apple Watch。我们得出的结论是，photoplethysmography-based 可穿戴设备适合定期监测心率平均值，尤其是在休息时，但对于用于研究或临床目的的心率持续分析，尤其是当涉及某种程度的身体活动时，它们的可行性仍有争议。这项工作的一个重要贡献是一个新的方法论，用于同步和测量协议与两个或多个设备在不同且不一定均匀的步速测量心率的黄金标准。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10095793">Mouth Breathing Detection Using Audio Captured Through Earbuds | IEEE Conference Publication | IEEE Xplore</a><br>口呼吸与各种负面健康结果有关，包括睡眠相关障碍和牙齿问题。在日常环境中检测口呼吸可能有助于早期干预和扭转负面影响。然而，现有研究尚未充分探索在日常环境中检测口呼吸的方法。这项研究提出了一种机器学习方法，使用市售耳塞捕获的音频来检测口呼吸。通过利用耳塞日益普及的健康监测，这种方法提供了一种更方便和无创的检测口呼吸的手段。我们对30名参与者进行了一项数据采集研究，以训练基于卷积神经网络的模型，该模型在检测口呼吸方面实现了78.4%的准确率。我们的发现表明，使用耳塞的基于音频的口呼吸检测可能是早期干预和改善健康结果的有前途的工具。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3569480">EarSpiro: Earphone-based Spirometry for Lung Function Assessment: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 6, No 4</a><br>肺活量测定法是评估肺功能的金标准。最近的研究提出，移动设备可以经济高效地测量肺功能指标。然而，这些设计在两个方面存在不足。首先，它们无法提供流量-体积（F-V）曲线，该曲线比肺功能指标更具信息量。其次，这些解决方案缺乏吸气测量，这对可变胸外梗阻等肺部疾病很敏感。在本文中，我们介绍了 EarSpiro，这是一种基于耳机的解决方案，它将呼吸量测定法测试期间记录的气流声音解释为 F-V 曲线，包括呼气和吸气测量。EarSpiro 利用卷积神经网络（CNN）和循环神经网络（RNN）来捕获气流声音和气流速度之间的复杂相关性。同时，EarSpiro 采用基于聚类的分割算法来跟踪来自原始音频录制的微弱吸气信号，以实现吸气测量。我们还启用 EarSpiro，每天使用类似烟嘴的对象，例如使用迁移学习的漏斗和解码器网络，仅借助用户提供的几个真实肺功能指数。对60名受试者的广泛实验表明，EarSpiro 在呼气和吸气流速估计方面实现了0.20L&#x2F;s 和0.42L&#x2F;s 的平均误差，在呼气和吸气 F-V 曲线估计方面实现了0.61L&#x2F;s 和0.83L&#x2F;s 的平均误差。估计的 F-V 曲线与真实曲线之间的平均相关系数为0.94。四种常见肺功能指数的平均估计误差为7.3%。</p>
<h1 id="imu"><a href="#imu" class="headerlink" title="imu"></a>imu</h1><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3569894">SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-grained Finger Microgestures | ACM Transactions on Computer-Human Interaction</a><br>通过徒手和抓握日常物体的手势交互可以实现始终可用的输入。为了感知这样的手势，用户手的最小仪器是可取的。然而，由于包括不同手指手势、物体和抓握的多因素空间的复杂性，选择有效但最小的IMU布局仍然具有挑战性。我们介绍了SparseIMU，这是一种快速选择基于惯性传感器的最小布局以实现有效手势识别的方法。此外，我们还提供了一种计算工具来指导设计人员优化传感器放置。我们的方法建立在我们通过17个惯性测量单元（IMU）的密集网络收集的广泛微手势数据集之上。我们进行了一系列分析，包括对徒手和抓握微手势（393 K布局）的整个组合空间的评估，并量化了不同布局选择的性能，揭示了IMU的新手势检测机会。最后，我们用四个场景展示了我们方法的多功能性。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3571306.3571390">Invited Paper: Hierarchical Activity Recognition with Smartwatch IMU | Proceedings of the 24th International Conference on Distributed Computing and Networking (acm.org)</a><br>普适计算领域的进步为智能设备中运行的大量移动应用程序铺平了道路，这些应用程序旨在协助日常生活的各个方面。在智能家居中的人机交互、老年人的远程监控或日常习惯的普遍改善的背景下，在适当的背景下实时估计人类活动变得越来越重要。与此同时，智能手表不断增强的功能及其优质的手腕式放置方式使其成为解决上述问题的令人兴奋的工具，即使是在高度手动灵巧的任务中也是如此。<br>在这项工作中，我们提出了一个关于智能手表IMU数据的分层框架，以学习不同颗粒度的日常生活活动：从高级别、通用描述到低级别的详细活动。我们在N个层次级别上使用了一群基于CNNLSTM的分类器，训练了一组针对每个任务定制的特征。与标准的非分层方法相比，我们提出的分层方法实现了分类准确性的切实提高，推理时间的速度提高了4倍。我们还提供了一个新的智能手表IMU数据集，用于构建和评估我们的方法，以便社区未来的研究可以从中受益。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3544793.3560337">On the Effectiveness of Virtual IMU Data for Eating Detection with Wrist Sensors | Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers</a>人类活动识别（HAR）系统的成功训练通常很大程度上依赖于足够数量的标记传感器数据的可用性。不幸的是，获得大规模标记数据集通常很昂贵，并且经常受到实际和&#x2F;或隐私原因的限制。最近，IMUtube被引入以通过从不受约束的视频存储库（如YouTube）生成弱标记的虚拟IMU数据来解决这一数据稀缺问题。IMUtube被证明在对涉及身体部位大运动的运动或健身房锻炼进行分类方面非常有效。然而，许多重要的日常活动，如吃饭，并没有表现出如此实质性的身体（部分）运动，而是基于更微妙、更细粒度的运动。这项工作探索了IMUtube在这种细微运动活动中的实用性，并在饮食检测方面具有特定的示范性应用。我们发现——令人惊讶的是——IMUtube对于这个具有挑战性的HAR领域也非常有效。我们的实验表明，饮食识别系统受益于从视频数据集中提取的虚拟IMU数据，识别精度显着提高（相对于基线的71.5%F1分数，精选和野外视频数据集的F1分数绝对值分别增加了8.4%和5.9%），这对于IMUtube等系统的更广泛使用是令人鼓舞的。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3411764.3445182">HulaMove: Using Commodity IMU for Waist Interaction | Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>我们展示了HulaMobile，这是一种新颖的交互技术，它利用腰部的运动作为物理世界和虚拟世界的一种新的免眼和免手输入方法。我们首先进行了一项用户研究（N&#x3D;12），以了解用户控制腰部的能力。我们发现用户可以轻松区分八个移位方向和两个旋转方向，并通过返回原始位置（快速返回）来快速确认动作。我们根据结果开发了一个带有八个手势的腰部交互设计空间，并实现了一个基于IMU的实时系统。使用分层机器学习模型，我们的系统可以以97.5%的准确率识别腰部手势。最后，我们进行了第二次用户研究（N&#x3D;12），用于在现实世界场景和虚拟现实设置中进行可用性测试。我们的可用性研究表明，与触摸屏方式相比，HulaMobile显著减少了41.8%的交互时间，大大提高了用户在虚拟世界中的存在感。这种新颖的技术在用户的眼睛或手忙碌时提供了一种额外的输入方式，加速了用户的日常操作，增强了他们在虚拟世界中的沉浸式体验。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3534620">Shoes++: A Smart Detachable Sole for Social Foot-to-foot Interaction: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies: Vol 6, No 2</a><br>脚是我们身体的基础，不仅执行运动，还参与意图和情感表达。因此，脚的手势是人际互动的一种直观和自然的表达形式。最近的研究大多将智能鞋作为个人小工具引入，而用于社交场景中多人脚互动的脚的手势在很大程度上仍未得到探索。我们展示了鞋++，它包括一个惯性测量单元（IMU）安装的鞋底和一个社交脚对脚手势的输入词汇表，以支持基于脚的交互。手势词汇表是由一组手势导出和浓缩的，这些手势是从与12个用户的参与式设计会议中引出的。我们在鞋++中实现了一个机器学习模型，可以以94.3%和96.6%的准确率识别两人和三人社交脚对脚手势（N&#x3D;18）。此外，鞋底设计可轻松附着和脱离各种日常鞋子，以支持舒适的社交足部互动，而无需脱鞋。基于用户的定性反馈，我们还发现鞋++可以支持团队协作，增强情感表达，从而使社交互动或人际动态在扩展的设计空间中更加参与。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3491102.3501904">Enabling Hand Gesture Customization on Wrist-Worn Devices | Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (acm.org)</a><br>我们提出了一个需要最少用户示例的手势定制框架，所有这些都不会降低现有手势集的性能。为了实现这一目标，我们首先部署了一项大规模研究（N&#x3D;500+）来收集数据并训练一个accelerometer-gyroscope识别模型，在日常非手势数据上进行测试时，该模型的跨用户准确率为95.7%，误报率为每小时0.6次。接下来，我们设计了一个少镜头学习框架，该框架从我们的预训练模型中派生出一个轻量级模型，在不降低性能的情况下实现知识转移。我们通过一项用户研究（N&#x3D;20）验证了我们的方法，该研究检查了来自12个新手势的设备上定制，在添加新手势时使用一个、三个或五个镜头的平均准确率为55.3%、83.1%和87.2%，同时保持与现有手势集相同的识别准确率和误报率。我们通过用户体验研究（N&#x3D;20）进一步评估了我们实时实施的可用性。我们的结果突出了我们定制框架的有效性、可学习性和可用性。我们的方法为用户不再受制于预先存在的手势铺平了道路，让他们能够创造性地引入适合自己偏好和能力的新手势。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3594739.3610736">Where Are the Best Positions of IMU Sensors for HAR? - Approach by a Garment Device with Fine-Grained Grid IMUs - | Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing</a><br>已经提出了许多基于IMU数据的HAR数据集。然而，整合这些数据集的使用是困难的，因为每个数据集都是由其独特的传感器位置测量的。因此，我们开发了一种服装设备，其中396个IMU传感器放置在一个大约7厘米行间距的网格中，以实现来自不同位置的IMU数据的匹配和多个数据库的集成使用。在本文中，使用该设备探索了用于分类的最佳和最合适的传感器位置，用于六个基本活动和八个日常活动，期望确定集成数据集的关键IMU位置，并在构建新数据集时创建标准传感器位置。在所有活动、基本活动和日常活动中分类性能最高的单个传感器位置是优势下臂的中上部、大腿前部和优势手臂的内手腕。除书写和打字外，日常活动分类得分最高的位置是优势臂的内手腕。书写分类得分最高的传感器位置是优势上臂的上部，打字是非优势下臂的中上部。此外，四个传感器的组合显示出所有活动集的最佳宏F1分数：所有活动为0.90，基本活动为1.00，日常活动为0.83。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3333581.3333597">A Road Condition Classifier via Lock Embedded IMU on Dock-Less Shared Bikes | Proceedings of the International Conference on Industrial Control Network and System Engineering Research (acm.org)</a>近年来，共享单车在全球范围内越来越受欢迎，骑手数量也在迅速增加。然而，并非所有骑手都具有相同的骑行能力，这些差异可能会对初学者构成风险。此外，虽然某些类型的自行车（如山地车）可以处理颠簸和洞，但不确定的路况可能会使共享单车的骑手感到不舒服，甚至危险。为了解决这个问题，应该通过一些有效的方法检测路况并上传到在线地图，以便骑手可以选择适合其骑行偏好的路线。因此，我们设计了一种基于共享单车上的锁嵌入式惯性测量单元（IMU）的道路分类器，并在骑行时启用了路面检测。为了训练和评估该系统，招募了20名受试者，使用嵌入式IMU收集无码头共享单车上的数据。为了准确分类路况，首先进行数据旋转和特征提取。然后，使用线性判别分析（LDA）建立最终模型。进行交叉验证，表明沥青路面、鹅卵石路径和颠簸路径路面分类模型的准确率为95.3%，这在在线地图的信息扩展方面显示出良好的潜力，可以显着提高骑手体验。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3550469.3555428">Transformer Inertial Poser: Real-time Human Motion Reconstruction from Sparse IMUs with Simultaneous Terrain Generation | SIGGRAPH Asia 2022 Conference Papers (acm.org)</a><br>从一组稀疏的（例如六个）可穿戴IMU进行实时人体运动重建提供了一种非侵入性和经济的动捕方法。由于无法直接从IMU获取位置信息，最近的工作采用了数据驱动的方法，利用大型人体运动数据集来解决这一不确定的问题。尽管如此，挑战仍然存在，如时间一致性、全局和关节运动的漂移以及不同地形上运动类型的多样化覆盖。我们提出了一种新方法，可以同时估计全身运动，并仅从六个IMU传感器实时生成可信的访问地形。我们的方法包含1.一个条件变压器解码器模型，通过显式推理预测历史给出一致的预测，2.一个简单但通用的学习目标，名为“静止体点”（SBPs），它可以由变压器模型稳定预测，并通过分析例程用于纠正联合和全局漂移，以及3.一种从嘈杂的SBP预测生成正则化地形高度图的算法，该算法反过来可以纠正嘈杂的全局运动估计。我们在综合和真实IMU数据以及实时现场演示上广泛评估我们的框架，并显示出优于强基线方法的卓越性能。</p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-45705-0_18">Functional Drift Filtering of IMU for Long Term Wholebody Motion Capturing | SpringerLink</a><br>使用IMU传感器的运动捕捉相对于其他方法具有优势。如果传感器漂移问题得到解决，它将扩展应用领域。漂移补偿有几种方法。即像麦奇威克滤波器的想法一样补偿单个传感器，通过传感器之间的封闭检查进行补偿，以及采用人体运动的骨骼和行为知识进行补偿，这是目前工作的重点。在本文中，我们报告了基于人体运动的骨骼和行为特征知识的反馈补偿长期全身运动捕捉的功能漂移滤波。我们提出了两种反馈补偿。一种是对解剖中立位置的弱反馈。另一种是从功能不允许的位置恢复的相当强的反馈。我们的实验研究说明了使用所提出的方法的IMU运动捕捉的特点和优势。</p>
<p><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9102309/">Motion Artifacts Correction from Single-Channel EEG and fNIRS Signals Using Novel Wavelet Packet Decomposition in Combination with Canonical Correlation Analysis - PMC (nih.gov)</a><br>electroencephalogram（EEG）和功能性近红外光谱（fNIRS）信号，本质上高度非平稳，在使用可穿戴传感器记录时，极大地受到运动伪影的影响。由于成功检测各种神经和神经肌肉疾病在很大程度上依赖于干净的EEG和fNIRS信号，因此使用可靠和稳健的方法从EEG和fNIRS信号中去除&#x2F;减少运动伪影是至关重要的。在这方面，本文提出了两种稳健的方法：（i）小波包分解（WPD）和（ii）WPD与典型相关分析（WPD-CCA）相结合，用于单通道EEG和fNIRS信号的运动伪影校正。使用基准数据集测试了这些提出的技术的有效性，并使用两个既定的性能矩阵测量了所提出方法的性能：（i）信噪比的差异（）和（ii）减少运动伪影的百分比（）。建议的基于WPD的单级运动伪影校正技术产生最高的平均值（29.44 dB）当合并db2小波包时，而最大平均值对于所有可用的23个EEG记录，使用db1小波包获得（53.48%）。我们提出的两阶段运动伪影校正技术，即利用db1小波包的WPD-CCA方法显示出最佳的去噪性能，产生平均值所有EEG记录的值分别为30.76 dB和59.51%。另一方面，对于可用的16个fNIRS记录，两阶段运动伪影去除技术，即WPD-CCA产生了最佳平均值（16.55 dB，使用db1小波包）和最大平均值（41.40%，使用fk8小波包）。最高平均值和对于所有使用fk4小波包的fNIRS信号，使用单级伪影去除技术（WPD）分别为16.11 dB和26.40%。在EEG和fNIRS模态中，与单级WPD方法相比，采用两级WPD-CCA技术时，运动伪影的减少百分比分别增加了11.28%和56.82%。此外，平均当EEG和fNIRS信号都使用WPD-CCA技术而不是单级WPD时，也会增加。两者的增量<br>和值清楚地表明，与单级WPD相比，两级WPD-CCA的性能相对更好。使用所提出的方法报告的结果优于大多数现有的最先进技术。</p>
<p><a target="_blank" rel="noopener" href="https://www.artinis.com/blogpost-all/2023/the-use-of-inertial-measurement-unit-imu-to-detect-and-remove-motion-artifacts#:~:text=With%20addition%20of%20inertial%20measurement%20unit%20%28IMU%29%20data,NIRS-data%2C%20which%20results%20in%20better%20movement%20artifact%20detection.">The use of Inertial Measurement Unit (IMU) to detect motion artifacts — Artinis Medical Systems | (f)NIRS devices</a></p>
<h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/129345757">2020 年 3月 新番 李宏毅 人类语言处理 独家笔记 StarGAN in VC - 11 - 知乎 (zhihu.com)</a><img src="https://picx.zhimg.com/70/v2-9cd8604bf2acad7b1685675ee1ea29de_1440w.awebp?source=172ae18b&biz_tag=Post"><br>StarGAN 可以看成是 Cycle GAN 在多类别上的拓展版<br>cyclegan <img src="https://img-blog.csdnimg.cn/20190327113307206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjQ3NDgwOQ==,size_16,color_FFFFFF,t_70"><br><a target="_blank" rel="noopener" href="https://deepai.org/machine-learning-glossary-and-terms/generative-adversarial-network">Generative Adversarial Network Definition | DeepAI</a></p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_33935895/article/details/106463386?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170135501416800222895068%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=170135501416800222895068&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-106463386-null-null.142%5Ev96%5Epc_search_result_base1&utm_term=cyclegan%20imu&spm=1018.2226.3001.4449">[论文解读] DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving-CSDN博客</a><br>虽然深度神经网络(DNNs)已经为基于图像的自动驾驶系统奠定了基础，但它们可能会表现出错误的行为，并导致致命的事故。为了解决自动驾驶系统中的安全问题，最近设计了一套测试技术来自动生成人工驾驶场景以丰富测试套件，例如，生成从原始图像转换而来的新输入图像。然而，由于两方面的限制，这些技术是不够的：首先，许多这样的合成图像往往缺乏驾驶场景的多样性，从而影响了所产生的有效性和可靠性。其次，对于基于机器学习的系统，训练域和应用域之间的不匹配会显著降低系统的精度，因此有必要验证输入以提高系统的鲁棒性。</p>
<p>本文提出了一种基于无监督DNN的自动一致性测试框架DeepRoad，用于自动测试基于DNN的自动驾驶系统的一致性和在线验证。首先，DeepRoad无需使用图像变换规则(如缩放、剪切和旋转)即可自动合成大量不同的驾驶场景。特别是，DeepRoad能够通过应用生成性对抗性网络(GAN)以及相应的真实天气场景来生成各种天气条件(包括那些相当极端的条件)的驾驶场景。第二，DeepRoad使用蜕变测试技术使用合成图像检查此类系统的一致性。第三，DeepRoad通过使用VGGNet功能测量输入图像和训练图像之间的距离来验证基于DNN的系统的输入图像。我们使用DeepRoad在Udacity自动驾驶汽车挑战赛中测试了三个公认的基于DNN的自动驾驶系统。实验结果表明，DeepRoad能够检测出这些系统数以千计的不一致行为，并有效地验证了输入图像，从而潜在地增强了系统的鲁棒性.</p>
<h2 id="2020-before"><a href="#2020-before" class="headerlink" title="2020-before"></a>2020-before</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8732945">Mic2Mic: Using Cycle-Consistent Generative Adversarial Networks to Overcome Microphone Variability in Speech Systems | IEEE Conference Publication | IEEE Xplore</a><br>移动和嵌入式设备越来越多地使用麦克风和基于音频的计算模型来推断用户上下文。构建将音频模型与商品麦克风相结合的系统的一个主要挑战是保证它们在现实世界中的准确性和鲁棒性。除了许多环境动态之外，影响音频模型鲁棒性的一个主要因素是麦克风可变性。在这项工作中，我们提出了Mic2Mic——一个机器学习的系统组件——它驻留在音频模型的推理管道中，并<strong>实时减少由麦克风特定因素引起的音频数据的可变性</strong>。Mic2Mic设计的两个关键考虑因素是：a）将麦克风可变性问题与音频任务分离，以及b）将最终用户提供训练数据的负担降至最低。考虑到这些，我们应用循环一致生成对抗网络（CycleGANs）的原理，使用从不同麦克风收集的未标记和未配对的数据来学习Mic2Mic。我们的实验表明，对于两个常见的音频任务，Mic2Mic可以在66之间恢复。</p>
<p><a target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/zhou20d_interspeech.pdf">Converting Anyone’s Emotion: Towards Speaker-Independent Emotional Voice Conversion (isca-speech.org)</a><br>情感语音转换旨在将语音的情感从一种状态转换到另一种状态，同时保留语言内容和说话者身份。先前关于情感语音转换的研究大多是在情感依赖于说话者的假设下进行的。我们考虑到在口语中，说话者之间存在用于情感表达的共同代码，因此，情感状态之间的说话者无关映射是可能的。在本文中，我们提出了一种独立于说话者的情感语音转换框架，它可以在不需要并行数据的情况下转换任何人的情感。我们提出了一种基于VAW-GAN的编码器-解码器结构来学习频谱和韵律映射。我们通过使用连续小波变换（CWT）来对时间依赖关系进行建模来执行韵律转换。我们还研究了使用F0作为解码器的附加输入以提高情感转换性能。实验表明，所提出的与说话人无关的框架对可见和不可见的说话人都取得了竞争性的结果。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9000040">DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems | IEEE Conference Publication | IEEE Xplore</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_33935895/article/details/106463386?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170135501416800222895068%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=170135501416800222895068&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-106463386-null-null.142%5Ev96%5Epc_search_result_base1&utm_term=cyclegan%20imu&spm=1018.2226.3001.4449">[论文解读] DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving-CSDN博客</a><br>虽然深度神经网络（DNN）已经建立了基于图像的自动驾驶系统的基础知识，但它们可能会表现出错误行为并导致致命事故。为了解决自动驾驶系统中的安全问题，最近设计了一套测试技术来自动生成人工驾驶场景以丰富测试套件，例如，生成从原始图像转换而来的新输入图像。然而，由于两个限制，这些技术是不够的：首先，许多这样的合成图像往往缺乏驾驶场景的多样性，从而损害了由此产生的功效和可靠性。其次，对于machine-learning-based系统，训练和应用领域之间的不匹配会极大地降低系统准确性，因此有必要验证输入以提高系统鲁棒性。在本文中，我们提出了DeepRoad，这是一个基于DNN的无监督框架，用于自动测试基于DNN的自动驾驶系统和在线验证的一致性。首先，DeepRoad<strong>自动合成大量不同的驾驶场景，而无需使用图像转换规则（例如缩放、剪切和旋转）</strong>。特别是，DeepRoad能够通过应用生成对抗网络（GAN）以及相应的真实世界天气场景来生成具有各种天气条件（包括那些相当极端的条件）的驾驶场景。其次，DeepRoad利用变质测试技术来检查使用合成图像的此类系统的一致性。第三，DeepRoad通过测量输入的距离并使用其VGGNet特征训练图像来验证基于DNN的系统的输入图像。我们实施DeepRoad以在Udacity自动驾驶汽车挑战赛中测试三个公认的基于DNN的自动驾驶系统。实验结果表明，DeepRoad可以检测到这些系统的数千种不一致行为，并有效地验证输入图像以潜在地…</p>
<h2 id="2022-1"><a href="#2022-1" class="headerlink" title="2022"></a>2022</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9874835">Cross-Speaker Emotion Transfer Through Information Perturbation in Emotional Speech Synthesis | IEEE Journals &amp; Magazine | IEEE Xplore</a><br>通过借用情感说话人的情感表达，跨说话人情感转移是一种在没有情感训练数据的情况下为目标说话人产生情感语音的有效方法。由于源说话人的情感和音色在语音中严重纠缠，现有方法往往难以在目标说话人的合成语音中权衡说话人相似性和情感表达。在这封信中，我们建议通过信息扰动来解开音色和情感，以进行跨说话人情感转移，从而有效地学习源说话人的情感表达并保持目标说话人的音色。具体来说，我们分别扰动源语音的音色和情感相关特征（例如共振峰和音高），以获得和建模与音色和情感无关的信号，在此基础上，所提出的模型可以为目标说话人传递情感表达。实验结果表明，该方法在自然性和相似性方面明显优于基线，表明信息扰动对跨说话人情感转移的有效性。</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9874835">Cross-Speaker Emotion Transfer Through Information Perturbation in Emotional Speech Synthesis | IEEE Journals &amp; Magazine | IEEE Xplore</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/577443023">论文推介：情感语音合成中基于信息扰动的跨说话人情感迁移 - 知乎 (zhihu.com)</a><br>通过借用情感说话人的情感表达，跨说话人情感转移是一种在没有情感训练数据的情况下为目标说话人产生情感语音的有效方法。由于源说话人的情感和音色在语音中严重纠缠，现有方法往往难以在目标说话人的合成语音中权衡说话人相似性和情感表达。在这封信中，我们建议通过<strong>信息扰动来解开音色和情感</strong>，以进行跨说话人情感转移，从而有效地学习源说话人的情感表达并保持目标说话人的音色。具体来说，我们分别扰动源语音的音色和情感相关特征（例如共振峰和音高），以获得和建模与音色和情感无关的信号，在此基础上，所提出的模型可以为目标说话人传递情感表达。实验结果表明，该方法在自然性和相似性方面明显优于基线，表明信息扰动对跨说话人情感转移的有效性。</p>
<h2 id="2023-1"><a href="#2023-1" class="headerlink" title="2023"></a>2023</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.06514">[2306.06514] Vocoder-Free Non-Parallel Conversion of Whispered Speech With Masked Cycle-Consistent Generative Adversarial Networks (arxiv.org)</a><br><a target="_blank" rel="noopener" href="https://github.com/audiodemo/voice-conversion">audiodemo&#x2F;voice-conversion: Vocoder-Free Non-Parallel Conversion of Whispered Speech With Masked Cycle-Consistent Generative Adversarial Networks (github.com)</a><br>循环一致生成对抗网络已广泛应用于<strong>非并行语音转换（VC）</strong>。它们无需依赖并行训练数据即可学习源和目标特征之间的映射，从而消除了时间对齐的需要。然而，大多数方法通过使用单独的模型进行转换和波形合成，将声学特征的转换与合成音频信号解耦。这项工作将<strong>转换和合成统一到单个模型中</strong>，从而消除了对单独声码器的需要。通过利用循环一致训练和自监督辅助训练任务，我们的模型能够有效地生成转换后的高质量原始音频波形。主观聆听测试表明，我们的方法在耳语语音转换中优于基线（最高可达6.7的相对改进），平均意见分数预测在传统VC中产生具有竞争力的结果（在0.5之间</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.05780">[2305.05780] Enhancing Gappy Speech Audio Signals with Generative Adversarial Networks (arxiv.org)</a><br>损坏音频的间隙、丢失和短片段是一个常见的问题，当它们发生在语音中时特别烦人。本文使用机器学习在音频语音信号中重新生成高达320毫秒的间隙。<strong>音频再生</strong>通过将音频转换为梅尔频谱图并使用绘画中的图像来重新生成间隙，从而转化为<strong>图像再生</strong>。然后，使用并行WaveGAN声码器将完整的梅尔频谱图传输回音频并集成到音频流中。使用从公开可用的LJSpeech数据集中获取的1300个1到10秒的语音音频片段样本，我们的结果显示使用配备GPU系统的GAN近乎实时地重新生成音频间隙。正如预期的那样，音频中的间隙越小，填充间隙的质量越好。在240毫秒的差距上，表现最好的模型的平均意见得分（MOS）为3.737，在1（最差）到5（最好）的范围内，这足以让人感知到接近不间断的人类语言。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><hr>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.11512">[2106.11512] An Accurate Non-accelerometer-based PPG Motion Artifact Removal Technique using CycleGAN (arxiv.org)</a><br>photoplethysmography（PPG）是一种简单且廉价的光学技术，广泛应用于医疗保健领域，用于提取有价值的健康相关信息，例如，心率变异性、血压和呼吸频率。使用便携式可穿戴设备可以轻松地连续远程收集PPG信号。然而，这些测量设备容易受到日常生活活动引起的运动伪影的影响。消除运动伪影的最常见方法使用额外的加速度计传感器，这些传感器受到两个限制：（i）高功耗，以及（ii）需要在可穿戴设备中集成加速度计传感器（这在某些可穿戴设备中是不需要的）。本文提出了一种优于现有方法精度的低功耗non-accelerometer-basedPPG运动伪影去除方法。我们使用循环生成对抗网络从嘈杂的PPG信号中重建干净的PPG信号。与最先进的技术相比，我们新颖的machine-learning-based技术在不使用加速度计等额外传感器的情况下实现了9.5倍的运动伪影去除，从而将能效提高了45%。</p>
<p>CycleGAN (Cycle-Consistent Generative Adversarial Network) is a technique that can be used to remove motion artifacts from photoplethysmography (PPG) signals without the need for an accelerometer. PPG signals are commonly used to measure changes in blood volume and heart rate by analyzing the pulsatile waveform.</p>
<p>Motion artifacts occur when there is movement or motion during PPG signal acquisition, leading to distorted or corrupted signals. Traditionally, accelerometer-based approaches are used to compensate for these artifacts, but they introduce additional complexity and hardware requirements.</p>
<p>The proposed technique using CycleGAN aims to remove the motion artifacts solely based on the PPG signal without any additional sensor or hardware. CycleGAN is a deep learning approach that can learn the mapping between two different domains. In this case, it can learn the mapping between the motion-corrupted PPG signal domain and the motion-free PPG signal domain.</p>
<p>The technique involves training a CycleGAN model using pairs of motion-corrupted and motion-free PPG signals. The model learns to translate the motion-corrupted PPG signals into motion-free ones and vice versa. It does this by leveraging the adversarial loss and cycle consistency loss, which ensures that the translated signals are consistent with the original ones.</p>
<p>During the testing phase, the trained CycleGAN model can be used to remove motion artifacts from new motion-corrupted PPG signals. The model takes the motion-corrupted PPG signal as input and generates a motion-free PPG signal as output. This motion-free signal can then be used for further analysis or measurements.</p>
<p>The advantage of this technique is that it eliminates the need for an accelerometer, making it more accessible and cost-effective. It also avoids the limitations and complexities associated with accelerometer-based methods, such as sensor placement and calibration.</p>
<p>Overall, the accurate non-accelerometer-based PPG motion artifact removal technique using CycleGAN offers a promising approach for motion artifact compensation in PPG signals, opening up possibilities for improved analysis and applications in healthcare monitoring and research.</p>
<hr>
<p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41524-023-01042-3">Leveraging generative adversarial networks to create realistic scanning transmission electron microscopy images | npj Computational Materials (nature.com)</a><br>电子显微镜中自动化和机器学习（ML）的兴起有可能通过自主数据采集和处理来彻底改变材料研究。一个重大挑战在于开发ML模型，这些模型可以在不同的实验条件下快速推广到大型数据集。我们通过使用带有互易空间鉴别器的循环生成对抗网络（CycleGAN）来解决这个问题，该网络使用逼真的空间频率信息增强模拟数据。这使得CycleGAN能够生成与真实数据几乎无法区分的图像，并为ML应用程序提供标签。我们通过训练全卷积网络（FCN）来识别450万原子数据集中的单原子缺陷，这些数据集是在aberration-corrected扫描透射电子显微镜（STEM）中使用自动采集收集的。我们的方法产生了适应性强的FCN，可以以最小的干预适应动态变化的实验变量，标志着朝着完全自主利用显微镜大数据迈出了关键一步。</p>
<hr>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.08407.pdf">Bridging the gap between paired and unpaired medical image translation</a><br>医学图像翻译有可能通过消除捕获某些序列的需要来减少成像工作量，并减少开发机器学习方法的注释负担。gan已经成功地用于将图像从一个域转换到另一个域，例如MR到CT。目前，需要配对数据(注册的MR和CT图像)或额外的监督(例如分割蒙版)来学习好的翻译模型。注册多个模态或在每个模态中注释结构是一项繁琐而费力的任务。因此，有必要开发改进的非配对数据翻译方法。在这里，我们引入了改进的pix2pix模型，用于CT→MR和MR→CT任务，该模型使用未配对的CT和MR数据以及从MR扫描生成的MRCAT对进行训练。所提出的修改利用配对的MR和MRCAT图像来确保输入图像和翻译图像之间的良好对齐，未配对的CT图像确保MR→CT模型产生逼真的CT, CT→MR模型在真实CT作为输入时也能很好地工作。提出的pix2pix变体在FID和KID方面优于基线pix2pix, pix2pixHD和CycleGAN，并生成更逼真的CT和MR翻译。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02557">[2203.02557] UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation (arxiv.org)</a><br>未配对图像到图像的翻译在艺术、设计和科学模拟中有着广泛的应用。一个早期的突破是CycleGAN，它强调通过generative-adversarial网络（GAN）以及循环一致性约束在两个未配对图像域之间进行一对一的映射，而最近的工作则促进了一对多的映射，以提高翻译图像的多样性。受科学模拟和一对一需求的驱动，这项工作重新审视了经典的CycleGAN框架，并提高了其性能，以在不放松循环一致性约束的情况下超越更现代的模型。为了实现这一点，我们为生成器配备了一个视觉转换器（ViT），并采用了必要的训练和正则化技术。与以前表现最好的模型相比，我们的模型表现更好，并且在原始图像和翻译图像之间保持了很强的相关性。伴随的消融研究表明，梯度惩罚和自监督预训练对改进都至关重要。为了促进可重复性和开放科学，此https URL提供了源代码、超参数配置和预训练模型。</p>
<p><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8641403/">Application of CycleGAN and transfer learning techniques for automated detection of COVID-19 using X-ray images - PMC (nih.gov)</a><br>冠状病毒（也称为2019冠状病毒病）正在严重影响全球许多人的健康和生活。目前有几种方法可以检测和监测疾病的进展，例如来自患者胸部的放射图像，测量症状并应用聚合酶链反应（RT-PCR）测试。X射线成像是用于可视化病毒对肺部影响的流行技术之一。虽然使用放射学图像手动检测这种疾病更受欢迎，但它可能很耗时，并且容易出现人为错误。因此，利用深度学习（Bowles et al.）技术对2019冠状病毒病导致的肺部病理进行自动检测可以帮助为庞大的数据库产生准确的结果。需要大量数据来实现可推广的DL模型；然而，很少有公共数据库可用于自动检测2019冠状病毒病病理。可以使用标准数据增强方法来增强模型的通用性。在这项研究中，使用了广泛的2019冠状病毒病X射线和CT胸部图像数据集，并应用生成对抗网络（GAN）与经过训练的半监督CycleGAN（SSA-CycleGAN）相结合来增强训练集。然后开发了一个新设计和微调的初始V3迁移学习模型来训练检测2019冠状病毒病大流行的算法。从提议的初始-CycleGAN模型中获得的结果表明准确性&#x3D;94.2%，曲线下的面积&#x3D;92.2%，均方误差&#x3D;0.27，平均绝对误差&#x3D;0.16。开发的初始-CycleGAN框架已准备好接受进一步的2019冠状病毒病胸部X射线图像测试。</p>
<p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41598-023-35794-1">Evaluation of motion artefact reduction depending on the artefacts’ directions in head MRI using conditional generative adversarial networks | Scientific Reports (nature.com)</a><br>由患者身体运动引起的运动伪影影响磁共振成像（MRI）的准确性。本研究旨在比较和评估使用条件生成对抗网络（CGAN）与自动编码器和U-net模型的运动伪影校正的准确性。训练集由通过模拟生成的运动伪影组成。运动伪影发生在相位编码方向，该方向设置为图像的水平或垂直方向。为了创建具有模拟运动伪影的T2加权轴向图像，每个方向使用了5500个头部图像。在这些数据中，90%用于训练，而其余部分用于画质的评估。此外，模型训练中使用的验证数据由10%的训练集组成。训练数据分为运动伪影外观的水平和垂直方向，并验证了将该数据与训练集结合的效果。使用结构图像相似性（SSIM）和峰值信噪比（PSNR）评估校正后的图像，并将指标与没有运动伪影的图像进行比较。在训练和评估数据集中，在运动伪影出现方向一致的条件下，SSIM和PSNR的改进效果最好。然而，对于具有两个图像方向的学习模型，SSIM&gt;0.9和PSNR&gt;29 dB。后一种模型对头部MRI图像中的实际患者运动表现出最高的鲁棒性。此外，使用CGAN校正图像的画质最接近原始图像，而SSIM和PSNR的改进率分别约为26%和7.7%。CGAN模型表现出较高的图像再现性，最显著的模型是学习模型的一致条件和运动伪影出现的方向。</p>
<h1 id="end"><a href="#end" class="headerlink" title="end"></a>end</h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://chierhy.github.io">神经蛙</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://chierhy.github.io/2023/08/29/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AE%BA%E6%96%87%E9%80%9F%E8%AF%BB-2023/">https://chierhy.github.io/2023/08/29/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AE%BA%E6%96%87%E9%80%9F%E8%AF%BB-2023/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://chierhy.github.io" target="_blank">chiblog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0%F0%9F%8E%AB/">笔记🎫</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E2%9C%8F/">论文✏</a></div><div class="post_share"><div class="social-share" data-image="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/08/29/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AE%BA%E6%96%87%E6%92%B0%E5%86%99/"><img class="prev-cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">论文撰写</div></div></a></div><div class="next-post pull-right"><a href="/2023/08/29/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/22-Anaconda/"><img class="next-cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Anaconda</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/08/29/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AE%BA%E6%96%87%E6%92%B0%E5%86%99/" title="论文撰写"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-29</div><div class="title">论文撰写</div></div></a></div><div><a href="/2024/01/29/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/Generative%20AI-LLM/" title="Generative AI-LLM"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-29</div><div class="title">Generative AI-LLM</div></div></a></div><div><a href="/2024/01/25/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/Generative%20AI/" title="Generative AI"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">Generative AI</div></div></a></div><div><a href="/2023/11/09/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AF%BE%E7%A8%8B-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="课程-模式识别与机器学习"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-09</div><div class="title">课程-模式识别与机器学习</div></div></a></div><div><a href="/2023/02/22/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AF%BE%E7%A8%8B-%E7%8E%B0%E4%BB%A3%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/" title="课程-现代控制理论"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-22</div><div class="title">课程-现代控制理论</div></div></a></div><div><a href="/2023/09/12/%E7%94%9F%E6%B4%BB/%E6%88%91%E8%AF%BB=%E8%99%9A%E6%8B%9F%E6%98%BE%E7%A4%BA%EF%BC%9A%E5%BC%95%E9%A2%86%E6%9C%AA%E6%9D%A5%E7%9A%84%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E9%9D%A9%E5%91%BD/" title="我读&#x3D;虚拟显示：引领未来的人机交互革命"><img class="cover" src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-12</div><div class="title">我读&#x3D;虚拟显示：引领未来的人机交互革命</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://pic4.zhimg.com/v2-da217cabde0a382120e68118571d60e3_r.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">神经蛙</div><div class="author-info__description">chierhy@163.com</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chierhy"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-8"><span class="toc-number">1.</span> <span class="toc-text">2023.8</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-10"><span class="toc-number">2.</span> <span class="toc-text">2023.10</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-11"><span class="toc-number">3.</span> <span class="toc-text">2023.11</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-12"><span class="toc-number">4.</span> <span class="toc-text">2023.12</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#authentication"><span class="toc-number">5.</span> <span class="toc-text">authentication</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#biometrics"><span class="toc-number">5.1.</span> <span class="toc-text">biometrics</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%AA%E5%BD%B1"><span class="toc-number">6.</span> <span class="toc-text">伪影</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">6.1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#acc-based"><span class="toc-number">6.2.</span> <span class="toc-text">acc-based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Statistical"><span class="toc-number">6.3.</span> <span class="toc-text">Statistical</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#machine-learning"><span class="toc-number">6.4.</span> <span class="toc-text">machine learning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%83%85%E7%BB%AA"><span class="toc-number">7.</span> <span class="toc-text">情绪</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2023"><span class="toc-number">7.1.</span> <span class="toc-text">2023</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%B4%E6%88%B4%E5%BC%8F-HMD"><span class="toc-number">7.2.</span> <span class="toc-text">头戴式 HMD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Non-invasive"><span class="toc-number">7.3.</span> <span class="toc-text">Non-invasive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#voice"><span class="toc-number">7.4.</span> <span class="toc-text">voice</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#touch"><span class="toc-number">7.5.</span> <span class="toc-text">touch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AA%E5%88%86%E7%B1%BB"><span class="toc-number">7.6.</span> <span class="toc-text">未分类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E4%BD%93%E6%B4%BB%E5%8A%A8%E8%AF%86%E5%88%AB"><span class="toc-number">8.</span> <span class="toc-text">人体活动识别</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#teeth"><span class="toc-number">8.1.</span> <span class="toc-text">teeth</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4"><span class="toc-number">9.</span> <span class="toc-text">指令</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2022"><span class="toc-number">9.1.</span> <span class="toc-text">2022</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%81%A5%E5%BA%B7"><span class="toc-number">10.</span> <span class="toc-text">健康</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#imu"><span class="toc-number">11.</span> <span class="toc-text">imu</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GAN"><span class="toc-number">12.</span> <span class="toc-text">GAN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2020-before"><span class="toc-number">12.1.</span> <span class="toc-text">2020-before</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2022-1"><span class="toc-number">12.2.</span> <span class="toc-text">2022</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2023-1"><span class="toc-number">12.3.</span> <span class="toc-text">2023</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">12.4.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#end"><span class="toc-number">13.</span> <span class="toc-text">end</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/04/12/%E6%8A%80%E6%9C%AF%E5%B7%A5%E5%85%B7/1-%E6%9C%8D%E5%8A%A1%E5%99%A8-GPU/" title="服务器-GPU"><img src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="服务器-GPU"/></a><div class="content"><a class="title" href="/2025/04/12/%E6%8A%80%E6%9C%AF%E5%B7%A5%E5%85%B7/1-%E6%9C%8D%E5%8A%A1%E5%99%A8-GPU/" title="服务器-GPU">服务器-GPU</a><time datetime="2025-04-12T09:49:28.477Z" title="发表于 2025-04-12 17:49:28">2025-04-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/12/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" title="method-知识蒸馏"><img src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="method-知识蒸馏"/></a><div class="content"><a class="title" href="/2025/04/12/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" title="method-知识蒸馏">method-知识蒸馏</a><time datetime="2025-04-12T09:49:26.684Z" title="发表于 2025-04-12 17:49:26">2025-04-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/12/7/" title="无题"><img src="https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2025/04/12/7/" title="无题">无题</a><time datetime="2025-04-12T09:48:44.496Z" title="发表于 2025-04-12 17:48:44">2025-04-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic2.zhimg.com/v2-d29a9ed9425ed9aae1d78cd5e9f3a9f1_r.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025 By 神经蛙</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">See you!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>