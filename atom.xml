<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>chiblog</title>
  
  
  <link href="https://chierhy.github.io/atom.xml" rel="self"/>
  
  <link href="https://chierhy.github.io/"/>
  <updated>2025-04-12T09:17:22.636Z</updated>
  <id>https://chierhy.github.io/</id>
  
  <author>
    <name>神经蛙</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>服务器-GPU</title>
    <link href="https://chierhy.github.io/2025/04/12/%E6%8A%80%E6%9C%AF%E5%B7%A5%E5%85%B7/1-%E6%9C%8D%E5%8A%A1%E5%99%A8-GPU/"/>
    <id>https://chierhy.github.io/2025/04/12/%E6%8A%80%E6%9C%AF%E5%B7%A5%E5%85%B7/1-%E6%9C%8D%E5%8A%A1%E5%99%A8-GPU/</id>
    <published>2025-04-12T09:49:28.477Z</published>
    <updated>2025-04-12T09:17:22.636Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p><a href="https://www.jianshu.com/p/a014016723d8">PyTorch指定GPU的方法 - 简书 (jianshu.com)</a><br><a href="https://blog.csdn.net/qq_33882464/article/details/138118495">RuntimeError: device ＞&#x3D; 0 &amp;&amp; device ＜ num_gpus，但明明device&#x3D;0且num_gpus＞1_device&gt;&#x3D;0-CSDN博客</a><br><a href="https://www.cnblogs.com/dive-into/p/16955702.html">vscode连接远程服务器的指定虚拟环境 - 七三七3 - 博客园 (cnblogs.com)</a></p></blockquote><h2 id="git"><a href="#git" class="headerlink" title="git"></a>git</h2><p>撤回commit<br>    git reset –soft HEAD~1<br>撤回 add<br>    git reset .</p><p>比较不同分支区别<br>    <code>git diff branch1 branch2 --stat</code></p><p>同步main分支的更新到分支：<br>    第一步：切换到本地的仓库，更新为最新的代码。 第二步：切换到要同步的子分支上。 第三步：在子分支上运行 <strong>git merge master</strong> 就会将主分支的代码同步到子分支上。</p><p>删除分支</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">列出本地</span><br><span class="line">git branch</span><br><span class="line"></span><br><span class="line">删除本地</span><br><span class="line">git branch -d  local_branch_name</span><br><span class="line"></span><br><span class="line">删除远程</span><br><span class="line">git push remote_name -d remote_branch_name</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="论文前"><a href="#论文前" class="headerlink" title="论文前"></a>论文前</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">python --version</span><br><span class="line">pip list</span><br></pre></td></tr></table></figure><h2 id="查看内存"><a href="#查看内存" class="headerlink" title="查看内存 "></a>查看内存 </h2><p> <code>-m</code> 选项是以MB为单位来展示内存使用信息; <code>-h</code> 选项则是以人类(human)可读的单位来展示。<br><code>free -m</code><br><code>free -h</code><br><code>du -sh</code> : 查看当前目录总共占的容量，而不单独列出各子项占用的容量；<br><code>du -sh ./*</code> : 单独列出各子项占用的容量。</p><h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><p><a href="https://blog.csdn.net/m0_74890428/article/details/130184164">成功解决：AssertionError: Torch not compiled with CUDA enabled-CSDN博客</a><br><a href="https://blog.csdn.net/weixin_42184315/article/details/134259075">通过命令行快速安装pytorch2.0（GPU）_torch&#x3D;&#x3D;2.0.1+cu118 国内镜像-CSDN博客</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.1.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure><h2 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h2><p>anaconda <a href="https://blog.csdn.net/feng8403000/article/details/127814067">Linux安装Anaconda(Anaconda3-2022.10-Linux-x86_64.sh版本)-CSDN博客</a></p><p>指令 <a href="https://zhuanlan.zhihu.com/p/676491841">24.01.07 | 计算机视觉博士生存指南（三） | 服务器配置与基础Linux指令大全 - 知乎 (zhihu.com)</a></p><p>Cuda <a href="https://blog.csdn.net/qq_45484237/article/details/123514175">实现Linux服务器配置深度学习环境并跑代码完整步骤_能用虚拟机的linux跑深度学习吗-CSDN博客</a><br><a href="https://blog.csdn.net/guotianqing/article/details/123175694">Linux conda中Tensorflow GPU安装配置全面梳理（包含cuda、cudnn）_gv102-CSDN博客</a><br><a href="https://blog.csdn.net/weixin_43412762/article/details/129824339">Tensorflow-gpu保姆级安装教程（Win11, Anaconda3，Python3.9）-CSDN博客</a></p><p>git <a href="https://zhuanlan.zhihu.com/p/629882046">在ubuntu上使用git克隆github上的项目 - 知乎 (zhihu.com)</a></p><p>vs<br><a href="https://zhuanlan.zhihu.com/p/412736012">VSCode使用Remote SSH连接远程服务器 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/MY_WARNING/article/details/122326926">GPU释放显存—–无进程但显存占满解决方法-CSDN博客</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br><span class="line">ps aux|grep python <span class="comment"># 查看进程号</span></span><br><span class="line">kill -<span class="number">9</span> 进程号</span><br></pre></td></tr></table></figure><p><code>ps aux | grep python | grep -v grep | awk &#39;&#123;print $2&#125;&#39; | xargs -I &#123;&#125; kill -9 &#123;&#125;</code><br>解决问题。<br>此外可以通过代码 <code>ps aux | grep python</code> 检查进程</p><hr><p>无法连接 VSCode 使用 Remote-SSH 连接服务器时报错：Resolver error: Error: The VS Code Server failed to start <a href="https://blog.csdn.net/pfl_327/article/details/133323344">VSCode使用Remote-SSH连接服务器时报错：Resolver error: Error: The VS Code Server failed to start_the vs code server fail to start-CSDN博客</a></p><ol><li>重启 linux</li><li><blockquote><p>在 VS Code 中 <code>ctrl+shift+p</code> 后，在弹出框中输入 <code>Remote-SSH:kill VS Code Server on Host</code>，重新打开对应的远端服务器即可。</p></blockquote></li></ol><h2 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h2><p><strong>无需每次密码，自动链接</strong></p><ul><li><p><strong>生成 SSH 密钥对</strong>： 在本地机器上打开终端并运行以下命令，按提示操作（默认保存路径和空密码）。</p><ul><li><code>ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot;</code></li></ul></li><li><p><strong>将公钥复制到远程服务器</strong>： 使用以下命令将公钥复制到远程服务器：<br>  <code>ssh-copy-id your-username@remote-server-ip</code><br>  如果 <code>ssh-copy-id</code> 命令不可用，可以手动将公钥内容 (<code>~/.ssh/id_rsa.pub</code>) 追加到远程服务器的 <code>~/.ssh/authorized_keys</code> 文件中。<br>  是的我不可用，于是</p></li></ul><ol start="2"><li>查看并复制公钥内容</li></ol><ul><li>使用以下命令查看公钥内容，并复制内容到剪贴板：<ul><li><code>cat ~/.ssh/id_rsa.pub</code> 复制显示的整行内容，应该以 <code>ssh-rsa</code> 开头，并包含你的邮箱地址（或你在生成密钥时指定的注释）。</li></ul></li><li>使用 SSH 连接到远程服务器：<code>ssh your-username@remote-server-ip</code></li><li>将复制的公钥内容追加到 <code>~/.ssh/authorized_keys</code> 文件中：<code>echo &quot;your-copied-public-key&quot; &gt;&gt; ~/.ssh/authorized_keys</code></li><li>断开当前 SSH 连接，然后在本地机器上尝试无密码登录到远程服务器：<code>ssh your-username@remote-server-ip</code></li></ul><h1 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h1><p>在Ubuntu系统上安装Miniconda的步骤如下：</p><h3 id="1-下载Miniconda安装脚本"><a href="#1-下载Miniconda安装脚本" class="headerlink" title="1. 下载Miniconda安装脚本"></a>1. 下载Miniconda安装脚本</h3><p>首先，使用 <code>wget</code> 或 <code>curl</code> 命令下载Miniconda的安装脚本。</p><p>打开终端，运行以下命令下载最新版本的Miniconda安装脚本（64位Linux系统）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><h3 id="2-验证安装脚本的完整性（可选）"><a href="#2-验证安装脚本的完整性（可选）" class="headerlink" title="2. 验证安装脚本的完整性（可选）"></a>2. 验证安装脚本的完整性（可选）</h3><p>你可以使用 <code>sha256sum</code> 来验证下载的文件是否损坏。运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sha256sum</span> Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>比较输出的哈希值是否与 <a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda官网</a> 上提供的哈希值一致。</p><h3 id="3-运行安装脚本"><a href="#3-运行安装脚本" class="headerlink" title="3. 运行安装脚本"></a>3. 运行安装脚本</h3><p>使用以下命令开始安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>安装程序会显示一些许可条款，按 <strong>Enter</strong> 键浏览条款，输入 <code>yes</code> 接受许可协议。</p><p>安装程序还会询问安装路径，默认路径是 <code>~/miniconda3</code>，你可以按Enter接受默认路径，或者输入自定义路径。</p><h3 id="4-初始化Miniconda"><a href="#4-初始化Miniconda" class="headerlink" title="4. 初始化Miniconda"></a>4. 初始化Miniconda</h3><p>安装完成后，安装脚本会询问是否运行 <code>conda init</code> 来初始化Conda。选择 <code>yes</code> 以便自动修改你的 shell 配置文件（如 <code>.bashrc</code> 或 <code>.zshrc</code>）。</p><p>如果你跳过了这个步骤，你可以手动运行以下命令来初始化：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda init</span><br></pre></td></tr></table></figure><h3 id="5-激活Conda"><a href="#5-激活Conda" class="headerlink" title="5. 激活Conda"></a>5. 激活Conda</h3><p>如果你执行了 <code>conda init</code>，需要重新加载Shell配置文件或者打开一个新的终端窗口，然后运行以下命令激活Conda环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><p>或者你可以直接运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate</span><br></pre></td></tr></table></figure><h3 id="6-验证安装"><a href="#6-验证安装" class="headerlink" title="6. 验证安装"></a>6. 验证安装</h3><p>检查Conda是否安装成功，运行以下命令查看Conda版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda --version</span><br></pre></td></tr></table></figure><p>你应该能够看到类似于 <code>conda 4.x.x</code> 的输出。</p><h3 id="7-更新Conda（可选）"><a href="#7-更新Conda（可选）" class="headerlink" title="7. 更新Conda（可选）"></a>7. 更新Conda（可选）</h3><p>如果你希望将Conda更新到最新版本，可以运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update conda</span><br></pre></td></tr></table></figure><p>至此，你已经成功在Ubuntu上安装了Miniconda！</p><h1 id="复制环境"><a href="#复制环境" class="headerlink" title="复制环境"></a>复制环境</h1><hr><p> 根据已有环境名复制生成新的环境</p><p>假设已有环境名为A，需要生成的环境名为B：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n B --clone A</span><br></pre></td></tr></table></figure><p> 根据已有环境路径复制生成新的环境</p><p>假设已有环境路径为D:\A，需要生成的新的环境名为B：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n B --clone D:\A</span><br></pre></td></tr></table></figure><h1 id="Tmux"><a href="#Tmux" class="headerlink" title="Tmux"></a>Tmux</h1><hr><p><a href="https://www.ruanyifeng.com/blog/2019/10/tmux.html">Tmux 使用教程 - 阮一峰的网络日志 (ruanyifeng.com)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>method-知识蒸馏</title>
    <link href="https://chierhy.github.io/2025/04/12/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"/>
    <id>https://chierhy.github.io/2025/04/12/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/</id>
    <published>2025-04-12T09:49:26.684Z</published>
    <updated>2024-07-28T05:36:56.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>知识蒸馏（Knowledge Distillation）是一种在计算机视觉领域应用广泛的模型压缩技术，它旨在将一个大型、训练好的复杂模型（称为教师模型）的知识转移至一个更小、更高效的模型（称为学生模型）。通过这种方式，学生模型能够在保持相对较高准确率的同时，减少计算资源的消耗和提高运行效率。这项技术对于在移动设备和边缘计算设备上运行大型深度学习模型尤为重要，广泛应用于图像分类、目标检测和面部识别等计算机视觉任务中。 </p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://chierhy.github.io/2025/04/12/7/"/>
    <id>https://chierhy.github.io/2025/04/12/7/</id>
    <published>2025-04-12T09:48:44.496Z</published>
    <updated>2024-10-04T01:22:05.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>课程-随机过程</title>
    <link href="https://chierhy.github.io/2025/04/12/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AF%BE%E7%A8%8B-%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/"/>
    <id>https://chierhy.github.io/2025/04/12/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E8%AF%BE%E7%A8%8B-%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/</id>
    <published>2025-04-12T09:48:44.475Z</published>
    <updated>2024-07-28T05:36:56.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><ol><li>方差、协方差，一维二维分布函数、宽平稳、独立增量（平稳独立增量，时间变长也增加）<ol><li><ul><li>实际上是采用平稳的这个定义想办法将随机过程中的时间这个参量进行了弱化，对于严平稳来说一维特性（一维概率密度、均值、方差）都和时间没有关系，二维特向仅与时间差有关（二维概率密度、相关函数、协方差函数）。对于广义平稳来说，均值与时间无关，相关函数仅与时间差有关。</li></ul></li></ol></li><li>泊松过程<ol><li>强度参数</li><li>可加性、负值、</li><li>（计数过程）</li></ol></li><li>马尔科夫链<ol><li>极限分布、转移概率、平均时间、哪些状态是⾮常返的？哪些状态是常返的？求常返状态的平均返回时间；求各状态的周期并将状态空间分解</li><li>ppt<ol><li>离散时间、连续时间</li><li>马氏性、条件独立性</li><li>本章只讨论时齐（齐次）马氏链，并且简称为马氏链</li><li>P_ij 代表 i 转移到 j 的概率</li><li>时齐马氏性</li><li>N 步转移概率</li><li>状态的分类</li><li>非常返和零常返，平稳分布，极限分布</li></ol></li></ol></li><li>鞅<ol><li>鞅停时定理</li></ol></li><li>布朗运动</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>对比学习-2019-MoCo</title>
    <link href="https://chierhy.github.io/2024/08/30/%E8%AE%BA%E6%96%87/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-2019-MoCo/"/>
    <id>https://chierhy.github.io/2024/08/30/%E8%AE%BA%E6%96%87/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-2019-MoCo/</id>
    <published>2024-08-30T13:35:00.000Z</published>
    <updated>2024-08-30T14:45:24.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p><a href="https://blog.csdn.net/qq_56591814/article/details/127564330">李沐论文精读系列三：MoCo、对比学习综述（MoCov1&#x2F;v2&#x2F;v3、SimCLR v1&#x2F;v2、DINO等）_moco论文-CSDN博客</a></p></blockquote><ul><li><code>MoCo</code>这个词，来自于论文标题的前两个单词<code>动量对比Momentum Contrast</code> 这个式子表示：当前时刻的输出，不仅依赖于当前时刻的输入，还依赖于前一时刻的输出。m越大，当前时刻的输入 对结果​ 就越小。</li><li>MoCo利用了动量的这种特性，从而缓慢地更新一个编码器，让中间学习的字典中的特征尽可能地保持一致。</li><li>本文是从另外一个角度来看对比学习，即把对比学习看作是一个字典查询任务。</li></ul><p><img src="https://i-blog.csdnimg.cn/blog_migrate/2d84e9fcf36e3d852be13792afa01eda.png"></p><p>在MoCo这篇论文当中，因为作者已经把所有的对比学习的方法归纳成为了一个动态字典的问题，所以很少使用anchor或者正负样本这些词，用的都是query和key。<br>作者认为，一个好的字典应该有两个特性：<br>字典足够大<br>    字典越大，key越多，所能表示的视觉信息、视觉特征就越丰富 ，这样拿query去做对比学习的时候，才越能学到图片的特征。<br>    反之，如果字典很小，模型很容易通过学习一些捷径来区分正负样本，这样在碰到大量的真实数据时，泛化就会特别差（我的理解是，字典中只有猫和狗，狗都是黑色，猫都是黄色。模型简单的判断图片中物体是否是黄色，来区分猫和狗，而不是真的学到了猫和狗的特征）<br>编码的特征尽量保持一致性<br>    字典里的key都应该用相同或者说相似的编码器去编码得到，否则模型在查找query时，可以简单的通过找到和它使用相同或者相似编码器的key，而不是真的和它含有相同语义信息的key（变相引入两一个捷径）。<br>以前的对比学习，都至少被上述所说的两个方面中的一个所限制（要么一致性不好，要么字典不够大）。本文最大的贡献，就是使用队列以及动量编码器来进行对比学习，解决了这个问题。具体来说：</p><ul><li>key（编码特征）并不需要梯度更新，而是通过更新编码器，新的编码器使输出的key更新。</li><li>queue ：整个队列里面的元素都是字典，队首输入当前batch的编码特征，队尾弹出最旧的batch特征。每次移除的是最老的那些key，从一致性的角度来说 ，有利于对比学习。<ul><li>用队列的好处是可以重复使用那些已经编码好的key，而这些key是从之前的那些mini-batch中得到的。</li><li>用队列结构，就可以把的mini_batch的大小和队列的大小直接分开了，所以最后这个队列的大小，也就是字典的大小可以设的非常大，因为它大部分的元素都不是每个iteration都需要更新的。</li><li>在字典里计算loss而不是整个数据集上计算loss，使用队列的数据结构，可以让维护这个字典的计算开销非常小。</li></ul></li><li>momentum encoder：<ul><li>如果只有当前batch的key是从当前的编码器得到特征，其它的key都是另外时刻的编码器输出的特征，这样就无法保证字典中key的一致性。所以作者又提出了动量编码器</li><li>动量编码器，即编码器参数的更新方式就是。</li><li>初始化的编码器来自于query的编码器，之后每次更新，只有1‰的参数会从query的编码器参数里拿过来更新，所以这个编码器参数更新的非常缓慢。从而保证了字典中所有的key都是由相似的编码器抽取得到的，尽最大可能地保持了他们的一致性。（直接更新编码器k的所有参数，会导致编码器更新过快，降低了这个队列中所有key的特征的一致性）</li></ul></li><li>动态字典：字典中的key都是随机取样的，而且key的编码器在训练的过程中也是在不停的改变。</li></ul><p><img src="https://i-blog.csdnimg.cn/blog_migrate/808ceafceb5d2b2d42a64c0c87d39214.png"></p><p>  <code>MoCo</code> 的主要贡献就是把之前对比学习的一些方法都归纳总结成了一个字典查询的问题，并提出了队列存储和动量编码器。前者解决字典太大不好存储和训练的问题，后者解决了字典特征 不一致的问题；从而形成一个又大又一致的字典，能帮助模型更好的进行对比学习。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="论文" scheme="https://chierhy.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
  </entry>
  
  <entry>
    <title>对比学习-时序-合集</title>
    <link href="https://chierhy.github.io/2024/07/30/%E8%AE%BA%E6%96%87/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-%E6%97%B6%E5%BA%8F-%E5%90%88%E9%9B%86/"/>
    <id>https://chierhy.github.io/2024/07/30/%E8%AE%BA%E6%96%87/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-%E6%97%B6%E5%BA%8F-%E5%90%88%E9%9B%86/</id>
    <published>2024-07-30T05:21:00.000Z</published>
    <updated>2024-07-30T05:36:29.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><ul><li><a href="https://www.cnblogs.com/ZERO-/p/18266304">SoftCLT: 时间序列的软对比学习《Soft Contrastive Learning for Time Series》(时间序列、时序分类任务、软对比学习、实例上软赋值距离差异、数据空间非嵌入空间度量相似性) - ZERO- - 博客园 (cnblogs.com)</a></li></ul></blockquote><p>时间序列中的对比学习。在时间序列分析领域，考虑到时间序列的不变特性，已经提出了几种正对和负对的对比学习设计。表 1 比较了包括我们在内的各种 TS 对比学习方法的若干特性。T-Loss（Franceschi 等人，2019 年）从一个 TS 中随机抽样一个子序列，如果属于该 TS 的子序列，则将其视为正序列；如果属于其他 TS 的子序列，则将其视为负序列。Self-Time（Fan 等人，2020 年）通过将同一 TS 的增强样本定义为正样本和负样本来捕捉 TS 之间的样本间关系，并通过解决分类任务来捕捉 TS 内部的时间关系，其中类标签使用子序列之间的时间距离来定义。TNC（Tonekaboni 等人，2021 年）使用正态分布定义窗口的时间邻域，并将邻域内的样本视为正样本。TS-SD（Shi 等人，2021 年）使用三元组相似性判别任务训练模型，目标是识别两个 TS 中哪个与给定的 TS 更相似，使用 DTW 来定义相似性。TS-TCC（Eldele 等人，2021 年）提出了一种时间对比损失，即让增强预测彼此的未来，而 CA-TCC（Eldele 等人，2023 年）是 TS-TCC 在半监督设置下的扩展，采用了相同的损失。TS2Vec（Yue 等人，2022 年）将 TS 分成两个子序列，并在实例和时间维度上定义了分层对比损失。Mixing-up（Wickstrøm 等人，2022 年）通过混合两个 TS 生成新的 TS，目标是预测混合权重。CoST（Woo 等人，2022 年）利用时域和频域对比损失来学习季节趋势表征。TimeCLR（Yang 等人，2022 年）引入了相移和振幅变化增强，这是一种基于 DTW 的数据增强方法。TF-C（Zhang 等人，2022 年）同时学习基于时间和频率的 TS 表示，并提出了一种新颖的时频一致性架构。在医疗领域，Subject-Aware CL（Cheng 等人，2020 年）提出了一种实例化的 CL 框架，通过架构设计将时间信息纠缠在一起；CLOCS（Kiyasseh 等人，2021 年）提出考虑其应用中特别可用的空间维度，这与一般 TS 中的信道接近。以前用于 TS 的 CL 方法计算的是硬对比损失，即所有负对之间的相似性同样最小化，而我们为 TS 引入了软对比损失。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>对比学习-2024-ICLR-SoftCLT</title>
    <link href="https://chierhy.github.io/2024/07/30/%E8%AE%BA%E6%96%87/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-%E6%97%B6%E5%BA%8F-2024-ICLR-SoftCLT/"/>
    <id>https://chierhy.github.io/2024/07/30/%E8%AE%BA%E6%96%87/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-%E6%97%B6%E5%BA%8F-2024-ICLR-SoftCLT/</id>
    <published>2024-07-30T05:10:00.000Z</published>
    <updated>2024-07-30T05:19:15.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>对比相似的时间序列实例或时间序列中相邻时间戳的值会忽略其内在的相关性，从而导致学习到的表征质量下降。</strong></p><ul><li>标准的对比学习CL目标可能对TS表示学习有害，因为在CL中忽略了时间序列TS中类似TS实例和时间戳附近值的固有相关性，而这种相关性可能是一种强大的自我监督。例如，动态时间扭曲（DTW）等距离指标已被广泛用于测量 TS 数据的相似性，而对比 TS 数据可能会丢失此类信息。此外，在自然 TS 数据中，时间戳相近的值通常是相似的，因此像以前的 CL 方法（Eldele 等人，2021 年；Yue 等人，2022 年）那样<strong>对所有时间戳不同的值进行同等程度的惩罚对比可能不是最优的</strong>。</li><li>如何考虑时间序列数据的相似性，以更好地进行对比表示学习？为此，我们提出了时间序列软对比学习（SoftCLT）。具体来说，我们建议<strong>不仅考虑正对的 InfoNCE 损失（Oord 等人，2018 年），还要考虑所有其他对的 InfoNCE 损失，并在实例性 CL 和时间性 CL 中计算其加权求和，其中实例性 CL 对比的是 TS 实例的表征，而时间性 CL 对比的是单个 TS 中时间戳的表征，</strong>如图 1 所示。我们建议在实例性 CL 中根据 TS 之间的距离进行软分配，在时间性 CL 中根据时间戳的差异进行软分配。如果我们将软分配替换为硬分配（负为 0 或正为 1），那么提出的损失就变成了对比损失。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="论文" scheme="https://chierhy.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
  </entry>
  
  <entry>
    <title>对比学习-2020-SIMCLR</title>
    <link href="https://chierhy.github.io/2024/07/30/%E8%AE%BA%E6%96%87/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-2020-SIMCLR/"/>
    <id>https://chierhy.github.io/2024/07/30/%E8%AE%BA%E6%96%87/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0-2020-SIMCLR/</id>
    <published>2024-07-30T04:52:00.000Z</published>
    <updated>2024-07-30T05:02:49.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p><a href="https://www.cnblogs.com/ZERO-/p/18199618">SimCLR: 一种视觉表征对比学习的简单框架《A Simple Framework for Contrastive Learning of Visual Representations》(对比学习、数据增强算子组合，二次增强、投影头、实验细节很nice)，好文章，值得反复看 - ZERO- - 博客园 (cnblogs.com)</a> </p><ul><li><a href="https://www.cnblogs.com/BlairGrowing/p/14852361.html">论文解读（SimCLR）《A Simple Framework for Contrastive Learning of Visual Representations》 - 多发Paper哈 - 博客园 (cnblogs.com)</a></li></ul></blockquote><p>𝑆𝑖𝑚𝐶𝐿𝑅 最终目的是<strong>最大化同一数据示例的不同增强视图之间的一致性来学习表示，即 max similar (v 1, v 2)</strong></p><ul><li><strong>核心思想</strong>：SimCLR 通过对比学习自监督地学习视觉表示。最大化同一数据样本不同增强视图在潜在空间中的相似性，最小化不同样本的相似性。</li><li><strong>关键组件</strong>：数据增强（如随机裁剪、颜色失真、高斯模糊）、神经网络编码器（如 ResNet）、投影头（两层 MLP）、对比损失（NT-Xent Loss）。</li><li><strong>应用</strong>：用于无标签数据预训练，适用于图像分类、对象检测等任务。</li><li><strong>优势</strong>：无需标签数据即可学习有效表示，提高下游任务的性能。</li><li></li><li><strong>多种数据增强操作的组合对于确定产生有效表征的对比预测任务至关重要。此外，与有监督学习相比，无监督对比学习受益于更强的数据增强。</strong><ul><li>——三种简单的增强技术：随机裁剪并调整为原始大小、随机色彩失真和随机高斯模糊。我们的研究表明，对监督学习没有产生准确性益处的数据增强仍然对对比学习有很大帮助。</li></ul></li><li><strong>在表征和对比损失之间引入可学习的非线性变换，可大幅提高所学表征的质量。</strong><ul><li>们观察到，非线性投影比线性投影好（+3%），比无投影好得多（&gt;10%）。使用投影头时，无论输出维度如何，都能观察到类似的结果。此外，即使使用了非线性投影，投影头之前的层 h 仍然比投影头之后的层 z &#x3D; g(h) 好得多（&gt;10%），<strong>这表明投影头之前的隐藏层比投影头之后的隐藏层具有更好的代表性。</strong>(GPT解释：因为投影头的设计目标是为了辅助训练特征提取层，而不是直接用于下游任务。投影头的设计主要目的是为了在训练过程中增强特征学习的效果。在对比学习(如SimCLR)中，投影头通常用于将高维特征映射到一个低维空间，使得对比损失在这个空间中计算得更为有效。)投影头之后的特征(通常记作z&#x3D;g(h)是专门为优化对比损失函数设计的。)（me：也就是说，如果用作下游任务分类，投影会造成信息损失，使用投影前的信息会更具有代表性，更好分类）</li></ul></li><li><strong>使用对比交叉熵损失进行表征学习，可以从归一化嵌入和适当调整的温度参数中获益。</strong><ul><li>1）<strong>l2 归一化（即余弦相似度）和温度能有效地对不同实例进行加权，而适当的温度能帮助模型从困难负样本(难负样本)中学习</strong>；2）与交叉熵不同，其他目标函数并不能根据负样本的相对难度对其进行加权。因此，我们必须对这些损失函数进行半难负样本挖掘</li></ul></li><li><strong>与监督学习相比，对比学习受益于更大的批量和更长的训练时间。与监督学习一样，对比学习受益于更深更广的网络。</strong><ul><li><strong>随着训练steps&#x2F;epochs的增加，不同批次(batch size)大小之间的差距会缩小或消失</strong>，前提是这些批次(batches)是随机重采样的。与监督学习（Goyal 等人，2017 年）不同的是，在对比学习中，较大的batch size能提供更多的负样本，从而促进收敛（即用较少的epochs和steps就能获得给定的准确率）。更长时间的训练也能提供更多负样本，从而改善结果。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="论文" scheme="https://chierhy.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
  </entry>
  
  <entry>
    <title>method-CNN</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-CNN/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-CNN/</id>
    <published>2024-07-28T06:34:00.000Z</published>
    <updated>2024-07-28T06:35:30.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p><a href="https://mp.weixin.qq.com/s/OSFbCdv_R54AeiS01t34DA">必会的六大卷积神经网络 (qq.com)</a></p></blockquote><h1 id="必会的六大卷积神经网络"><a href="#必会的六大卷积神经网络" class="headerlink" title="必会的六大卷积神经网络"></a>必会的六大卷积神经网络</h1><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>LeNet-5 由 Yann LeCun 设计，是最早的卷积神经网络 (CNN) 之一，主要用于手写数字识别。</p><p>它有 5 层，具有可学习的参数，并以灰度图像作为输入。</p><p>以下是其架构的详细分解：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnmJp2CbP3ibN4Q4xZZxe6licURQoHNcyUAcK6WwPtRkrNLUGkiafnOCwYQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><ol><li><p><strong>输入层</strong></p><p> 输入：32x32 灰度图像。</p></li><li><p><strong>3组卷积+平均池化层</strong></p><p> 卷积层通过应用滤波器从输入图像中提取特征，而平均池化层则减少空间维度，使得网络对输入图像的小平移保持不变。</p></li></ol><ul><li><p>第 1 层，6 个大小为 5x5 的滤波器，产生 28x28x6 的特征图。</p></li><li><p>第 2 层，使用平均池化（2x2 窗口），减少到 14x14x6。</p></li><li><p>第 3 层，16 个大小为 5x5 的滤波器，产生 10x10x16 的特征图。</p></li><li><p>第 4 层，使用平均池化（2x2 窗口），减少到 5x5x16。</p></li><li><p>第 5 层，120 个大小为 5x5 的滤波器，产生 1x1x120 的特征图。</p></li></ul><ol start="4"><li><p><strong>2个完全连接层</strong></p><p> 全连接层根据从卷积层提取的特征进行分类。</p></li></ol><ul><li><p>第 6 层，84 个神经元，与前一层完全连接。</p></li><li><p>输出层，10 个神经元，每个数字类别一个，使用 Softmax 进行分类。</p></li></ul><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>AlexNet 由 Alex Krizhevsky 开发，在 2012 年 ImageNet 竞赛中获胜，彻底改变了计算机视觉。</p><p>它有 8 层，有 6230 万个可学习参数。</p><p>以下是逐层详细分解：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnOthdfvAoUO9ASI2HIyusCH58icwvAzib2NmQeNzlsEpiaicWGvYz10gaeQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><ol><li><p><strong>输入层</strong></p><p> 输入，224x224 RGB 图像。</p></li><li><p><strong>5 组卷积 + 最大池化层</strong></p><p> 这些层执行特征提取，使用 ReLU 激活加速训练速度。最大池化层减少空间维度，同时保留重要特征。使用填充来防止特征图的大小急剧减小</p></li></ol><ul><li><p>第 1 层，96 个大小为 11x11、步幅为 4 的过滤器，产生 55x55x96 的特征图，然后进行 ReLU 激活和最大池化。</p></li><li><p>第 2 层，256 个大小为 5x5 的过滤器，产生 27x27x256 的特征图，然后进行 ReLU 激活和最大池化。</p></li><li><p>第 3 至第 5 层，384 个大小为 3x3 的过滤器，为第 3 层和第 4 层生成 13x13x384 的特征图，为第 5 层生成 13x13x256 的特征图，每个特征图后跟 ReLU 激活。第 5 层后跟最大池化。</p></li></ul><ol start="4"><li><p><strong>完全连接层</strong></p><p> 全连接层对卷积层提取的特征进行分类。</p></li></ol><ul><li><p>第 6-7 层，每层 4096 个神经元，随后是 ReLU 激活和 dropout。Dropout 通过在训练期间随机将一部分输入单元设置为零来帮助防止过度拟合。</p></li><li><p>输出层 8：最后一层使用 Softmax</p></li></ul><h3 id="VGG16"><a href="#VGG16" class="headerlink" title="VGG16"></a>VGG16</h3><p>VGG16，由牛津大学的视觉几何小组提出。它于 2014 年提出。它有 16 层，有 1.38 亿个可学习参数，以其简单和统一的架构而闻名。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBEln0OWXyXX2iaNcjCqDIdgcAJ62U4kwtyhEfvicdl2Xia66TSfYg1fDYbXhg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><ol><li><p><strong>输入层</strong></p><p> 输入，224x224 RGB 的图像。</p></li><li><p><strong>卷积+最大池化层</strong></p></li></ol><ul><li><p>块 1：2 个卷积层，带有 64 个大小为 3x3 的过滤器，然后是 ReLU 激活和最大池化。</p></li><li><p>块 2：2 个卷积层，带有 128 个大小为 3x3 的过滤器，然后是 ReLU 激活和最大池化。</p></li><li><p>块 3：3 个卷积层，带有 256 个大小为 3x3 的过滤器，然后是 ReLU 激活和最大池化。</p></li><li><p>块 4：3 个卷积层，带有 512 个大小为 3x3 的过滤器，然后是 ReLU 激活和最大池化。</p></li><li><p>块 5：3 个卷积层，带有 512 个大小为 3x3 的过滤器，然后是 ReLU 激活和最大池化。</p></li></ul><p>过滤器的数量不断增加，直到块 4 和块 5 中达到 512。所有卷积层的过滤器大小固定为 3 X 3，步长 &#x3D;1，填充 &#x3D;1</p><ol start="3"><li><strong>完全连接层</strong></li></ol><ul><li><p>第 14-15 层，每层 4096 个神经元，随后是 ReLU 激活和 dropout。</p></li><li><p>输出层 16，1000 个神经元（每个 ImageNet 类一个），使用 Softmax 进行分类。</p></li></ul><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p>谷歌于 2014 年推出的 Inception V1 旨在<strong>通过增加网络的深度和宽度来提高性能</strong>，同时缓解过度拟合和参数数量的增加。</p><p>它有 22 层，有 680 万个可训练参数</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnoygcf7PzGzfB35bZYTfentpz8M1IbbETYjL9gkU8PVPgCLEsicyYYpA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><ol><li><p><strong>输入层</strong></p><p> 输入，224x224 RGB 图像。</p></li><li><p><strong>2 组初始卷****积+最大池化层</strong></p></li></ol><ul><li><p>第 1 层，64 个大小为 7x7 的过滤器，产生 112x112x64 的特征图，然后进行最大池化。</p></li><li><p>第 2 层，192 个大小为 3x3 的过滤器，产生 56x56x192 的特征图，然后进行最大池化。</p></li></ul><ol start="4"><li><p><strong>9个Inception模块</strong></p><p> Inception 模块<strong>结合</strong>了多种尺寸（1x1、3x3、5x5）的过滤器，以捕获输入的不同方面。包括用于降维的 1x1 卷积。</p><p> Inception 模块允许网络以多种尺度捕获特征，并通过降维提高计算效率。</p><p> <strong>Inception 模块的优势</strong></p></li></ol><ul><li><p>无需担心确定过滤器的大小，因为 Inception 模型包含各种尺寸</p></li><li><p>使用降维模块减少操作次数</p></li><li><p>可以构建更深的网络，最终可以提高我们模型的性能</p></li><li><p>降维模块</p><p>  为了解决深度增加时过拟合概率的问题，在 Inception 模块中包含了使用 1x1 卷积的降维模块。</p><p>  <strong>它通过减少通道数量来大幅减少操作次数。</strong></p><p>  下图显示了使用&#x2F;不使用 1x1 卷积时操作次数的比较。</p></li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnqvfDjHc9ByzOKN4V6VEGFUG4S33g2UBW91Is2Tjzh4auelfR7JicQNg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><ul><li>使用不同形状的过滤器来提取特征<br>  <img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnVaKthDlCG4caDrVYQ9gVW8icwRwozXND0styPjYG1yib6bjp6pHaaOTQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></li></ul><ol start="9"><li><p><strong>2个辅助分类器</strong></p><p> 辅助分类器通过提供中间监督来帮助主网络更好地学习，从而降低梯度消失的风险。</p><p> 辅助分类器由以下内容组成：</p></li></ol><ul><li><p>平均池化，过滤器大小 5 X 5，步幅为 3</p></li><li><p>具有 128 个滤波器的 1 X 1 卷积</p></li><li><p>具有 1024 个神经元 + ReLU 的全连接层</p></li><li><p>Dropout 层，dropout 率为70%</p></li><li><p>具有 1000 个神经元 + Softmax 的全连接层</p></li></ul><ol start="11"><li><strong>2个完全连接层</strong></li></ol><ul><li><p>全连接层</p></li><li><p>输出层，1000 个神经元（每个 ImageNet 类一个），使用 Softmax 进行分类。</p></li></ul><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>ResNet 由微软研究院于 2015 年推出，解决了梯度消失&#x2F;爆炸问题和退化问题。</p><p>在退化问题中，训练误差和测试误差都会随着层数的增加而增加。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnEsdfHMn61Dxzib4B75uuvxCERZJAib3sOOOpwxtH8TauAppHGxdsWZnA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>这些问题通过残差块得到解决。</p><p>ResNet34 中有 16 个残差块。ResNet34 有 34 个可学习参数的层。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnDz86nDza9NjlYcLDP4cW8sU4Hq4Zo7H3wUP9gC5FXSwqp59p7ibkz2A/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>其架构包括：</p><ol><li><p><strong>输入层</strong></p><p> 输入，224x224 RGB 图像。</p></li><li><p><strong>初始卷积+最大池化层</strong></p><p> 第 1 层，64 个大小为 7x7 的过滤器，产生 112x112x64 的特征图，然后进行最大池化。</p></li><li><p><strong>残差块</strong></p><p> 16 个残差块，其中大部分为 3x3 过滤器。</p><p> 残差块允许网络学习残差函数，通过防止退化问题，使得训练非常深的网络变得更容易。</p><p> 恒等映射使得网络可以跳过不添加新信息的层，从而确保高效的梯度流。</p><p> <img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnk1Yym7kNEAg4EL8m1KRYGSKhPyIqKGianyqLlxeUXvvmOOEgTu1Vedw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p></li></ol><p>    <strong>残差块的优点</strong></p><ul><li><p>可以将初始信息传递到更深层</p></li><li><p>F(x) + x 是元素逐一相加，因此没有额外的参数，也没有额外的计算复杂度</p></li><li><p>更容易优化残差映射</p></li></ul><p><strong>4. 批量标准化和ReLU</strong></p><p>每次卷积后都会应用批量标准化来稳定和加速训练，然后进行 ReLU 激活。</p><p><strong>5. 自适应平均池+全连接层</strong></p><ul><li><p>自适应平均池，将特征图减少为每个特征图一个值。</p></li><li><p>输出层，1000 个神经元（每个 ImageNet 类一个），使用 Softmax 进行分类。</p></li></ul><h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p>DenseNet 于 2018 年提出，以前馈方式将每一层连接到其他每一层。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnLVMPgMaGGwkIbVXZ2fw4wdt6J7JjVX3nDkbAT9OAP4tswn1zTiaibo5w/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>密集块</p><p>DenseNet 从 conv 层开始，然后是 Desne Block</p><p>密集块内的每个单独块都是一个复合函数，由 3 个操作组成，即批量标准化、ReLU 和最后的转换操作。</p><p>在 3x3 转换之前应用 1x1 转换，并且在每个转换之前使用批量标准化和 ReLU，如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnayCIBMsrfXL0ogfzecwrIHjhicCXnfZibFPRGO9NibohDC8JdNZibDwjiag/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>每一层都接收来自所有前几层的输入，并连接它们的特征图。</p><p>密集块的优点</p><ul><li><p>解决梯度消失的问题：因为梯度可以直接从第一个块流到最后一个块</p></li><li><p>鼓励特征重用：由于特征连接，他们发现较早层提取的特征可直接由同一密集块中的较深层使用</p></li></ul><h4 id="过渡层"><a href="#过渡层" class="headerlink" title="过渡层"></a>过渡层</h4><p>使用 1x1 卷积和池化将特征图的大小减少一半，从而降低计算复杂度。</p><p>Transition 层的架构如下所示。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/ibPqnScajrbEALoyDn0BwQCooElibEBElnia1hmqK2r7EFdQnCn50Gnj6qxoZQXeiblAuDSoGibTeiagA9I80rlCADrQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>与往常一样，在 1x1 转换层之前应用批量标准化和 ReLU 激活，然后应用 Avage Pooling。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>大模型-合集</title>
    <link href="https://chierhy.github.io/2024/07/28/%E8%AE%BA%E6%96%87/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%90%88%E9%9B%86/"/>
    <id>https://chierhy.github.io/2024/07/28/%E8%AE%BA%E6%96%87/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%90%88%E9%9B%86/</id>
    <published>2024-07-28T06:31:00.000Z</published>
    <updated>2024-07-28T06:33:05.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p><a href="https://mp.weixin.qq.com/s/AVfdwCwKheF6auYxDcfd9g">谷歌｜清华｜CMU近期值得关注的3个时序大模型研究 (qq.com)</a></p></blockquote><p>TimesFM<br><strong>论文标题</strong>：A DECODER - ONLY FOUNDATION MODEL FOR TIME - SERIES FORECASTING<br>受到自然语言处理（NLP）领域中大型语言模型最新进展的启发，Google研究人员设计了一个时间序列基础模型<strong>TimesFM</strong>，用于预测。该模型在多种公共数据集上的即开即用零样本性能，接近于每个单独数据集上最先进的监督预测模型的准确性。模型基于预训练一个带有输入patch的解码器风格注意力模型，使用一个包含真实世界和合成数据集的大型时间序列语料库。在一系列之前未见过的预测数据集上的实验表明，该模型能够在不同的领域、预测范围和时间粒度上产生准确的零样本预测。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/9RtQ04O5eOrhPrejM7uDu1nxXYSKq76brvayouXNmHQsY2r39JRX1X6bFyfLpibuU0bzLOfCGykgFcfKJM71J9w/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><hr><p><strong>论文标题</strong>：Timer: Generative Pre-trained Transformers Are Large Time Series Models (ICML24)<br>随着大型语言模型的出现，不断取得进展，展现出前所未有的能力，如少样本泛化、可扩展性和任务通用性，这些在小型深度模型中是缺失的。为了改变从零开始训练特定于场景的小型模型的现状，本文旨在早期开发大型时间序列模型（large time series models LTSM）。在预训练期间，作者策划了包含多达10亿时间点的大规模数据集，将异构时间序列统一为单序列序列（S3）格式，并发展了面向LTSM的GPT风格架构。为了满足多样化的应用需求，作者将时间序列的预测、插值和异常检测转化为统一的生成任务。本研究的成果是一种时间序列变换器（Timer），它通过下一个标记预测进行生成预训练，并适应于各种下游任务，展现出作为LTSM的有希望的能力。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/9RtQ04O5eOrhPrejM7uDu1nxXYSKq76bGIqc72lNNyQaUPhkZQpNXBPYU7LXTos32OOAmFUMnsZV9XGh9ORsWw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><hr><p><strong>论文标题：</strong>MOMENT: A Family of Open Time-series Foundation Models (ICML24)</p><p>本文推出了<strong>MOMENT</strong>，这是一系列用于通用时间序列分析的开源基础模型。在时间序列数据上预训练大型模型面临挑战，原因包括：(1) 缺乏一个庞大且统一的公共时间序列资料库；(2) 时间序列特性的多样性使得多数据集训练变得复杂；(3) 用于评估这些模型的实验基准，尤其是在资源、时间和监督有限的情况下，仍处于起步阶段。为了应对这些挑战，本文编纂了一个庞大且多样化的公共时间序列集合，称为“<strong>时间序列堆</strong>”，并系统地解决时间序列特有的挑战，以实现大规模多数据集预训练。最后，作者基于最近的工作设计了一个基准，用于在有限监督的设置中评估时间序列基础模型在多样化任务和数据集上的表现。在这个基准上的实验表明，本文的预训练模型在最少的数据和任务特定微调下的有效性。最后，作者提出了一些关于大型预训练时间序列模型的有趣实证观察。预训练模型（AutonLab&#x2F;MOMENT-1-large）和时间序列堆（AutonLab&#x2F;Timeseries-PILE）可在<a href="https://huggingface.co/AutonLab%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/AutonLab上获取。</a></p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/9RtQ04O5eOrhPrejM7uDu1nxXYSKq76bw1ZDESAv3zaJKpjZS9SB7LzFoCgUqf6ndbkRkHUaN0xOqibxpf6ia0og/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="论文" scheme="https://chierhy.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
  </entry>
  
  <entry>
    <title>task-data explainablity</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/task-data%20explainablity/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/task-data%20explainablity/</id>
    <published>2024-07-28T06:19:00.000Z</published>
    <updated>2024-07-28T06:19:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p><a href="https://zhuanlan.zhihu.com/p/647147836">模型解释——特征重要性、LIME与SHAP - 知乎 (zhihu.com)</a><br><a href="https://zhuanlan.zhihu.com/p/435528676">机器学习6种模型可解释性方法汇总，你最常用哪一种？ - 知乎 (zhihu.com)</a><br><a href="https://blog.csdn.net/xieyan0811/article/details/107594196">分类模型的可解释性_why should i trust you论文介绍-CSDN博客</a><br><a href="https://github.com/oneTaken/awesome_deep_learning_interpretability">oneTaken&#x2F;awesome_deep_learning_interpretability: 深度学习近年来关于神经网络模型解释性的相关高引用&#x2F;顶会论文(附带代码) (github.com)</a></p></blockquote><p>基于规则的模型（Rule-based Model），例如决策树，得益于其透明的内部结构和良好的模型表达能力，仍在医疗、金融和政治等对模型可解释性要求较高的领域发挥着重要作用。然而，传统的基于规则的模型由于其离散的参数和结构而难以优化，尤其在大规模数据集上，这严重限制了规则模型的应用范围。而集成模型、软规则和模糊规则等，虽然提升了分类预测能力，但牺牲了模型可解释性。</p><h1 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h1><ul><li><p><a href="https://www.thepaper.cn/newsDetail_forward_15131799">鱼和熊掌不可兼得？清华团队提出高准确率的可解释分类模型_澎湃号·湃客_澎湃新闻-The Paper</a>论文链接：<a href="https://arxiv.org/abs/2109.15103%E4%BB%A3%E7%A0%81%E9%93%BE%E6%8E%A5%EF%BC%9Ahttps://github.com/12wang3/rrl">https://arxiv.org/abs/2109.15103代码链接：https://github.com/12wang3/rrl</a></p></li><li></li><li><p>Partial Dependence Plot (PDP)；<br>  PDP是十几年之前发明的，它可以显示一个或两个特征对机器学习模型的预测结果的边际效应。它可以帮助研究人员确定当大量特征调整时，模型预测会发生什么样的变化。</p></li><li><p>Individual Conditional Expectation (ICE)<br>  ICE和PDP非常相似，但和PDP不同之处在于，PDP绘制的是平均情况，但是ICE会显示每个实例的情况。ICE可以帮助我们解释一个特定的特征改变时，模型的预测会怎么变化。</p></li><li><p>Permuted Feature Importance<br>  Permuted Feature Importance的特征重要性是通过<strong>特征值打乱后</strong>模型预测误差的变化得到的。换句话说，Permuted Feature Importance有助于定义模型中的特征对最终预测做出贡献的大小。</p></li><li><p>Global Surrogate<br>  Global Surrogate方法采用不同的方法。它通过训练一个可解释的模型来近似黑盒模型的预测。- 首先，我们使用经过训练的黑盒模型对数据集进行预测；   然后我们在该数据集和预测上训练可解释的模型。训练好的可解释模型可以近似原始模型，我们需要做的就是解释该模型。- 注：代理模型可以是任何可解释的模型：线性模型、决策树、人类定义的规则等。- 由于代理模型仅根据黑盒模型的预测而不是真实结果进行训练，因此全局代理模型只能解释黑盒模型，而不能解释数据。</p></li><li><p>Local Surrogate (LIME)<br>  LIME（Local Interpretable Model-agnostic Explanations）和global surrogate是不同的，因为它不尝试解释整个模型。相反，它训练可解释的模型来近似单个预测。LIME试图了解当我们扰乱数据样本时预测是如何变化的。——上面左边的图像被分成可解释的部分。然后，LIME 通过“关闭”一些可解释的组件（在这种情况下，使它们变灰）来生成扰动实例的数据集。对于每个扰动实例，可以使用经过训练的模型来获取图像中存在树蛙的概率，然后在该数据集上学习局部加权线性模型。最后，使用具有最高正向权重的成分来作为解释。</p></li><li><p>Shapley Value (SHAP)<br>  Shapley Value的概念来自博弈论。我们可以通过假设实例的每个特征值是游戏中的“玩家”来解释预测。每个玩家的贡献是通过在其余玩家的所有子集中添加和删除玩家来衡量的。一名球员的Shapley Value是其所有贡献的加权总和。Shapley 值是可加的，局部准确的。如果将所有特征的Shapley值加起来，再加上基值，即预测平均值，您将得到准确的预测值。这是许多其他方法所没有的功能。</p></li></ul><h1 id="特征重要性"><a href="#特征重要性" class="headerlink" title="特征重要性"></a>特征重要性</h1><ul><li><p>优点：</p></li><li><p>简单易用，不需要额外的计算或工具，只需调用XGBoost的内置函数即可得到结果。</p></li><li><p>适用于任何类型的XGBoost模型，无论是回归、分类、排序还是多输出问题。</p></li><li><p>可以选择不同的重要性类型，比如weight、gain、cover等，来反映不同的评价标准。</p></li><li><p>可以方便地绘制特征重要性的柱状图或树状图，来直观地展示和比较每个特征的重要性。</p></li><li><p>缺点：</p></li><li><p>不够精确，因为它忽略了特征之间的<code>相互作用</code>和<code>依赖关系</code>，可能会<code>高估</code>或<code>低估</code>某些特征的影响。</p></li><li><p>不够局部，因为它只能给出全局的特征重要性，<code>不能针对某个特定的样本或预测结果进行解释</code>。</p></li><li><p>不够模型无关，因为它<code>依赖于XGBoost模型</code>本身提供的指标，不能应用于其他类型的机器学习模型。</p></li></ul><h1 id="LIME"><a href="#LIME" class="headerlink" title="LIME"></a><strong>LIME</strong></h1><p>LIME（Local Interpretable Model-Agnostic Explanation）是一种<code>与模型无关</code>的<code>事后解释方法</code>。</p><p>LIME的关键特性包括两个：</p><ul><li>与模型无关。LIME只使用原始输入特征和模型预测值，对具体模型种类无要求，可以兼容树模型、神经网络等模型。</li><li>局部解释。通过局部代理模型，对单个样本的预测值进行解释。</li></ul><p>与模型无关比较好理解，局部解释需要理解局部代理模型的概念。代理模型是指对复杂黑盒模型的特征和预测值，使用<code>简单易懂的模型</code>（比如线性回归）重新拟合，从而理解黑盒模型所发现的规律，这里的简单易懂的模型就是黑盒模型的代理模型。而局部代理指的是代理模型的样本取自待解释样本的<code>附近</code>。</p><ul><li><p>优点</p></li><li><p>应用比较广泛，表格数据、图像或文本领域都可以使用。</p></li><li><p>模型无关。SVM、XGBoost、神经网络等都可以用LIME解释。</p></li><li><p>LIME可以通过预测值之间的距离，度量出局部代理模型接近单个样本的黑盒模型预测值的程度。</p></li><li><p>LIME可能会使用原有模型中未使用到的，但是易于理解的特征。例如，一个文本分类器用词语embedding之后的特征入模，但是解释的时候可以用衍生的变量（比如，这个单词在句子中是否出现过），来拟合局部代理模型。相对于原特征，这个新特征更容易解释。</p></li><li><p>缺点</p></li><li><p>LIME的<code>解释不稳定</code>。不稳定性主要来自于对要解释的样本进行抽样所带来的随机性，重新抽样后生成的样本发生变化，再拟合模型，单个样本中关于特征的重要性可能会发生变化。</p></li><li><p>只进行正态抽样会<code>忽略掉特征之间的相关性</code>，导致产生一些<code>不合理的样本</code>，比如身高180的人体重90斤，此处身高和体重一般是具有相关性的。这里核函数（权重）的设置需要进行多次尝试，以判断得到的解释是否合理。</p></li></ul><h1 id="SHAP"><a href="#SHAP" class="headerlink" title="SHAP"></a>SHAP</h1><p>SHAP（Shapley Additive Explanation）是一种借鉴了<code>博弈论思想</code>的事后解释方法，SHAP通过计算模型中各个特征的<code>边际贡献</code>来衡量各个特征的影响大小，进而对黑盒模型进行解释，该边际贡献在SHAP中称为Shapley Value。<br>SHAP具有3条重要性质：</p><ul><li><p>局部保真性（Local Accuracy）：对于单个样本，每个特征的影响与常数项之和等价于模型的输出结果。事后解释模型对单个样本的预测值与黑盒模型对单个样本的预测值相等。</p></li><li><p>缺失无影响（Missingness）：特征值中的缺失值对模型的影响为0。这不同于XGBoost的特征重要度，因为XGBoost在处理缺失值时会根据增益判断缺失值分裂到左孩子节点还是右孩子节点，对模型一定会造成影响。</p></li><li><p>一致性（Consistency）：当更改某个特征的边际贡献度时，其他特征的边际贡献度不应该发生变化，模型最终结果的变化等价于该特征的边际贡献度的变化。而XGBoost的特征重要性，因为是在整个训练集上计算得到重要性期望，更改某个特征会使得其他特征的重要性也发生变化。另外，一致性还体现在，当黑盒模型发生变化时，特征的SHAP值随着特征在新模型中贡献的变化而变化。</p></li><li><p>优点</p></li><li><p>基于博弈论，理论基础完备，包括对称性、可加性、有效性等公理；对比LIME只是假设机器学习模型在局部有线性关系，没有坚实的理论基础。</p></li><li><p>SHAP公平地分配了样本中每个特征的贡献值，最终解释了单个样本模型预测值与平均模型预测值之间的差异；LIME并不能保证模型的预测值可以公平地分配给每一个特征。</p></li><li><p>Shapley Value可以有不同的<code>对比解释</code>，其既可以解释单个样本的模型预测值与平均模型预测值之间的差异，也可以解释单个样本的模型预测值与另一个样本的模型预测值之间的差异。</p></li><li><p>缺点</p></li><li><p><code>计算耗时过长</code>。虽然TreeSHAP的时间复杂度只有多项式级别，但这仅仅是针对树模型而言的。其他近似算法的计算复杂度仍然不低。</p></li><li><p>当特征之间存在相关性时，有些近似算法的效果会变差，如Kernel SHAP的近似算法要求<code>特征之间要互相独立</code>，然而大多数情况并不满足这一条件（如用户的身高和体重往往存在很大的相关性），这会导致得到的关于黑盒模型的解释性结果不够准确。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>数据、验证</title>
    <link href="https://chierhy.github.io/2024/07/28/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E6%95%B0%E6%8D%AE%E3%80%81%E9%AA%8C%E8%AF%81/"/>
    <id>https://chierhy.github.io/2024/07/28/%E5%AD%A6%E7%A7%91-%E7%AB%9E%E8%B5%9B-%E9%A1%B9%E7%9B%AE/%E6%95%B0%E6%8D%AE%E3%80%81%E9%AA%8C%E8%AF%81/</id>
    <published>2024-07-28T06:19:00.000Z</published>
    <updated>2024-07-28T06:19:43.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="https://www.bilibili.com/read/cv12736369/">深入研究k折交叉验证（K fold Cross Validation） - 哔哩哔哩 (bilibili.com)</a></p><ol start="2"><li><p><a href="https://zhuanlan.zhihu.com/p/149928673">微表情识别（Micro expression recognition）简述 - 知乎 (zhihu.com)</a><br> 为了减少不同受试者之间面部的差异，可以基于面部特征点对所有片段进行<strong>面部配准（face register）</strong>，<br> 简单来说，face register 首先选择一张脸作为标准人脸图像，提取其特征点；对于每个视频片段，提取视频片段中第一帧的面部特征点，并计算一个映射函数（此处使用 LWM 算法），将这帧图像的特征点映射到标准图像的特征点上；最后，将此映射作用在视频中的所有帧上。这种方法可以使所有视频片段中的人脸特征点位置相同，从而减少不同人脸的差异。</p><p> 我在毕设中使用python的dlib库识别人脸特征点，进行face register，经过实验发现register可以提高模型在10-fold验证时的效果，但降低了模型在LOSO验证时的效果。由于dlib对于人脸特征点的识别并不是那么准，因此上述现象也可能是由dlib的误差导致。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="学科" scheme="https://chierhy.github.io/categories/%E5%AD%A6%E7%A7%91/"/>
    
    
  </entry>
  
  <entry>
    <title>model-autoencoder</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/model-autoencoder/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/model-autoencoder/</id>
    <published>2024-07-28T06:18:00.000Z</published>
    <updated>2024-07-28T06:18:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在各种研究工作中，已经探索了自动编码器的结合，以提高分类器的性能。自动编码器表现出一系列的变化，每个都针对特定的目标，如去噪，特征提取，生成合成数据和降低维度。部署自编码器的核心原则是在输出端复制输入数据，同时保持最大相似度。编码器和解码器都由一个编码器和一个解码器组成，一旦编码器被训练，它与分类器的集成无缝地增强了结果。值得注意的是，在具体的研究中，自动编码器被用于消除噪声和增强结果[ 24 ]。</p><p>% 自编码器代替PCA 进行无监督特征学习 . 自编码器的变体 将自动特征学习和降维结合到一个集成神经网络中进行活动识别<br>Autoencoders are used to replace Principal Component Analysis (PCA) for unsupervised feature learning.<br>Variants of autoencoders integrate automatic feature learning and dimensionality reduction into a single neural network for activity recognition.</p><h1 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h1><ul><li>自编码器是一种前馈神经网络，它学习输入样本 (通常通过引入瓶颈层来实现)的紧凑潜在表示，并被训练以重建输入样本。与基于线性变换的方法相比，基于自编码器的模型具有优势，因为自编码器能够学习到最能表征数据的流形结构。此外，这种方法还可以通过最小化一些差异度量来显式地减少领域差异。</li><li></li></ul><h1 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h1><ul><li>VAE-based semi-supervised methods is that the latent representations are stochastically sampled from the prior distribution instead of being directly rendered from the explicit observations.</li></ul><h1 id="变种"><a href="#变种" class="headerlink" title="变种"></a>变种</h1><p><strong>SDAE</strong></p><ul><li>The stacked denoising autoencoders learns features in an unsupervised manner by using the greedy layer-wise training method</li><li>The training process of the stacked denoising autoencoders starts by training the first layer on the input to learn the features in the first hidden layer. Similarly, this operation is repeatedly conducted for subsequent layers to finish the whole training process. After the layer-wise training, a finetuning operation is followed to optimize the parameters of all layers through backpropagation, which will further improve the results.</li><li>Masking Noise Level: The masking noise level indicates the fraction of elements of the input data that are forced to 0. Fig. 6 (a) demonstrates the influence of different masking noise levels on the classification accuracy. The best performance arises at around the corruption ratio (or masking noise level) of 0.3, from which increasing or decreasing the ratio will lead to a decrease in the classification accuracy. Note that when the corruption ratio equals 0, the model becomes a stacked sparse autoencode</li></ul><p>VAE</p><ul><li>VAE is a variant of autoencoder that is able to denoise motion signals. As autoencoder, VAE encodes the input data into a feature-representation vector in the latent space. However, in VAE, the feature representation vector is constrained to Gaussian distributions [33]. Also, VAE uses two loss functions, a combination of Kullback-Leibler divergence (KL) and Mean Square Error (MSE). The first loss function forces the feature-representation vector in the latent space to follow a Gaussian distribution, measuring the relative entropy between the approximate posterior and the prior probability density function [33]. The second loss measures the similarity between the output and the input</li><li></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>model-diffusion</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/model-diffusion/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/model-diffusion/</id>
    <published>2024-07-28T06:18:00.000Z</published>
    <updated>2024-07-28T06:18:03.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a>.</p><ul><li>Diffusion models are inspired by non-equilibrium thermodynamics. They define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise. Unlike VAE or flow models, diffusion models are learned with a fixed procedure and the latent variable has high dimensionality (same as the original data).</li><li>Several diffusion-based generative models have been proposed with similar ideas underneath, including <em>diffusion probabilistic models</em> (<a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a>), <em>noise-conditioned score network</em> (<strong>NCSN</strong>; <a href="https://arxiv.org/abs/1907.05600">Yang &amp; Ermon, 2019</a>), and <em>denoising diffusion probabilistic models</em> (<strong>DDPM</strong>; <a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a>).</li><li>文中还有很多算法细节</li></ul><p><a href="https://mp.weixin.qq.com/s/7ky2ZFLX8xCWhoK2BvzuJw">OpenAI Lilian Weng小姐姐教你从头设计视频生成扩散模型 (qq.com)</a></p><h1 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h1><p><a href="https://mp.weixin.qq.com/s/OelhMW_v8KFkxfCAGBfE8A">扩散模型的原理及实现（Pytorch） (qq.com)</a><br>扩散模型的导火索，是始于2020 年所提出的 DDPM（Denoising Diffusion Probabilistic Model）<br><strong>VAE</strong><br>VAE 采用了编码器、概率潜在空间和解码器。在训练过程中，编码器预测每个图像的均值和方差。然后从高斯分布中对这些值进行采样，并将其传递到解码器中，其中输入的图像预计与输出的图像相似。这个过程包括使用KL Divergence来计算损失。VAEs的一个显著优势在于它们能够生成各种各样的图像。在采样阶段简单地从高斯分布中采样，解码器创建一个新的图像。<br><strong>GAN</strong><br>在变分自编码器(VAEs)的短短一年之后，一个开创性的生成家族模型出现了——生成对抗网络(GANs)，标志着一类新的生成模型的开始，其特征是两个神经网络的协作:一个生成器和一个鉴别器，涉及对抗性训练过程。生成器的目标是从随机噪声中生成真实的数据，例如图像，而鉴别器则努力区分真实数据和生成数据。在整个训练阶段，生成器和鉴别器通过竞争性学习过程不断完善自己的能力。生成器生成越来越有说服力的数据，从而比鉴别器更聪明，而鉴别器又提高了辨别真实样本和生成样本的能力。这种对抗性的相互作用在生成器生成高质量、逼真的数据时达到顶峰。在采样阶段，经过GAN训练后，生成器通过输入随机噪声产生新的样本。它将这些噪声转换为通常反映真实示例的数据。</p><p><strong>为什么我们需要扩散模型：DDPM</strong><br>两种模型都有不同的问题，虽然GANs擅长于生成与训练集中的图像非常相似的逼真图像，但VAEs擅长于创建各种各样的图像，尽管有产生模糊图像的倾向。但是现有的模型还没有成功地将这两种功能结合起来——创造出既高度逼真又多样化的图像。这一挑战给研究人员带来了一个需要解决的重大障碍。<br>在第一篇 GAN 论文发表六年后，在 VAE 论文发表七年后，一个开创性的模型出现了:去噪扩散概率模型(DDPM)。<strong>DDPM 结合了两模型的优势，擅长于创造多样化和逼真的图像。</strong></p><p>扩散模型的基本思想受到非平衡统计物理的启发，即通过迭代向前扩散的方法逐步消除数据分布中的结构。然后学习一个反向扩散过程，在 data中恢复结构</p><p>优点</p><ul><li>One of the principal advantages of DDPM is that the formation of the original motion sequence can be retained. It means that we can easily apply more constraints during the denoising process.</li><li>By specifying desired properties, such as noise levels and variability, diffusion modelling can generate predictable and consistent synthetic data, making it easier to train models.</li><li>However, this regularity can also limit the ability of synthetic data to capture the complexity and variability of real-world data. In general, real data is preferred for training models, as it better reflects the diversity of realworld conditions. Synthetic data, however, can be a useful supplement shown in Fig. 4 when real data is scarce or expensive</li></ul><h1 id="code"><a href="#code" class="headerlink" title="code"></a>code</h1><p><a href="https://mp.weixin.qq.com/s/OelhMW_v8KFkxfCAGBfE8A">扩散模型的原理及实现（Pytorch） (qq.com)</a><br><a href="https://github.com/Camaltra/this-is-not-real-aerial-imagery/">https://github.com/Camaltra/this-is-not-real-aerial-imagery/</a></p><h1 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h1><p>DDPM Paper <a href="https://arxiv.org/abs/2006.11239ConvNext">https://arxiv.org/abs/2006.11239ConvNext</a> Paper <a href="https://arxiv.org/abs/2201.03545UNet">https://arxiv.org/abs/2201.03545UNet</a> Paper: <a href="https://arxiv.org/abs/1505.04597ACC">https://arxiv.org/abs/1505.04597ACC</a> UNet: <a href="https://arxiv.org/abs/2308.13680">https://arxiv.org/abs/2308.13680</a><br><a href="https://mp.weixin.qq.com/s/erDxOZ03Hd7fWOhH6nTpBQ">扩散模型最新有何进展？普林斯顿伯克利最新「扩散模型」综述：应用、引导生成、统计率和优化 (qq.com)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>model-machine classifier</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/model-machine%20classifier/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/model-machine%20classifier/</id>
    <published>2024-07-28T06:18:00.000Z</published>
    <updated>2024-07-28T06:18:22.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="XGBoost-极限梯度提升"><a href="#XGBoost-极限梯度提升" class="headerlink" title="XGBoost (极限梯度提升)"></a>XGBoost (极限梯度提升)</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理:"></a>原理:</h2><ul><li><strong>提升算法:</strong> XGBoost 是梯度提升决策树的实现，旨在提高速度和性能。</li><li><strong>梯度提升框架:</strong> 通过关注前一轮学习器的错误，结合多个弱学习器（通常是决策树）来创建一个强学习器。</li></ul><h2 id="优点"><a href="#优点" class="headerlink" title="优点:"></a>优点:</h2><ul><li><strong>高性能:</strong> 以高预测能力著称，常用于赢得机器学习竞赛的解决方案。</li><li><strong>高效:</strong> 针对速度和内存使用进行了优化，利用并行处理。</li><li><strong>灵活性:</strong> 支持多种目标函数和自定义评估函数。</li><li><strong>正则化:</strong> 采用 L 1 和 L 2 正则化来防止过拟合。</li></ul><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点:"></a>缺点:</h2><ul><li><strong>复杂性:</strong> 需要仔细调整超参数。</li><li><strong>计算密集:</strong> 对大数据集来说资源消耗较大。</li><li><strong>可解释性差:</strong> 相比简单模型如决策树，模型更难解释。</li></ul><h1 id="CatBoost-类别提升"><a href="#CatBoost-类别提升" class="headerlink" title="CatBoost (类别提升)"></a>CatBoost (类别提升)</h1><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理:"></a>原理:</h2><ul><li><strong>提升算法:</strong> 类似于 XGBoost，但专门针对类别特征进行了优化。</li><li><strong>有序提升:</strong> 利用有序提升来减少过拟合。</li></ul><h2 id="优点-1"><a href="#优点-1" class="headerlink" title="优点:"></a>优点:</h2><ul><li><strong>处理类别特征:</strong> 自动处理类别变量，减少了大量预处理工作。</li><li><strong>快速准确:</strong> 提供高准确性且速度较快，因为实现高效。</li><li><strong>减少过拟合:</strong> 有序提升有助于减少过拟合，相比传统提升方法更有效。</li></ul><h2 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点:"></a>缺点:</h2><ul><li><strong>复杂性:</strong> 超参数调整较为复杂。</li><li><strong>资源使用:</strong> 可能会占用大量内存和计算资源。</li><li><strong>社区支持少:</strong> 相比 XGBoost，社区支持和资源较少。</li></ul><h1 id="AdaBoost-自适应提升"><a href="#AdaBoost-自适应提升" class="headerlink" title="AdaBoost (自适应提升)"></a>AdaBoost (自适应提升)</h1><h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理:"></a>原理:</h2><ul><li><strong>提升算法:</strong> 将多个弱分类器（通常是决策桩）结合成一个强分类器。</li><li><strong>加权投票:</strong> 每次迭代中，错误分类的实例权重增加，正确分类的实例权重减小。</li></ul><h2 id="优点-2"><a href="#优点-2" class="headerlink" title="优点:"></a>优点:</h2><ul><li><strong>简单易实现:</strong> 算法简单直接，易于实现。</li><li><strong>提升弱学习器:</strong> 可以显著提高弱分类器的性能。</li><li><strong>多功能:</strong> 可以与各种基础分类器一起使用。</li></ul><h2 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点:"></a>缺点:</h2><ul><li><strong>对噪声数据敏感:</strong> 在有噪声数据和离群值时性能会下降。</li><li><strong>过拟合:</strong> 如果基础学习器过于复杂或迭代次数过多，可能会导致过拟合。</li><li><strong>学习速度慢:</strong> 尤其在有大量弱学习器时，可能会很慢。</li></ul><h1 id="集成方法"><a href="#集成方法" class="headerlink" title="集成方法"></a>集成方法</h1><h2 id="原理-3"><a href="#原理-3" class="headerlink" title="原理:"></a>原理:</h2><ul><li><strong>组合模型:</strong> 集成方法通过组合多个模型的预测来提高准确性和鲁棒性。</li><li><strong>投票&#x2F;加权:</strong> 通过投票、平均或堆叠等技术组合基础模型的输出。</li></ul><h2 id="优点-3"><a href="#优点-3" class="headerlink" title="优点:"></a>优点:</h2><ul><li><strong>提高准确性:</strong> 通常比单个模型提供更好的性能。</li><li><strong>鲁棒性:</strong> 对数据中的过拟合和噪声更鲁棒。</li><li><strong>灵活性:</strong> 可以组合不同类型的模型，发挥各自的优势。</li></ul><h2 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点:"></a>缺点:</h2><ul><li><strong>复杂性:</strong> 比单个模型更复杂，难以实现和解释。</li><li><strong>计算密集:</strong> 由于多个模型，需更多计算资源。</li><li><strong>超参数调整:</strong> 需调整多个模型的超参数，耗时费力。</li></ul><h1 id="分类器之间的区别和使用场景"><a href="#分类器之间的区别和使用场景" class="headerlink" title="分类器之间的区别和使用场景:"></a>分类器之间的区别和使用场景:</h1><ul><li><strong>XGBoost</strong> 适用于需要高准确性和速度的场景，尤其是在参数调优良好的情况下，广泛用于竞赛中。</li><li><strong>CatBoost</strong> 擅长处理类别数据，提供快速且准确的模型，并减少过拟合。</li><li><strong>AdaBoost</strong> 是一种简单有效的提升方法，但在噪声数据下表现较差。</li><li><strong>集成方法</strong> 对提高准确性和鲁棒性非常有效，但计算代价高且复杂。</li></ul><p>这些分类器各有优势和劣势，选择分类器通常取决于任务的具体需求，例如数据的性质、可解释性的需求以及可用的计算资源。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>task-Data bias</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/task-Data%20bias/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/task-Data%20bias/</id>
    <published>2024-07-28T06:18:00.000Z</published>
    <updated>2024-07-28T06:18:56.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>迁移：样本数据、特征、参数、关系<br> 2.2. Knowledge Transfer The literature on knowledge transfer can be generally categorized into four main approaches based on the type of knowledge they transfer [21]: </p><ul><li>• Instance Transfer: Methods placing in this approach, mainly aim for weighting and transforming labeled instances into the target domain. Standard supervised machine learning models can be applied on transferred samples afterward. </li><li>• Feature Representation Transfer: The core idea of this category’s models is about finding a common representation of both source and target domain that decreases the distance between domains while keeping their classes discernible. </li><li>• Parameter Transfer: The basic assumption is that the source and target domains share some parameters or prior distributions of the models’ hyperparameters. These methods focus on the transformation of prior knowledge and parameters between domains. </li><li>• Relational Transfer: The knowledge to be transferred is the relationship among the data. Mapping of relational knowledge between the source domain and target domains is built. Both domains should be relational.</li></ul><p>Authors in [12] proposed an instance-based transfer model in HAR domain that interprets the data of source domain as pseudo training data with respect to their similarity measure to the target domain samples. These pseudo data then will be fed into supervised learning algorithms for training the classifier.<br>Quite recently, another Cross-Domain Activity Recognition translation framework was proposed by researchers in [28]. It first obtains pseudo labels for the target domain using the majority voting technique. Then, it transforms both domains into common subspaces considering intra-class correlations. This model which is working in a semi-supervised manner obtains labels of target domain via the second annotation.<br>Transfer Component Analysis (TCA) is a domain adaptation method introduced in [20]. TCA learns transfer components across domains in a Reproducing Kernel Hilbert Space for establishing a representation transfer. With the new representation in the subspace spanned by these transfer components, standard machine learning methods are applicable to train classifiers or regression models in the source domain for use in the target domain.</p><p>迁移模型，映射数据到无环境无关，对抗学习与环境无关的鲁棒特征<br>The commonplace approach for tackling such heterogeneity, by using instance-specific labeled data to build individual &amp; devicespecific classifiers, is clearly infeasible for practical societal-scale deployment. Instead, significant research has focused on techniques for automated domain adaptation – i.e., a transfer learning-based mechanism that allows a model trained on one domain to flexibly evolve and cater to data collected under a different domain&#x2F;context, while requiring modest-to-no labeled training data from the target domain. A variety of approaches for such HAR-oriented domain adaptation have been suggested in recent years, including techniques that </p><ul><li>(a) employ transfer learning to modify a source domain model with only modest amounts of target domain labeled data (Khan, Roy, &amp; Misra, 2018; Qin, Chen, Wang, &amp; Yu, 2019);</li><li>(b) map domain-dependent sensor values to a domain-independent, common low-dimensional latent space (Jeyakumar, Lai, Suda, &amp; Srivastava, 2019); and</li><li>(c) use adversarial learning techniques to learn a set of robust features that are invariant to data from either training (source) or test (target domains) (Ganin &amp; Lempitsky, 2014).<br>In general, these techniques suffer from at least one of three key limitations: (a) They often require at least modest amounts of labeled target domain data, with their performance degrading sharply in the absence of any labeled data; (b) They require capture of synchronously paired data–i.e., the simultaneous capture of target and source domain data streams, as a means of implicit labeling (e.g., Akbari and Jafari (2019) and Jeyakumar et al. (2019)); (c) They require modification of the gesture classification model—while not technically difficult, this requirement presents practical difficulties as many ML-based activity models are now bundled as standard executable binaries by either OS or App developers.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>task-量化研究</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/task-%E9%87%8F%E5%8C%96%E7%A0%94%E7%A9%B6/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/task-%E9%87%8F%E5%8C%96%E7%A0%94%E7%A9%B6/</id>
    <published>2024-07-28T06:18:00.000Z</published>
    <updated>2024-07-28T06:18:40.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="%E5%B7%A5%E4%BD%9C%E5%B2%97%E4%BD%8D.md">工作岗位</a></p><p>Trexquant<br><a href="https://www.1point3acres.com/bbs/tag-3147-2.html">公司:Trexquant | 一亩三分地 (1point3acres.com)</a><br>职责<br>    开发预测未来股票回报的市场中性中频阿尔法<br>    调查和实施最近的学术研究<br>    开发算法来过滤和组合阿尔法<br>    解析用于未来alpha开发的数据集<br>    将机器学习技术应用于alpha发现和投资组合构建<br>工作内容<br>    - 指导我的full-time员工推荐几篇经典的量化领域paper，通过自己阅读后理解这篇paper里提出的alpha来源然后据此自行加以修改+测试，直到产生了各项指标都通过测试的信号。。。好像我身边也是<br>    - 在团队里，由researcher贡献各种各样的alpha idea，pm自己一般不会亲力亲为挖因子，而是只负责提供初步的方向，指导reseacher做研究，并且负责挑选、组合和管理researcher贡献的信号，类似于教授和博士生。这种模式的弊端就是每个团队的researcher要负责从idea generation, data colletion, back-testing甚至下单的整个投资流程，用来挖因子的时间精力不够，挖出的因子有限（但好处是researcher发展更全面，对策略的理解更深刻，长期来看更容易成长为pm）。<br>    - 工作类型相当于最底层的信号挖掘机。他家的模式和worldquant相同，一个full-time员工会同时指导多个这种远程搬砖的alpha researcher，后者在他们的内部回测系统上实验自己的信号。我的个人看法是，纯粹的数据挖掘肯定是存在的，据说干的事类似于把一堆指标和运算符号扔进池子里，然后返回各种可能组合里表现最好的信号，但是应该是只有部分组做这种事，或者是某些员工project的一种而已。至少我工作的时候，模式都是由指导我的full-time员工推荐几篇经典的量化领域paper，通过自己阅读后理解这篇paper里提出的alpha来源然后据此自行加以修改+测试，直到产生了各项指标都通过测试的信号。一般地，通过不断的尝试和开脑洞修修补补，一篇paper的idea其实能用来自创七八个相关度不高的信号。我刚干的时候不太懂，有时候强行测试大量参数，最后选了一个看起来没有任何道理（比如1.684, 37.523）之类的数值，指导我的人还会谆谆告诫曰我们找的信号一定要有很强的经济含义，不能靠强行fit。所以纯粹的数据挖掘应该不至于，大多数signal都是产生于读一些资料得到的idea。我的猜想是即使那些纯数据挖掘的组，得到的信号也都会进行一系列robustness test，并且能用经济学解释的才会真的上实盘。顺带一提，在提交这些alpha的时候，是被要求每周写一个研究报告，介绍自己如何得到这个alpha，背后的经济含义如何给研究员看的，可见公司对于alpha背后的经济含义非常在乎。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>method-agents</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-agents/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-agents/</id>
    <published>2024-07-28T06:17:00.000Z</published>
    <updated>2024-07-28T06:17:31.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="http://link.zhihu.com/?target=https://github.com/sxontheway/Keep-Learning/blob/master/Research/Federated_Learning.md">http://link.zhihu.com/?target=https%3A//github.com/sxontheway/Keep-Learning/blob/master/Research/Federated_Learning.md</a></p><h1 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h1><p><a href="https://arxiv.org/abs/2405.02957">[2405.02957] Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents (arxiv.org)</a></p><ul><li>质量和效率，隐私。医疗记录库、经验库。</li><li>agent hostpital. Pdf</li><li>在本文中，我们介绍了一个名为Agent医院的医院模拟，它模拟了治疗疾病的整个过程。所有患者、护士和医生都是由大型语言模型（LLM）驱动的自治代理。我们的中心目标是让医生代理学习如何在模拟中治疗疾病。为此，我们提出了一种名为MedAgent-Zero的方法。由于模拟可以基于知识库和LLM模拟疾病的发作和进展，医生代理可以不断从成功和不成功的案例中积累经验。模拟实验表明，医生代理在各种任务中的治疗性能不断提高。更有趣的是，医生代理在Agent医院获得的知识适用于现实世界的医疗保险基准。在治疗了大约一万名患者（现实世界的医生可能需要两年多的时间）后，进化的医生代理在涵盖主要呼吸系统疾病的MedQA数据集子集上实现了93.06%的最先进准确率。这项工作为推进LLM驱动的代理技术在医疗场景中的应用铺平了道路。</li><li>MedAgent-Zero”这一创新的训练策略。与以往基于反向传播的模型训练方法不同，MedAgent-Zero 通过动态更新医疗记录库和经验库，有效提升了医疗代理的决策能力。这种策略的显著优势在于，它允许模型在不直接修改权重参数的情况下进行优化。这意味着，我们可以利用第三方的大型语言模型 API，而无需动用昂贵的 GPU 资源来训练模型。通过知识库的持续积累和优化，模型不仅保持了其底层的语言处理能力，还增强了对特定医疗知识的掌握。这不仅提高了模型的决策质量，也为 AI 在医疗等高知识要求领域的应用提供了新的可能性。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>method-few shot learning</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-few%20shot%20learning/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-few%20shot%20learning/</id>
    <published>2024-07-28T06:17:00.000Z</published>
    <updated>2024-07-28T06:17:37.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="http://link.zhihu.com/?target=https://github.com/sxontheway/Keep-Learning/blob/master/Research/Few_Sample_Learning.md">http://link.zhihu.com/?target=https%3A//github.com/sxontheway/Keep-Learning/blob/master/Research/Few_Sample_Learning.md</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>method-adversial learning</title>
    <link href="https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-adversial%20learning/"/>
    <id>https://chierhy.github.io/2024/07/28/%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%9E%8B/method-adversial%20learning/</id>
    <published>2024-07-28T06:17:00.000Z</published>
    <updated>2024-07-28T06:17:27.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p><a href="https://spaces.ac.cn/archives/7234">对抗训练浅谈：意义、方法和思考（附Keras实现） - 科学空间|Scientific Spaces</a></p></blockquote><p>深度学习中的对抗，一般会有两个含义：一个是生成对抗网络（Generative Adversarial Networks，GAN），代表着一大类先进的生成模型；另一个则是跟对抗攻击、对抗样本相关的领域，它跟 GAN 相关，但又很不一样，它主要关心的是模型在小扰动下的稳健性。</p><h1 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/91269728">【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现 - 知乎 (zhihu.com)</a><br><a href="https://spaces.ac.cn/archives/7234">对抗训练浅谈：意义、方法和思考（附Keras实现） - 科学空间|Scientific Spaces</a></p></blockquote><p>Goodfellow 认为，<strong>神经网络由于其线性的特点，很容易受到线性扰动的攻击。</strong>于是，他提出了 Fast Gradient Sign Method (FGSM) ，来计算输入样本的扰动。Goodfellow 还总结了对抗训练的两个作用：</p><ol><li>提高模型应对恶意对抗样本时的鲁棒性；</li><li>作为一种 regularization，减少 overfitting，提高泛化能力。<br><strong>在 NLP 任务中，对抗训练的角色不再是为了防御基于梯度的恶意攻击，反而更多的是作为一种 regularization，提高模型的泛化能力</strong>。</li></ol><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><p><a href="https://www.sohu.com/a/636653930_121119001">ICLR 2023 | 环境标签平滑，一行代码提升对抗学习稳定性&#x2F;泛化性_训练_数据_Domain (sohu.com)</a><br>    <a href="https://github.com/yfzhang114/Environment-Label-Smoothing/tree/master">yfzhang114&#x2F;Environment-Label-Smoothing: This is an official PyTorch implementation of the ICLR 2023 paper 《Free Lunch for Domain Adversarial Training: Environment Label Smoothing》. (github.com)</a></p><h1 id="code"><a href="#code" class="headerlink" title="code"></a>code</h1><ul><li><a href="https://github.com/kdhht2334/Contrastive-Adversarial-Learning-FER/tree/main">kdhht2334&#x2F;Contrastive-Adversarial-Learning-FER: [AAAI2021] A repository of Contrastive Adversarial Learning for Person-independent FER (github.com)</a></li><li><a href="https://github.com/markub3327/HAR-Transformer/blob/main/Training.ipynb">HAR-Transformer&#x2F;Training.ipynb at main · markub3327&#x2F;HAR-Transformer (github.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/163740677">Transferable Adversarial Training学习+代码(pytorch) - 知乎 (zhihu.com)</a></li><li><a href="https://github.com/iantangc/ContrastiveLearningHAR?tab=readme-ov-file">iantangc&#x2F;ContrastiveLearningHAR: Contrastive Learning (SimCLR) for Human Activity Recognition (github.com)</a></li><li></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="科研" scheme="https://chierhy.github.io/categories/%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
</feed>
